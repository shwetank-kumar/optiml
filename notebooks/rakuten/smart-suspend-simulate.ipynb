{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d8e6eee-ff95-42f2-8343-28ba0ee4d1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6150c8f5-db47-4ddd-aa17-afd1e593706c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06:08:28 INFO:Snowflake Connector for Python Version: 2.7.12, Python Version: 3.8.16, Platform: macOS-10.15.7-x86_64-i386-64bit\n",
      "06:08:28 INFO:This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.\n",
      "06:08:28 INFO:Setting use_openssl_only mode to False\n",
      "06:08:29 INFO:Snowflake Connector for Python Version: 2.7.12, Python Version: 3.8.16, Platform: macOS-10.15.7-x86_64-i386-64bit\n",
      "06:08:29 INFO:This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "connected to account OMWYKHW-US_WEST_2 wh DEMO db SHARE_COPY schema KNOT_MONITORING with role DEV\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06:08:31 INFO:Snowpark Session information: \n",
      "\"version\" : 1.3.0,\n",
      "\"python.version\" : 3.8.16,\n",
      "\"python.connector.version\" : 2.7.12,\n",
      "\"python.connector.session.id\" : 9098488785371206,\n",
      "\"os.name\" : Darwin\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 16.1 ms (started: 2023-06-06 18:08:31 -04:00)\n"
     ]
    }
   ],
   "source": [
    "%reload_ext dotenv\n",
    "\n",
    "%dotenv ../../env/.env\n",
    "\n",
    "import warnings\n",
    "import utils\n",
    "from pandas import Timedelta\n",
    "import time\n",
    "from utils import logger, sql_to_df, run_sql, session, conn\n",
    "import pandas as pd\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "try:\n",
    "    %load_ext autotime\n",
    "except:\n",
    "    !pip install ipython-autotime\n",
    "    %load_ext autotime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7edbffc8-3643-4d3a-9ccf-cc29aa6cd83c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 429 µs (started: 2023-06-03 20:45:29 -04:00)\n"
     ]
    }
   ],
   "source": [
    "# %conda install python-dotenv\n",
    "# %conda install snowflake-connector-python[pandas]\n",
    "# %conda install pandas\n",
    "# %conda install plotly\n",
    "# %conda install snowflake-snowpark-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "607b1a34-64db-4981-bfa5-281b6344b590",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06:08:31 INFO:query: [select warehouse_id, era_start, era_end from query_era  -- where warehouse_id in...]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using arrow to fetch results...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06:08:35 INFO:query execution done\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>warehouse_id</th>\n",
       "      <th>era_start</th>\n",
       "      <th>era_end</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>2023-02-17 11:39:08.580000-08:00</td>\n",
       "      <td>2023-02-17 11:39:08.782000-08:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>2023-02-17 11:39:09.963000-08:00</td>\n",
       "      <td>2023-02-17 11:39:10.076000-08:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>2023-02-17 11:39:10.546000-08:00</td>\n",
       "      <td>2023-02-17 11:39:17.920000-08:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>19</td>\n",
       "      <td>2023-02-17 11:39:10.942000-08:00</td>\n",
       "      <td>2023-02-17 11:39:13.754000-08:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>19</td>\n",
       "      <td>2023-02-17 11:39:16.967000-08:00</td>\n",
       "      <td>2023-02-17 11:39:19.862000-08:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2860458</th>\n",
       "      <td>19</td>\n",
       "      <td>2023-06-06 09:59:42.122000-07:00</td>\n",
       "      <td>2023-06-06 09:59:42.344000-07:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2860459</th>\n",
       "      <td>19</td>\n",
       "      <td>2023-06-06 09:59:43.604000-07:00</td>\n",
       "      <td>2023-06-06 09:59:55.034000-07:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2860460</th>\n",
       "      <td>4</td>\n",
       "      <td>2023-06-06 09:59:56.708000-07:00</td>\n",
       "      <td>2023-06-06 09:59:57.429000-07:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2860461</th>\n",
       "      <td>4</td>\n",
       "      <td>2023-06-06 09:59:57.970000-07:00</td>\n",
       "      <td>2023-06-06 09:59:58.526000-07:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2860462</th>\n",
       "      <td>27</td>\n",
       "      <td>2023-06-06 09:59:58.679000-07:00</td>\n",
       "      <td>2023-06-06 09:59:59.444000-07:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2860463 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         warehouse_id                        era_start  \\\n",
       "0                   4 2023-02-17 11:39:08.580000-08:00   \n",
       "1                   4 2023-02-17 11:39:09.963000-08:00   \n",
       "2                   4 2023-02-17 11:39:10.546000-08:00   \n",
       "3                  19 2023-02-17 11:39:10.942000-08:00   \n",
       "4                  19 2023-02-17 11:39:16.967000-08:00   \n",
       "...               ...                              ...   \n",
       "2860458            19 2023-06-06 09:59:42.122000-07:00   \n",
       "2860459            19 2023-06-06 09:59:43.604000-07:00   \n",
       "2860460             4 2023-06-06 09:59:56.708000-07:00   \n",
       "2860461             4 2023-06-06 09:59:57.970000-07:00   \n",
       "2860462            27 2023-06-06 09:59:58.679000-07:00   \n",
       "\n",
       "                                 era_end  \n",
       "0       2023-02-17 11:39:08.782000-08:00  \n",
       "1       2023-02-17 11:39:10.076000-08:00  \n",
       "2       2023-02-17 11:39:17.920000-08:00  \n",
       "3       2023-02-17 11:39:13.754000-08:00  \n",
       "4       2023-02-17 11:39:19.862000-08:00  \n",
       "...                                  ...  \n",
       "2860458 2023-06-06 09:59:42.344000-07:00  \n",
       "2860459 2023-06-06 09:59:55.034000-07:00  \n",
       "2860460 2023-06-06 09:59:57.429000-07:00  \n",
       "2860461 2023-06-06 09:59:58.526000-07:00  \n",
       "2860462 2023-06-06 09:59:59.444000-07:00  \n",
       "\n",
       "[2860463 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 17.9 s (started: 2023-06-06 18:08:31 -04:00)\n"
     ]
    }
   ],
   "source": [
    "# warehouse_ids = [41,42,43]\n",
    "warehouse_ids = [38,39,40]\n",
    "joined = ','.join([str(w) for w in warehouse_ids])\n",
    "sql_str = f\"({joined})\"\n",
    "\n",
    "sql = f\"\"\"\n",
    "select \n",
    "    warehouse_id, \n",
    "    era_start, \n",
    "    era_end\n",
    "from query_era\n",
    "\n",
    "-- where warehouse_id in {sql_str}\n",
    "order by era_start asc\n",
    "\"\"\"\n",
    "\n",
    "df = sql_to_df(sql)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62d43f52-4309-4363-ae76-f8412d7da043",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06:08:49 INFO:query: [select warehouse_id, era_start, era_end from query_era  -- where warehouse_id in...]\n",
      "06:08:50 INFO:query execution done\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>warehouse_id</th>\n",
       "      <th>era_start</th>\n",
       "      <th>era_end</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>2023-02-17 11:39:08.580000-08:00</td>\n",
       "      <td>2023-02-17 11:39:08.782000-08:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>2023-02-17 11:39:09.963000-08:00</td>\n",
       "      <td>2023-02-17 11:39:10.076000-08:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>2023-02-17 11:39:10.546000-08:00</td>\n",
       "      <td>2023-02-17 11:39:17.920000-08:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>19</td>\n",
       "      <td>2023-02-17 11:39:10.942000-08:00</td>\n",
       "      <td>2023-02-17 11:39:13.754000-08:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>19</td>\n",
       "      <td>2023-02-17 11:39:16.967000-08:00</td>\n",
       "      <td>2023-02-17 11:39:19.862000-08:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2860458</th>\n",
       "      <td>19</td>\n",
       "      <td>2023-06-06 09:59:42.122000-07:00</td>\n",
       "      <td>2023-06-06 09:59:42.344000-07:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2860459</th>\n",
       "      <td>19</td>\n",
       "      <td>2023-06-06 09:59:43.604000-07:00</td>\n",
       "      <td>2023-06-06 09:59:55.034000-07:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2860460</th>\n",
       "      <td>4</td>\n",
       "      <td>2023-06-06 09:59:56.708000-07:00</td>\n",
       "      <td>2023-06-06 09:59:57.429000-07:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2860461</th>\n",
       "      <td>4</td>\n",
       "      <td>2023-06-06 09:59:57.970000-07:00</td>\n",
       "      <td>2023-06-06 09:59:58.526000-07:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2860462</th>\n",
       "      <td>27</td>\n",
       "      <td>2023-06-06 09:59:58.679000-07:00</td>\n",
       "      <td>2023-06-06 09:59:59.444000-07:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2860463 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         warehouse_id                        era_start  \\\n",
       "0                   4 2023-02-17 11:39:08.580000-08:00   \n",
       "1                   4 2023-02-17 11:39:09.963000-08:00   \n",
       "2                   4 2023-02-17 11:39:10.546000-08:00   \n",
       "3                  19 2023-02-17 11:39:10.942000-08:00   \n",
       "4                  19 2023-02-17 11:39:16.967000-08:00   \n",
       "...               ...                              ...   \n",
       "2860458            19 2023-06-06 09:59:42.122000-07:00   \n",
       "2860459            19 2023-06-06 09:59:43.604000-07:00   \n",
       "2860460             4 2023-06-06 09:59:56.708000-07:00   \n",
       "2860461             4 2023-06-06 09:59:57.970000-07:00   \n",
       "2860462            27 2023-06-06 09:59:58.679000-07:00   \n",
       "\n",
       "                                 era_end  \n",
       "0       2023-02-17 11:39:08.782000-08:00  \n",
       "1       2023-02-17 11:39:10.076000-08:00  \n",
       "2       2023-02-17 11:39:17.920000-08:00  \n",
       "3       2023-02-17 11:39:13.754000-08:00  \n",
       "4       2023-02-17 11:39:19.862000-08:00  \n",
       "...                                  ...  \n",
       "2860458 2023-06-06 09:59:42.344000-07:00  \n",
       "2860459 2023-06-06 09:59:55.034000-07:00  \n",
       "2860460 2023-06-06 09:59:57.429000-07:00  \n",
       "2860461 2023-06-06 09:59:58.526000-07:00  \n",
       "2860462 2023-06-06 09:59:59.444000-07:00  \n",
       "\n",
       "[2860463 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 12.6 s (started: 2023-06-06 18:08:49 -04:00)\n"
     ]
    }
   ],
   "source": [
    "snow_df = session.sql(sql)\n",
    "df = snow_df.to_pandas()\n",
    "# display(df.to_pandas())\n",
    "df.columns = df.columns.str.lower()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a78e658b-42c5-4ed6-96ab-ccc10f0d2bbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 3.24 ms (started: 2023-06-06 18:09:02 -04:00)\n"
     ]
    }
   ],
   "source": [
    "import snowflake.snowpark.functions as F\n",
    "import snowflake.snowpark.dataframe\n",
    "\n",
    "polling_sec = 1\n",
    "# where warehouse_id in (\n",
    "#     select warehouse_id from (\n",
    "#         select top 10\n",
    "#             warehouse_id,\n",
    "#             warehouse_name,\n",
    "#             sum(dollars_used)\n",
    "#         from warehouse_metering_history_enriched\n",
    "#         group by 1,2 order by 3 desc\n",
    "#     )\n",
    "\n",
    "\n",
    "def greedy_suspend(query_eras, autosuspend_sec):\n",
    "    started = False\n",
    "    resumed_on = None\n",
    "    previous_end = None\n",
    "    dt = Timedelta(seconds=autosuspend_sec)\n",
    "    events = []\n",
    "    for (start, end) in query_eras:\n",
    "        # first process events between previous_end and start\n",
    "        # suspend happens at: \n",
    "        # previous_end + 2*polling_period, as long as we're after resumed_on + 60 + 2*polling_period, \n",
    "        if started:\n",
    "            timediff = start - previous_end \n",
    "            if timediff > dt:\n",
    "                suspend_time = previous_end + dt\n",
    "                started = False\n",
    "                events.append((suspend_time, 'suspend'))\n",
    "\n",
    "        if not started:\n",
    "            started = True\n",
    "            resumed_on = start\n",
    "            events.append((resumed_on, 'resume'))\n",
    "\n",
    "        previous_end = end\n",
    "    \n",
    "    if started:\n",
    "        suspend_time = previous_end + dt\n",
    "        started = False\n",
    "        events.append((suspend_time, 'suspend')) \n",
    "        \n",
    "    events_df = pd.DataFrame(events, columns=['timestamp', 'event_type'])\n",
    "    return events_df\n",
    "\n",
    "def snowflake_suspend(query_eras, autosuspend_sec):\n",
    "#     dt = Timedelta(seconds=autosuspend_sec)\n",
    "#     sixty = Timedelta(seconds=60)\n",
    "    return greedy_suspend(query_eras, max(autosuspend_sec, 30))\n",
    "\n",
    "def smart_suspend(query_eras, polling_sec):\n",
    "    started = False\n",
    "    running = 0\n",
    "    resumed_on = None\n",
    "\n",
    "    previous_end = None\n",
    "    resumed_plus_sixty = None\n",
    "    events = []\n",
    "    for (start, end) in query_eras:\n",
    "        \n",
    "        # first process events between previous_end and start\n",
    "        # suspend happens at: \n",
    "        # previous_end + 2*polling_period, as long as we're after resumed_on + 60 + 2*polling_period, \n",
    "\n",
    "        if started:\n",
    "            first_check_after_previous_end = previous_end + 2*Timedelta(seconds=polling_sec)\n",
    "            earliest_possible_suspend = max(first_check_after_previous_end, resumed_plus_sixty)\n",
    "            if earliest_possible_suspend < start:\n",
    "                started = False\n",
    "                events.append((earliest_possible_suspend, 'suspend'))\n",
    "\n",
    "        if not started:\n",
    "            started = True\n",
    "            resumed_on = start\n",
    "            resumed_plus_sixty = resumed_on + Timedelta(seconds=60)\n",
    "            events.append((resumed_on, 'resume'))\n",
    "\n",
    "        previous_end = end\n",
    "    \n",
    "    if started:\n",
    "        first_check_after_previous_end = previous_end + 2*Timedelta(seconds=polling_sec)\n",
    "        earliest_possible_suspend = max(first_check_after_previous_end, resumed_plus_sixty)\n",
    "        events.append((earliest_possible_suspend, 'suspended'))\n",
    "\n",
    "\n",
    "    events_df = pd.DataFrame(events, columns=['timestamp', 'event_type'])\n",
    "    return events_df\n",
    "\n",
    "def wh_events_sim(df):\n",
    "    warehouse_event_dfs = []\n",
    "    for warehouse_id in df.warehouse_id.unique():\n",
    "        print(warehouse_id)\n",
    "        wh_df = df[df.warehouse_id == warehouse_id].sort_values('era_start', ascending=True)\n",
    "\n",
    "        eras = list(zip(wh_df.era_start, wh_df.era_end))\n",
    "\n",
    "        strategy = {'type': 'greedy_after_one_min', 'polling_sec': polling_sec}\n",
    "\n",
    "        events_df = smart_suspend(eras, polling_sec)\n",
    "        events_df['strategy'] = str(strategy)\n",
    "        events_df['warehouse_id'] = warehouse_id\n",
    "        warehouse_event_dfs.append(events_df)\n",
    "\n",
    "        # autosuspend_sec = 60\n",
    "        for autosuspend_sec in [30, 60, 120]:\n",
    "\n",
    "            strategy = {'type': 'greedy', 'autosuspend_sec': autosuspend_sec}\n",
    "            # print(strategy)\n",
    "            events_df = greedy_suspend(eras, autosuspend_sec)\n",
    "            events_df['strategy'] = str(strategy)\n",
    "            events_df['warehouse_id'] = warehouse_id\n",
    "            warehouse_event_dfs.append(events_df)\n",
    "\n",
    "            strategy = {'type': 'snowflake', 'autosuspend_sec': autosuspend_sec}\n",
    "            # print(strategy)\n",
    "            events_df = snowflake_suspend(eras, autosuspend_sec)\n",
    "            events_df['strategy'] = str(strategy)\n",
    "            events_df['warehouse_id'] = warehouse_id\n",
    "            warehouse_event_dfs.append(events_df)\n",
    "\n",
    "    final_df = pd.concat(warehouse_event_dfs)\n",
    "\n",
    "    def uppercase_all_columns(df: snowflake.snowpark.dataframe) -> snowflake.snowpark.dataframe:\n",
    "        return df.select([F.col(column).as_(column.upper()) for column in df.columns])\n",
    "\n",
    "    final_df.timestamp = final_df.timestamp.map(str)\n",
    "\n",
    "    df = session.create_dataframe(final_df)\n",
    "    uppercase_all_columns(df).write.mode(\"overwrite\").save_as_table(\"wh_events_sim_python\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "28b83a97-2324-4aeb-9a97-0105eb8d106f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "19\n",
      "27\n",
      "23\n",
      "17\n",
      "20\n",
      "29\n",
      "28\n",
      "30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06:11:24 INFO:query: [create temporary stage /* Python:snowflake.connector.pandas_tools.write_pandas()...]\n",
      "06:11:26 INFO:query execution done\n",
      "06:11:27 INFO:query: [PUT /* Python:snowflake.connector.pandas_tools.write_pandas() */ 'file:///var/fo...]\n",
      "06:11:27 INFO:query execution done\n",
      "06:11:37 INFO:query: [CREATE FILE FORMAT \"eopgi\" /* Python:snowflake.connector.pandas_tools.write_pand...]\n",
      "06:11:38 INFO:query execution done\n",
      "06:11:38 INFO:query: [SELECT COLUMN_NAME, TYPE FROM table(infer_schema(location=>'@\"tmcjj\"', file_form...]\n",
      "06:11:38 INFO:query execution done\n",
      "06:11:38 INFO:query: [CREATE TEMPORARY TABLE IF NOT EXISTS \"SHARE_COPY\".\"KNOT_MONITORING\".\"SNOWPARK_TE...]\n",
      "06:11:39 INFO:query execution done\n",
      "06:11:39 INFO:query: [DROP FILE FORMAT IF EXISTS \"eopgi\"]\n",
      "06:11:39 INFO:query execution done\n",
      "06:11:39 INFO:query: [COPY INTO \"SHARE_COPY\".\"KNOT_MONITORING\".\"SNOWPARK_TEMP_TABLE_BYSD8LV9YY\" /* Pyt...]\n",
      "06:11:44 INFO:query execution done\n",
      "06:11:44 INFO:query: [SELECT  *  FROM (\"SHARE_COPY\".\"KNOT_MONITORING\".\"SNOWPARK_TEMP_TABLE_BYSD8LV9YY\"...]\n",
      "06:11:45 INFO:query execution done\n",
      "06:11:45 INFO:query: [SELECT  *  FROM \"SHARE_COPY\".\"KNOT_MONITORING\".\"SNOWPARK_TEMP_TABLE_BYSD8LV9YY\"]\n",
      "06:11:45 INFO:query execution done\n",
      "06:11:45 INFO:query: [CREATE  OR  REPLACE    TABLE  wh_events_sim_python AS  SELECT  *  FROM ( SELECT ...]\n",
      "06:11:48 INFO:query execution done\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2min 46s (started: 2023-06-06 18:09:02 -04:00)\n"
     ]
    }
   ],
   "source": [
    "wh_events_sim(df)\n",
    "\n",
    "# to update downstream dbt models, run this after:\n",
    "# dbt run -s warehouse_era_simulated+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16265716-01dd-4f83-9242-b1cea468bf2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # testing integrity of simulation against sql simulation\n",
    "# with sql as (\n",
    "# select \n",
    "#     strategy,\n",
    "#     warehouse_id,\n",
    "#     timestamp::timestamp_ltz as timestamp,\n",
    "#     event_type \n",
    "# from warehouse_events_history_sim \n",
    "# ),\n",
    "# python as (\n",
    "# select \n",
    "#     try_parse_json(strategy) as strategy,\n",
    "#     warehouse_id,\n",
    "#     timestamp::timestamp_ltz as timestamp,\n",
    "#     event_type\n",
    "# from wh_events_sim_python\n",
    "# ),\n",
    "# common_strategies as (\n",
    "#     select distinct strategy from sql\n",
    "#     intersect\n",
    "#     select distinct strategy from python\n",
    "# )\n",
    "# ,\n",
    "# common_warehouses as (\n",
    "#     select distinct warehouse_id from sql\n",
    "#     intersect\n",
    "#     select distinct warehouse_id from python\n",
    "# ),\n",
    "# diffs as (\n",
    "# (select * from python where strategy in (select * from common_strategies) and warehouse_id in (select * from common_warehouses)\n",
    "# minus \n",
    "# select * from sql where strategy in (select * from common_strategies) and warehouse_id in (select * from common_warehouses))\n",
    "# union\n",
    "# (select * from sql where strategy in (select * from common_strategies) and warehouse_id in (select * from common_warehouses)\n",
    "# minus \n",
    "# select * from python where strategy in (select * from common_strategies) and warehouse_id in (select * from common_warehouses))\n",
    "# )\n",
    "# -- select distinct strategy from python;\n",
    "# select \n",
    "# (select count(*) from sql) num_records_sql,\n",
    "# (select count(*) from python) num_records_python,\n",
    "# (select count(*) num_diffs from diffs) num_diffs, \n",
    "# (select array_agg(strategy) from common_strategies) common_strategies,\n",
    "# (select count(*) from common_warehouses) num_warehouses;\n",
    "\n",
    "# num_diffs should be zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d33d67e0-2ab3-4779-acb9-aae7f82c9e48",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "stop",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstop\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: stop"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 320 ms (started: 2023-04-24 00:28:38 -04:00)\n"
     ]
    }
   ],
   "source": [
    "raise ValueError('stop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5ad275b6-4c80-4da2-b636-d412b1277cee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01:18:50 INFO:Snowflake Connector for Python Version: 2.7.12, Python Version: 3.8.16, Platform: macOS-10.15.7-x86_64-i386-64bit\n",
      "01:18:50 INFO:This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.\n",
      "01:18:51 INFO:Snowflake Connector for Python Version: 2.7.12, Python Version: 3.8.16, Platform: macOS-10.15.7-x86_64-i386-64bit\n",
      "01:18:51 INFO:This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.\n",
      "01:18:52 INFO:Snowpark Session information: \n",
      "\"version\" : 1.3.0,\n",
      "\"python.version\" : 3.8.16,\n",
      "\"python.connector.version\" : 2.7.12,\n",
      "\"python.connector.session.id\" : 9098488784785414,\n",
      "\"os.name\" : Darwin\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "connected to account OMWYKHW-US_WEST_2 wh DEMO db SHARE_COPY schema KNOT with role DEV\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'eras' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m events_df \u001b[38;5;241m=\u001b[39m smart_suspend(\u001b[43meras\u001b[49m, polling_sec)\n\u001b[1;32m      2\u001b[0m events_df\n",
      "\u001b[0;31mNameError\u001b[0m: name 'eras' is not defined"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 77.6 ms (started: 2023-04-27 01:18:52 -04:00)\n"
     ]
    }
   ],
   "source": [
    "events_df = smart_suspend(eras, polling_sec)\n",
    "events_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413dcf45-9292-401f-88c8-928dd05dfcfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# query_eras = \\\n",
    "# [(Timestamp('2023-04-10 16:44:00'), Timestamp('2023-04-10 16:44:10')),\n",
    "#  (Timestamp('2023-04-10 16:44:15'), Timestamp('2023-04-10 16:44:25')),\n",
    "#  (Timestamp('2023-04-10 16:44:55'), Timestamp('2023-04-10 16:45:05')),\n",
    "#  (Timestamp('2023-04-10 16:45:15'), Timestamp('2023-04-10 16:45:25'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116707b2-0844-42d3-934d-08ec9f03b14c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pandas import Timestamp\n",
    "# expected = [\n",
    "#     {'timestamp': Timestamp('2023-04-10 16:44:00'), 'event_type': 'resume'},\n",
    "#     {'timestamp': Timestamp('2023-04-10 16:45:07'), 'event_type': 'suspend'},\n",
    "#     {'timestamp': Timestamp('2023-04-10 16:45:15'), 'event_type': 'resume'},\n",
    "#     {'timestamp': Timestamp('2023-04-10 16:46:15'), 'event_type': 'suspended'}\n",
    "# ]\n",
    "\n",
    "# expected_df = pd.DataFrame.from_dict(expected)\n",
    "# pd.testing.assert_frame_equal(expected_df, events_df)\n",
    "# # events_df.to_dict('records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5dfede2-b7a6-4e49-89e2-25d988f68483",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install duckdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1c897e-718b-499f-9487-934df0ca0f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import duckdb\n",
    "# duckdb.query(\"select * from wh_df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543837a8-65b4-4439-b86e-175d8bd16b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# duckdb.query(\"\"\"\n",
    "# select \n",
    "#     generate_series(\n",
    "#         (select min(era_start) from wh_df),\n",
    "#         (select max(era_end) from wh_df),\n",
    "#         interval 1 second\n",
    "#     )\n",
    "# \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81dcd9b2-a79e-4bbf-b511-b8e512d318c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "eras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86a067d-e882-4926-b8c4-3441641e5d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.strategy.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091d2ebf-6c1f-4661-a4f8-4771f9fb590b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369b2809-02e6-4c17-ab00-f0069d57c72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2da0379-b166-4c8b-a93e-c13c0c37a5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = \"\"\"\n",
    "with query_era as (\n",
    "    select * from sandbox.query_era\n",
    "),\n",
    "range as (\n",
    "    select \n",
    "        min(era_start) as min, \n",
    "        max(era_end) as max,\n",
    "        timediff(seconds, min, max) as num_seconds\n",
    "    from query_era\n",
    "\n",
    ")\n",
    "-- select * from range;\n",
    ",\n",
    "seconds as (\n",
    "    select\n",
    "        dateadd(\n",
    "            'second',\n",
    "            '-' || row_number() over (order by seq4() asc),\n",
    "            dateadd('second', '+1', date_trunc('second', current_timestamp()))\n",
    "        ) as second_start\n",
    "        \n",
    "        -- dateadd('second', '+1', second_start) as second_end\n",
    "        \n",
    "    from table(generator(rowcount => (3600*24 * 100)))\n",
    "),\n",
    "seconds_limited as (\n",
    "    select second_start as timestamp\n",
    "    from seconds\n",
    "    where \n",
    "        timestamp >= date_trunc(second, (select min from range)) and \n",
    "        timestamp <= date_trunc(second, (select max from range))  \n",
    "\n",
    "),\n",
    "eras_per_second as (\n",
    "-- select * from query_era;\n",
    "select\n",
    "    seconds.timestamp,\n",
    "    count(era_start) as eras_found\n",
    "from seconds_limited seconds\n",
    "left join query_era as era\n",
    "    on date_trunc('hour', seconds.timestamp) = date_trunc('hour', era.era_start) -- NEW: equi-join condition\n",
    "    -- and seconds.warehouse_id = queries.warehouse_id\n",
    "    and seconds.timestamp -- range join condition\n",
    "      between date_trunc('second', era.era_start) and date_trunc('second', era.era_end)\n",
    "group by 1\n",
    ")\n",
    "select *\n",
    "from eras_per_second\n",
    "order by timestamp asc;\n",
    "\"\"\"\n",
    "df = sql_to_df(sql)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588e9e64-7af5-4813-a638-3a66876ad41e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8950c50-32f2-4e24-af47-228e715e6d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# state = list(zip(df.timestamp, df.eras_found))\n",
    "\n",
    "# def wh_events_simulated(wh_activity_series):\n",
    "started = False\n",
    "events = []\n",
    "for (ts, active) in state:\n",
    "    # simulate auto-resume\n",
    "    if active and not started:\n",
    "        started = True\n",
    "        resumed_on = ts\n",
    "        events.append((ts, 'resumed'))\n",
    "        # logger.info(f\"resumed on {ts}\")\n",
    "\n",
    "    time_since_resumed = ts - resumed_on\n",
    "\n",
    "    # smart shutdown\n",
    "    if started:\n",
    "        if (not active) and time_since_resumed >= Timedelta(seconds=60):\n",
    "            started = False\n",
    "            # logger.info(f\"suspended on {ts}\")\n",
    "            events.append((ts, 'suspended'))\n",
    "\n",
    "events_df = pd.DataFrame(events, columns=['ts', 'type'])\n",
    "# return events_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e7caa5-6d47-482b-8e5b-de3b860d8f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024c8471-f824-4b7a-b357-f47ed0cd3b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "suspend_lags_sql = \"\"\"\n",
    "with eras as (\n",
    "    select\n",
    "        'query' as type,\n",
    "        warehouse_id,\n",
    "        warehouse_name,\n",
    "        era_start,\n",
    "        era_end\n",
    "    from query_era\n",
    "\n",
    "    union\n",
    "\n",
    "    select\n",
    "        'warehouse'as type,\n",
    "        warehouse_id,\n",
    "        warehouse_name,\n",
    "        era_start,\n",
    "        era_end\n",
    "    from warehouse_era\n",
    "    where era_start > (select min(era_start) from query_era)\n",
    "),\n",
    "enriched as (\n",
    "    select \n",
    "        row_number() over(order by warehouse_id, era_end) as era_id,\n",
    "        *,\n",
    "        -- max(era_end) over (partition by warehouse_id)\n",
    "        lag(type) over (partition by warehouse_id order by era_end) as previous_ending_type,\n",
    "        lag(era_end) over (partition by warehouse_id order by era_end) as previous_ending_time,\n",
    "        case when type = 'warehouse' and previous_ending_type = 'query' then timediff(milliseconds, previous_ending_time, era_end)/1000 else null end as suspend_lag,\n",
    "        case when type = 'query' and previous_ending_type = 'query' then timediff(milliseconds, previous_ending_time, era_start)/1000 else null end as time_since_last_query,\n",
    "        timediff(seconds, era_start, era_end) as era_seconds\n",
    "    from eras\n",
    ")\n",
    "select * from enriched\n",
    "\"\"\"\n",
    "\n",
    "df = sql_to_df(suspend_lags_sql)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5c8651-8b95-4e1f-88cc-2c60207e4766",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import plotly.express as px\n",
    "px.histogram(df[df.warehouse_name == 'DEMO_SMALL'], 'suspend_lag')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7a6b60-a9ae-48de-aa0a-6d0ae855e9f6",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "warehouses_strategies = [\n",
    "    (\"autosuspend_sixty\", \"autosuspend_sixty\"),\n",
    "    (\"smartsuspend_ten\", \"smartsuspend_ten\"),\n",
    "    (\"suspend_ten\", \"suspend_ten\"),\n",
    "    # (\"demo\", \"suspend_ten\")\n",
    "]\n",
    "\n",
    "while True:\n",
    "    time.sleep(1)\n",
    "    logger.info(\"\\n=======================================================================================================\\n\")\n",
    "\n",
    "    for (wh, strategy) in warehouses_strategies:\n",
    "        df = sql_to_df(f\"show warehouses like '{wh}'\")\n",
    "        logger.info(f\"warehouse {wh}\")\n",
    "        if (len(df) == 0):\n",
    "            logging.info(\"not found\")\n",
    "            continue\n",
    "        status = df.iloc[0].to_dict()\n",
    "        state = status['state']\n",
    "        running = status['running']\n",
    "        queued = status['queued']\n",
    "        inactive = (running + queued) == 0\n",
    "        time_since_resumed = pd.Timestamp.utcnow() - status['resumed_on']\n",
    "        logger.info(f\"state {state} | running {running} | queued {queued} | inactive {inactive} | time_since_resumed {time_since_resumed}\")\n",
    "        logger.info(\"---------------------------------------------------------------------------------------------------------------\")\n",
    "        if state == 'STARTED':\n",
    "\n",
    "            # if wh == 'autosuspend_sixty':\n",
    "            #     logging.info(\"doing nothing\")\n",
    "\n",
    "            if strategy == \"suspend_ten\":\n",
    "                if inactive:\n",
    "                    run_sql(f\"ALTER WAREHOUSE {wh} SUSPEND\")\n",
    "\n",
    "            if strategy == \"smartsuspend_ten\":\n",
    "                if inactive and time_since_resumed >= Timedelta(seconds=60):\n",
    "                    run_sql(f\"ALTER WAREHOUSE {wh} SUSPEND\")\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b7d02d-c365-4428-b7cd-dc4a43f629d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    time.sleep(1)\n",
    "    logger.info(\"\\n=======================================================================================================\\n\")\n",
    "    df = sql_to_df(\"show warehouses\")\n",
    "    \n",
    "    logger.info(f\"warehouse {wh}\")\n",
    "    if (len(df) == 0):\n",
    "        logging.info(\"not found\")\n",
    "        continue\n",
    "    status = df.iloc[0].to_dict()\n",
    "    state = status['state']\n",
    "    running = status['running']\n",
    "    queued = status['queued']\n",
    "    inactive = (running + queued) == 0\n",
    "    time_since_resumed = pd.Timestamp.utcnow() - status['resumed_on']\n",
    "    logger.info(f\"state {state} | running {running} | queued {queued} | inactive {inactive} | time_since_resumed {time_since_resumed}\")\n",
    "    logger.info(\"---------------------------------------------------------------------------------------------------------------\")\n",
    "    if state == 'STARTED':\n",
    "\n",
    "        # if wh == 'autosuspend_sixty':\n",
    "        #     logging.info(\"doing nothing\")\n",
    "\n",
    "        if strategy == \"suspend_ten\":\n",
    "            if inactive:\n",
    "                run_sql(f\"ALTER WAREHOUSE {wh} SUSPEND\")\n",
    "\n",
    "        if strategy == \"smartsuspend_ten\":\n",
    "            if inactive and time_since_resumed >= Timedelta(seconds=60):\n",
    "                run_sql(f\"ALTER WAREHOUSE {wh} SUSPEND\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3778c778-493f-48a8-90dc-0c8505a863c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "status['resumed_on']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173c0fac-7ec5-4900-bdb4-4bc6e61def7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8a1597-d561-4a4e-b291-87d1cfd508c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.Timestamp.utcnow() - status['resumed_on']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406fdb73-9d26-4767-8524-a7db1559ad0e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "autoscrollcelloutput": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
