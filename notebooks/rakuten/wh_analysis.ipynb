{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
    "    return false;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding system path\n",
    "import sys, pathlib, os\n",
    "sys.path.append(str(pathlib.Path.cwd().parent.parent))\n",
    "# sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set to show warnings only once\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setup connection to DWH\n",
    "# customer = 'KIVA'\n",
    "# schema = 'KIVA_PROD.OPTIML'\n",
    "customer = 'RAKUTEN' # Use this for testing\n",
    "schema = 'SNOwFLAKE.ACCOUNT_USAGE' # Use this for testing\n",
    "username = customer + '_USERNAME'\n",
    "password = customer + '_PASSWORD'\n",
    "account = customer + '_ACCOUNT'\n",
    "warehouse = customer + '_WAREHOUSE'\n",
    "rolename = customer + '_ROLENAME'\n",
    "\n",
    "user = os.getenv(username)\n",
    "password = os.getenv(password)\n",
    "account = os.getenv(account)\n",
    "warehouse = os.getenv(warehouse)\n",
    "rolename = os.getenv(rolename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setup pandas\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from tabulate import tabulate\n",
    "color_scheme=[\"red\",\"blue\",\"green\",\"orange\",\"purple\",\"brown\",\"pink\",\"gray\",\"olive\",\"cyan\",\"darkviolet\",\"goldenrod\",\"darkgreen\",\"chocolate\",\"lawngreen\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize connection to Snowflake and set analysis date\n",
    "from optiml.connection import SnowflakeConnConfig\n",
    "connection = SnowflakeConnConfig(username=user,\n",
    "                                 password=password,\n",
    "                                 accountname=account,\n",
    "                                 rolename=rolename,\n",
    "                                 warehousename=warehouse).create_connection()\n",
    "\n",
    "# Initialize query library\n",
    "from optiml.backend.cost_profile import CostProfile\n",
    "cqlib = CostProfile(connection, schema)\n",
    "from optiml.backend.warehouse_profile import WarehouseProfile\n",
    "wqlib = WarehouseProfile(connection, schema)\n",
    "from optiml.backend.query_profile import QueryProfile\n",
    "qqlib = QueryProfile(connection, schema)\n",
    "\n",
    "# Initialize dates\n",
    "import datetime \n",
    "# edate = datetime.date.today() - datetime.timedelta(days=1)\n",
    "# sdate = edate - datetime.timedelta(days=6)\n",
    "# edate = datetime.datetime.strptime('2022-10-04', '%Y-%m-%d').date()\n",
    "# sdate = datetime.datetime.strptime('2022-09-29', '%Y-%m-%d').date()\n",
    "edate = datetime.datetime.strptime('2023-04-07', '%Y-%m-%d').date()\n",
    "sdate = datetime.datetime.strptime('2023-03-23', '%Y-%m-%d').date()\n",
    "# edate = str(edate)\n",
    "# sdate = str(sdate)\n",
    "\n",
    "print('Customer:', customer)\n",
    "print('Schema:', schema)\n",
    "print(str(sdate), str(edate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up autoreload for libs\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%aimport optiml.backend.warehouse_profile"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimally provision the warehouse:\n",
    "- Correct size\n",
    "- Set the right auto suspend time\n",
    "- Set the right scaling policy\n",
    "- Set the right suspension time for query\n",
    "\n",
    "## What are the things to look at:\n",
    "- Time series plots for warehouse of:\n",
    "  - running load \n",
    "  - queued load \n",
    "  - credit consumption\n",
    "  - % of time the warehouse was on during and hour\n",
    "  - Number of clusters running at any point in time\n",
    "- Queries haveing max resource utilization for the wh:\n",
    "  - compilation time\n",
    "  - execution time\n",
    "  - ...\n",
    "  - spillovers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Scaling policy for DEV_WH and dither queries to other places when WH query load is low\n",
    "## Analyze which queries are triggering a queue in PROD_WH.\n",
    "## Analyze if there is opportunity to dither queries between ML_WH, DAILY_REFRESH_WH and DEV_WH\n",
    "## Us there a specific user who is triggering a queue in PROD_WH, DEV_WH\n",
    "## Is there a specific user whose queries are going to DEV_WH during quiet times?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "wh_names_lists = wqlib.wh_names(sdate, edate).values\n",
    "wh_names = list(itertools.chain(*wh_names_lists))\n",
    "max_consumption_wh_names = wh_name = [\"ATSCALE_WH\", \"SALES_BU_WH\", \"PLATFORM_DE_WH\", \n",
    "                                      \"ACQUI_MARKETING_BU_WH\",\"PLATFORM_DE_WH_L\", \"RET_MARKETING_BU_WH\",\n",
    "                                        \"ACQUI_MARKETING_BI_WH\", \"PLATFORM_DE_WH_SOC\", \"MEMBER_PROFILE_DE_WH_L\", \n",
    "                                        \"SHOPSTYLE_DE_CUBE_WH\" ]\n",
    "\n",
    "wh_names_set = set(wh_names)\n",
    "max_consumption_wh_names_set = set(max_consumption_wh_names)\n",
    "\n",
    "wh_names_analysis = list(max_consumption_wh_names_set.intersection(wh_names_set))\n",
    "wh_names_analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Warehouse profiling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Warehouse efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta = 'hour'\n",
    "# warehouse_name = 'SHOPSTYLE_DE_CUBE_WH'\n",
    "# warehouse_name = 'MEMBER_PROFILE_DE_WH_L'\n",
    "# warehouse_name = 'RET_MARKETING_BU_WH'\n",
    "# warehouse_name = 'ATSCALE_WH'\n",
    "# warehouse_name =  'PLATFORM_DE_WH_L',\n",
    "# warehouse_name =  'PLATFORM_DE_WH_SOC',\n",
    "# warehouse_name = 'SALES_BU_WH',\n",
    "warehouse_name =  'PLATFORM_DE_WH'\n",
    "# warehouse_name = 'ACQUI_MARKETING_BI_WH',\n",
    "# warehouse_name = 'ACQUI_MARKETING_BU_WH'\n",
    "\n",
    "# warehouse_name = wh_names_analysis[0] ## Candidate - could easily drop a size without moving any queries around\n",
    "# warehouse_name = wh_names_analysis[1] ## Candidate - I would say this is appropriately provisioned given ratios of 3 buckets\n",
    "# warehouse_name = wh_names_analysis[2] ## Candidate - could easily move 1 query and drop the WH size by 3x (2x will do)\n",
    "# warehouse_name = wh_names_analysis[3] ## Candidate - could easily move 1 query and drop the WH size by 3x (2x will do)\n",
    "# warehouse_name = wh_names_analysis[4] ## Candidate - could easily drop a size without moving any queries around\n",
    "# warehouse_name = wh_names_analysis[5] ## Candidate - could easily drop a size without moving any queries around\n",
    "# warehouse_name = wh_names_analysis[6] ## Maybe Candidate - could easily drop a size without moving any queries around, compilation time seems high for queries in this WH\n",
    "# warehouse_name = wh_names_analysis[7] ## Candidate - could easily move 1 query and drop the WH size by 3x (2x will do)\n",
    "# warehouse_name = wh_names_analysis[8] ## Maybe Candidate - could easily drop a size without moving any queries around\n",
    "# warehouse_name = wh_names_analysis[9] ## May be correctly provisioned - move smaller queries elsewhere and use for only larger queries, shut down aggressively\n",
    "df = wqlib.wh_load_and_efficiency(start_date=sdate, end_date=edate, warehouse_name=warehouse_name,delta=delta)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interpolate hourly time column\n",
    "# Set the 'Time' column as the index\n",
    "df.set_index('hourly_start_time', inplace=True)\n",
    "# Interpolate missing values\n",
    "df = df.resample('1H').asfreq()\n",
    "df = df.apply(lambda col: col.fillna(0) if col.dtype.kind in 'biufc' else col.fillna(warehouse_name))\n",
    "df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(warehouse_name)\n",
    "trace1 = go.Bar(\n",
    "            x=df[\"hourly_start_time\"], y=df[\"avg_queued_load\"],\n",
    "            name='Average Queued Load',marker=dict(color='rgb(222,0,0)')\n",
    "        )\n",
    "trace2 = go.Bar(\n",
    "    x=df[\"hourly_start_time\"], y=df[\"avg_running_load\"],\n",
    "    name='Average Running load',marker=dict(color='rgb(0,0,255)')\n",
    ")\n",
    "trace3 = go.Scatter(\n",
    "    x=df['hourly_start_time'] ,y=df['avg_credits'],\n",
    "    name='Average Credits', mode='lines+markers',\n",
    ")\n",
    "\n",
    "trace4 = go.Scatter(\n",
    "    x=df['hourly_start_time'] ,y=df['avg_efficiency'],\n",
    "    name='Average Efficiency', mode='lines+markers',\n",
    ")\n",
    "\n",
    "f = make_subplots(specs=[[{\"secondary_y\": True}]])\n",
    "f.add_trace(trace1, secondary_y=False)\n",
    "f.add_trace(trace2, secondary_y=False)\n",
    "f.update_layout(barmode='stack')\n",
    "f.add_trace(trace3, secondary_y=True)\n",
    "f.update_layout(\n",
    "    xaxis_title=\"Hourly start time (UTC)\",\n",
    "    xaxis=dict(showgrid=False),\n",
    "    yaxis=dict(showgrid=False)\n",
    ")\n",
    "f.update_yaxes(title_text=\"Query Load\", secondary_y=False)\n",
    "f.update_yaxes(title_text=\"Credits\", secondary_y=True)\n",
    "f.update_yaxes(rangemode=\"tozero\", secondary_y=True)\n",
    "\n",
    "f.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytz\n",
    "# convert timestamp column to datetime format with timezone\n",
    "df['hourly_start_time'] = pd.to_datetime(df['hourly_start_time'])\n",
    "df['hourly_start_time'] = df['hourly_start_time'].dt.tz_convert(pytz.timezone('America/Los_Angeles'))\n",
    "\n",
    "# specify the date of interest\n",
    "day_of_interest_naive = pd.to_datetime('2023-03-29')\n",
    "la_tz = pytz.timezone('America/Los_Angeles')\n",
    "day_of_interest = la_tz.localize(day_of_interest_naive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter rows before and after the day of interest\n",
    "df_before = df[df['hourly_start_time'] < day_of_interest]\n",
    "df_after = df[df['hourly_start_time'] >= day_of_interest]\n",
    "\n",
    "# calculate mean of the value column for each group\n",
    "mean_before = df_before['avg_credits'].mean()\n",
    "mean_after = df_after['avg_credits'].mean()\n",
    "# calculate mean of the value column for each group\n",
    "std_before = df_before['avg_credits'].std()\n",
    "std_after = df_after['avg_credits'].std()\n",
    "\n",
    "print('Stats before', day_of_interest.date())\n",
    "print('Mean: ', round(mean_before,2), ', Std: ', round(std_before,2))\n",
    "print('Stats after', day_of_interest.date())\n",
    "print('Mean: ', round(mean_after,2), ', Std: ', round(std_after,2))\n",
    "print('% reduction in credit consumption:', round((mean_before-mean_after)/mean_before*100,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Queries using most credits\n",
    "df= qqlib.warehouse_resource_utilization(start_date=sdate, end_date=edate, warehouse_name=warehouse_name)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_to_min = 1./1000./60.\n",
    "scaling_factors = {\"total_elapsed_time\": scale_to_min, \n",
    "                   \"compilation_time\": scale_to_min, \n",
    "                   \"execution_time\": scale_to_min,\n",
    "                   \"queued_provisioning_time\": scale_to_min,\n",
    "                   \"queued_repair_time\": scale_to_min,\n",
    "                   \"queued_overload_time\": scale_to_min,\n",
    "                   \"transaction_blocked_time\": scale_to_min,\n",
    "                   \"list_external_files_time\": scale_to_min}\n",
    "\n",
    "# scaled_df = df.copy()\n",
    "df.loc[:, list(scaling_factors.keys())] = df.loc[:, list(scaling_factors.keys())].multiply(pd.Series(scaling_factors), axis=1)\n",
    "df[\"active_time\"] = df[\"execution_time\"] + df[\"compilation_time\"]\n",
    "df.sort_values(\"total_elapsed_time\",inplace=True, ascending=False)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Trimodal query groups\n",
    "time_elapsed_breakpoint_1 = 1 ## 1 min\n",
    "time_elapsed_breakpoint_2 = 60 ## 60 min\n",
    "nbins = 100\n",
    "metric = \"active_time\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Short queries < 1 min, Medium 1-60 min, long > 60 min\n",
    "df_short = df[df[metric] <= time_elapsed_breakpoint_1]\n",
    "df_medium = df[(df[metric] > time_elapsed_breakpoint_1) & (df[metric] <= time_elapsed_breakpoint_2)]\n",
    "df_long = df[df[metric] > time_elapsed_breakpoint_2]\n",
    "print(f\"Short queries < {time_elapsed_breakpoint_1} min: {len(df_short)}\") \n",
    "print(f\"Medium queries > {time_elapsed_breakpoint_1} min and < {time_elapsed_breakpoint_2} min: {len(df_medium)}\") \n",
    "print(f\"Long queries > {time_elapsed_breakpoint_2} min: {len(df_long)}\") \n",
    "    #   len(df_medium), len(df_long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metric = \"total_elapsed_time\"\n",
    "fig = px.histogram(df, x=metric, nbins=nbins)\n",
    "fig.update_layout(xaxis_title=metric + ' (min)')\n",
    "fig_short = px.histogram(df_short, x=metric, nbins=nbins)\n",
    "fig_short.update_layout(xaxis_title=metric + ' (min)')\n",
    "fig_medium = px.histogram(df_medium, x=metric, nbins=nbins)\n",
    "fig_medium.update_layout(xaxis_title=metric + ' (min)')\n",
    "fig_long = px.histogram(df_long, x=metric, nbins=nbins)\n",
    "fig_long.update_layout(xaxis_title=metric + ' (min)')\n",
    "fig.show()\n",
    "fig_short.show()\n",
    "fig_medium.show()\n",
    "fig_long.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_long.sort_values(\"active_time\", inplace=True, ascending=False)\n",
    "fig = px.bar(df_long, x=\"query_id\", y=[\"compilation_time\", \n",
    "                                       \"execution_time\",\n",
    "                                        \"queued_provisioning_time\",\n",
    "                                        \"queued_repair_time\",\n",
    "                                        \"queued_overload_time\",\n",
    "                                        \"transaction_blocked_time\",\n",
    "                                        \"list_external_files_time\"], title=\"Longest running queries\")\n",
    "fig.update_layout(yaxis_title='Total Elapsed Time (min)')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_medium.sort_values(\"active_time\", inplace=True, ascending=False)\n",
    "fig = px.bar(df_medium[0:400], x=\"query_id\", y=[\"compilation_time\", \n",
    "                                       \"execution_time\",\n",
    "                                        \"queued_provisioning_time\",\n",
    "                                        \"queued_repair_time\",\n",
    "                                        \"queued_overload_time\",\n",
    "                                        \"transaction_blocked_time\",\n",
    "                                        \"list_external_files_time\"], title=\"Queries running medium time\")\n",
    "fig.update_layout(yaxis_title='Total Elapsed Time (min)')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "fcbd4ad66d969ea49516a1cf27383420b67e9e950ebdd1bbb64e01b736f968b6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
