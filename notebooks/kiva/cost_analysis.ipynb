{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
    "    return false;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding system path\n",
    "import sys, pathlib\n",
    "sys.path.append(str(pathlib.Path.cwd().parent.parent))\n",
    "# sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set to show warnings only once\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up displays\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "from dash import Dash,html,dcc,Input,Output\n",
    "app = Dash(__name__)\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from tabulate import tabulate\n",
    "color_scheme=[\"red\",\"blue\",\"green\",\"orange\",\"purple\",\"brown\",\"pink\",\"gray\",\"olive\",\"cyan\",\"darkviolet\",\"goldenrod\",\"darkgreen\",\"chocolate\",\"lawngreen\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up autoreload for libs\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%aimport optiml.queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize connection to Snowflake and set analysis date\n",
    "from optiml.connection import SnowflakeConnConfig\n",
    "connection = SnowflakeConnConfig(accountname='jg84276.us-central1.gcp',warehousename=\"XSMALL_WH\").create_connection()\n",
    "# Initialize local environment\n",
    "import os\n",
    "cache_dir = os.path.expanduser('~/data/kiva')\n",
    "# Initialize query library\n",
    "from optiml.queries import SNFLKQuery\n",
    "qlib = SNFLKQuery(connection, 'KIV', cache_dir)\n",
    "# Initialize analysis dates\n",
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "sdate = '2022-09-12'\n",
    "edate = '2022-10-12'\n",
    "\n",
    "\n",
    "# Most recent rolling month that we have data for\n",
    "# print(f\"The analysis is carried our for date range {sdate} to {edate}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Total cost breakdown "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis setup\n",
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "* Analysis date range: '2022-09-12' to '2022-10-12': last rolling month in the data we collected.\n",
    "\n",
    "* Type of Snowflake account: Standard Edition\n",
    "\n",
    "* Credit to dollar conversion: `$`2 per credit\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function for date time analysis\n",
    "##TODO: Move to a library function\n",
    "def get_previous_dates(sdate, edate, date_shift_months):\n",
    "    sdate_datetime = datetime.strptime(sdate,'%Y-%m-%d')\n",
    "    prev_sdates_datetime = datetime.strptime(sdate,'%Y-%m-%d') - relativedelta(months=date_shift_months)\n",
    "    prev_sdates = prev_sdates_datetime.strftime(\"%Y-%m-%d\")\n",
    "    edate_datetime = datetime.strptime(edate,'%Y-%m-%d')\n",
    "    prev_edates_datetime = datetime.strptime(edate,'%Y-%m-%d') - relativedelta(months=date_shift_months)\n",
    "    prev_edates = prev_edates_datetime.strftime(\"%Y-%m-%d\")\n",
    "    return prev_sdates, prev_edates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost by usage category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = qlib.total_cost_breakdown_ts(sdate, edate)\n",
    "df = df.fillna('Unassigned')\n",
    "df_by_usage_category = df.groupby(\"category_name\").sum(\"numeric_only\").reset_index()\n",
    "df_by_usage_category.loc[len(df_by_usage_category.index)] = ['Total', df_by_usage_category['credits'].sum(), df_by_usage_category['dollars'].sum()]\n",
    "df_by_usage_category = df_by_usage_category.round(2)\n",
    "print('Credit and dollar usage by category (Current month)')\n",
    "print('---------------------------------------------------')\n",
    "print(tabulate(df_by_usage_category, headers='keys', tablefmt='rounded_outline', showindex=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get usage for previous month as a predictive sanity check\n",
    "p1_sdate, p1_edate = get_previous_dates(sdate, edate, 1)\n",
    "df_prev = qlib.total_cost_breakdown_ts(p1_sdate, p1_edate)\n",
    "df_prev = df_prev.fillna('Unassigned')\n",
    "df_by_usage_category_prev = df_prev.groupby(\"category_name\").sum(\"numeric_only\").reset_index()\n",
    "df_by_usage_category_prev.loc[len(df_by_usage_category_prev.index)] = ['Total', df_by_usage_category_prev['credits'].sum(), \n",
    "                                                                       df_by_usage_category_prev['dollars'].sum()]\n",
    "df_by_usage_category_prev = df_by_usage_category_prev.round(2)\n",
    "print('Credit and dollar usage by category (Previous month)')\n",
    "print('----------------------------------------------------')\n",
    "print(tabulate(df_by_usage_category_prev, headers='keys', tablefmt='rounded_outline', showindex=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_change = pd.DataFrame().assign(category_name=df_by_usage_category[\"category_name\"])\n",
    "df_change[\"percent_change\"] = ((df_by_usage_category[\"dollars\"] - df_by_usage_category_prev[\"dollars\"])/df_by_usage_category_prev[\"dollars\"]*100).round(2)\n",
    "print('Percentage change in dollar usage')\n",
    "print('---------------------------------')\n",
    "print(tabulate(df_change, headers='keys', tablefmt='rounded_outline', showindex=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pie charts for total cost breakdown\n",
    "## Remove the last row of totals for the plot\n",
    "df_by_usage_category.reset_index(inplace=True)\n",
    "df_by_usage_category.drop(columns=[\"index\"], inplace=True)\n",
    "df_by_usage_category = df_by_usage_category.drop(len(df_by_usage_category)-1) \n",
    "fig = make_subplots(\n",
    "    rows=1, cols=2,\n",
    "    specs=[[{\"type\": \"pie\"},{\"type\": \"pie\"}]],\n",
    "    subplot_titles=(\"Dollars\", \"Credits\")\n",
    ")\n",
    "\n",
    "fig.add_trace(go.Pie(labels=df_by_usage_category['category_name'].tolist(), values=df_by_usage_category['dollars'].tolist(),name=\"Dollars\", \n",
    "                     rotation=45, marker_colors=color_scheme),row=1,col=1)\n",
    "fig.add_trace(go.Pie(labels=df_by_usage_category['category_name'].tolist(), values=df_by_usage_category['credits'].tolist(),name='Credits',\n",
    "                     rotation=45, marker_colors=color_scheme),row=1,col=2)\n",
    "\n",
    "fig.update_layout(\n",
    "    title={\n",
    "        'text': \"Breakdown of total cost by usage category\",\n",
    "        'y':0.95,\n",
    "        'x':0.5,\n",
    "        'xanchor': 'center',\n",
    "        'yanchor': 'top'})\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost by usage category timeseries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_by_category_ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_by_category_ts = df.groupby(['category_name','hourly_start_time']).sum('numeric_only').reset_index()\n",
    "fig = px.area(df_by_category_ts, x=\"hourly_start_time\", y=\"dollars\", color=\"category_name\",color_discrete_sequence=color_scheme)\n",
    "fig.update_layout(\n",
    "    title={\n",
    "        'text': \"Timeseries of cost by usage category\",\n",
    "        'y':0.95,\n",
    "        'x':0.5,\n",
    "        'xanchor': 'center',\n",
    "        'yanchor': 'top'},\n",
    "    xaxis_title=\"Hourly start time (UTC)\",\n",
    "    yaxis_title=\"US Dollars\"\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_compute = df_by_category_ts[df_by_category_ts[\"category_name\"] == \"Compute\"].round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_consumption = df_compute.mean().round(2)\n",
    "max_consumption = df_compute.loc[df_compute['credits'].idxmax()]\n",
    "max_consumption.drop(\"category_name\", inplace=True)\n",
    "min_consumption = df_compute.loc[df_compute['credits'].idxmin()]\n",
    "min_consumption.drop(\"category_name\", inplace=True)\n",
    "\n",
    "print('Avg. hourly consumption:')\n",
    "print('-----------------')\n",
    "print(avg_consumption)\n",
    "\n",
    "print('')\n",
    "\n",
    "print('Max. hourly consumption:')\n",
    "print('-----------------')\n",
    "print(max_consumption)\n",
    "\n",
    "print('')\n",
    "\n",
    "print('Min. hourly consumption:')\n",
    "print('-----------------')\n",
    "print(min_consumption)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "#### Compute\n",
    "* Compute consumes > 95% of the credits budget and > 90% of the dollar budget. This is as expected.  \n",
    "* The average hourly compute consumption is Credits: 4.03, Dollars: `$`8.06, with usage peaks typically at 9 am and 2 pm with a period max of Credits: 10.09, Dollars: `$` 20.18. \n",
    "* The typical compute through the night is non-zero at ~ 3 credits. \n",
    "* The nightime compute consumption is flat till 23rd but becomes noisy starting 24th.\n",
    "* There is an increase in nightime consumption on 9th and 10th of ~30% which reverts to usual pattern subsequently. \n",
    "* The 2 pm spike in credit usage becomes very prominent on some days - can this be tied to any specific usage?\n",
    "\n",
    "#### Cloud Services\n",
    "* Cloud services in aggregate consume < 10% of the budget - which is consistent with no over charges if this is true for each warehouse on an hourly basis.\n",
    "* The overall timeseries for credit consumption of cloud services is also < 10% of compute.\n",
    "\n",
    "#### Storage\n",
    "* Storage cost is a small fraction of the `$` budget. Storage is charged by flat `$` amount per TB starting at `$`23/TB. For the timeseries we have spread that evenly through the day.\n",
    "* Storage costs are almost flat during the period.\n",
    "    \n",
    "#### General trends\n",
    "* Month over month:\n",
    "    * Total ~ +1.4%\n",
    "    * Compute ~ +2%\n",
    "    * Cloud Services ~ -7.5%\n",
    "    * Storage ~ -4.5%\n",
    "* Since Compute is majority of the expense a smaller increase in it may have a significant impact\n",
    "* This is very basic predictive analysis - more complex analyses can be made available in the pilot    \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actions and Recommendations\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "\n",
    "* Analyze Cloud Services credit consumption on an hourly basis at a warehouse level to make sure no overages hourly by warehouse.\n",
    "* Investigate noisy night time usage after Sept 23.\n",
    "* Investigate increased baseline consumption that occurs on 10th and 11th Oct.\n",
    "* Evaluate queries and tasks are driving spikes in credits and if it can be tied to individual queries.\n",
    "* Evaluate the system for compute bottlenecks and queue lengths for - warehouses, tasks, and queries at 9 am and 2 pm due to usage spikes.\n",
    "* Based on cost we expect ~10 TB storage is being used. Verify this number for consistency.\n",
    "* Set up a time varying resource monitor on the account level based on these usage patterns to flag any anomalous usage. Usage monitoring set naively:\n",
    "    * Will not be proactive about cost containment \n",
    "    * Can generate false positives/negatives causing nuisance alerts/shutdowns. \n",
    "* Evaluate opportunities for savings through:\n",
    "    * Dithering peak workloads to alternate times.\n",
    "    * Reducing weekend consumption \n",
    "    * Dithering jobs to or reducing night time consumption\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost by user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = qlib.cost_by_user_ts(sdate, edate)\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_by_user = df.groupby(['user_name']).sum('numeric_only').reset_index()\n",
    "df_by_user = df_by_user.round(2)\n",
    "df_by_user.loc[len(df_by_user.index)] = ['Total', df_by_user['approximate_credits_used'].sum()]\n",
    "print('Credit and dollar usage by user (Current month)')\n",
    "print('-----------------------------------------------')\n",
    "print(tabulate(df_by_user, headers='keys', tablefmt='rounded_outline', showindex=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_by_user[\"percent_usage\"] = df_by_user[\"approximate_credits_used\"]/df_by_user[df_by_user[\"user_name\"]==\"Total\"][\"approximate_credits_used\"].values[0]*100\n",
    "df_by_user[\"percent_usage\"] = df_by_user[\"percent_usage\"].round(3)\n",
    "x = df_by_user.loc[df_by_user[\"percent_usage\"]<1.00].sum(axis=0,numeric_only=True)\n",
    "df_low_usage_users = df_by_user.loc[df_by_user[\"percent_usage\"] < 1.00].reset_index(drop=True)\n",
    "print('Credit and dollar for low usage users (Current month)')\n",
    "print('-----------------------------------------------------')\n",
    "print(tabulate(df_low_usage_users, headers='keys', tablefmt='rounded_outline', showindex=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_by_user = df_by_user.loc[df_by_user[\"percent_usage\"] > 1.00].reset_index(drop=True)\n",
    "df_by_user.loc[len(df_by_user)-1.5] = [\"Low_usage_users\", x[\"approximate_credits_used\"], x[\"percent_usage\"]]\n",
    "df_by_user = df_by_user.sort_index().reset_index(drop=True)\n",
    "print('Credit and dollar usage by user with low usage users consolidated (Current month)')\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print(tabulate(df_by_user, headers='keys', tablefmt='rounded_outline', showindex=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_by_user.drop(df_by_user.tail(1).index,inplace=True)\n",
    "fig = make_subplots(\n",
    "    rows=1, cols=1,\n",
    "    specs=[[{\"type\": \"pie\"}]],\n",
    "    subplot_titles=(\"Credits\")\n",
    ")\n",
    "\n",
    "fig.add_trace(go.Pie(labels=df_by_user['user_name'].tolist(), values=df_by_user['approximate_credits_used'].tolist(),name=\"Credits\", rotation=270,marker_colors=color_scheme),row=1,col=1)\n",
    "# fig.add_trace(go.Pie(labels=df_by_user['user_name'].tolist(), values=df_by_user['credits'].tolist(),name='Credits', rotation=45,marker_colors=color_scheme),row=1,col=2)\n",
    "\n",
    "fig.update_layout(\n",
    "    title={\n",
    "        'text': \"Breakdown of total cost by user\",\n",
    "        'y':0.1,\n",
    "        'x':0.5,\n",
    "        'xanchor': 'center',\n",
    "        'yanchor': 'bottom'})\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_by_user_ts = df.groupby(['user_name','hourly_start_time']).sum('numeric_only').reset_index()\n",
    "df_by_user_ts = df_by_user_ts[~df_by_user_ts.user_name.isin(df_low_usage_users[\"user_name\"].values)]\n",
    "df_by_user_ts.reset_index(drop=True)\n",
    "fig = px.area(df_by_user_ts, x=\"hourly_start_time\", y=\"approximate_credits_used\", color=\"user_name\",color_discrete_sequence=color_scheme)\n",
    "fig.update_layout(\n",
    "    title={\n",
    "        'text': \"Timeseries of cost by user\",\n",
    "        'y':0.95,\n",
    "        'x':0.5,\n",
    "        'xanchor': 'center',\n",
    "        'yanchor': 'top'},\n",
    "    xaxis_title=\"Hourly start time (UTC)\",\n",
    "    yaxis_title=\"Credits used (approx.)\"\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "* User FIVETRAN_USER incurs ~ 36% of the costs at ~ 1086 credits.\n",
    "    * FIVETRAN_USER had an 80% consumption increase on Oct. 1. This was visible as a sharp peak in total consumption increase.\n",
    "* Users ML_SERVICE_DEV and DBT_PROD have next highest consumption at ~ 14.4% and ~ 430 credits.                                              \n",
    "* Following users are extremely bursty in their usage and should be dithered if possible:\n",
    "    * DBT_DEV\n",
    "    * DBT_PROD\n",
    "* FIVETRAN_USER_DEV, ML_SERVICE_DEV, VERTEX_API_PROD show a sharp increase in usage of ~ 200%, Oct 8th-10th which correlates well with increase in total usage \n",
    "* Increased noisiness starting 24th Sept. is due to correlated usage between FIVETRAN_USER and ML_SERVICE_PROD                                             \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actions and Recommendations\n",
    "<div class=\"alert alert-success\">\n",
    "\n",
    "* Similar trend analysis as total cost by usage can be provided for each user and especially for high usage users to capture increases in cost trends and set monitors against\n",
    "* FIVETRAN_USER:\n",
    "    * Has high usage which is mostly uniformly spread through the day\n",
    "    * If most queries for FIVETRAN_USER are similar size/cost/credit consumption (##TODO: Optiml, Query analysis notebook) it might warrant having a separate warehouse for it\n",
    "    * Create resource monitor to warn of 80% usage increase for the user on Oct. 1. Further analyze the causal driver.\n",
    "* Low_usage_users should be analyzed for activity. If they are inactive they should be granted reduced privileges or removed to avoid security issues (##TODO: Optiml, User analysis notebook) \n",
    "* Tagging should be made available on queries from biggest users for better cost attribution.\n",
    "* Set up a time varying resource monitor at account and user level based on usage patterns to flag any anomalous usage.    \n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost by warehouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns results only for ACCOUNTADMIN role or any other role that has been granted MONITOR USAGE global privilege\n",
    "# So results consisten with Greg's usage\n",
    "df = qlib.cost_by_wh_ts(sdate, edate)\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_by_wh = df.groupby(['warehouse_name']).sum('numeric_only').reset_index()\n",
    "df_by_wh = df_by_wh.round(2)\n",
    "df_by_wh.loc[len(df.index)] = ['Total', df_by_wh['credits'].sum(), df_by_wh['dollars'].sum(),  df_by_wh['cloud_services_credits'].sum(), df_by_wh['cloud_services_dollars'].sum()]\n",
    "print('Credit and dollar usage overall and for cloud services by warehouse (Current month)')\n",
    "print('-----------------------------------------------------------------------------------')\n",
    "print(tabulate(df_by_wh, headers='keys', tablefmt='rounded_outline', showindex=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the last row of totals for the plot\n",
    "df_by_wh.drop(df_by_wh.tail(1).index,inplace=True)\n",
    "\n",
    "fig = make_subplots(\n",
    "    rows=1, cols=1,\n",
    "    specs=[[{\"type\": \"pie\"}]],\n",
    ")\n",
    "\n",
    "fig.add_trace(go.Pie(labels=df_by_wh['warehouse_name'].tolist(), values=df_by_wh['dollars'].tolist(),name='dollars',marker_colors=color_scheme),row=1,col=1)\n",
    "\n",
    "fig.update_layout(\n",
    "    title={\n",
    "        'text': \"Breakdown of total cost by warehouse\",\n",
    "        'y':0.1,\n",
    "        'x':0.5,\n",
    "        'xanchor': 'center',\n",
    "        'yanchor': 'top'})\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_by_wh_ts = df.groupby(['warehouse_name','hourly_start_time']).sum('numeric_only').reset_index()\n",
    "# df_by_wh_ts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##TODO: Investigate why tunring off cloud services only makes daily refresh plot jump in some points\n",
    "fig = px.area(df_by_wh_ts, x=\"hourly_start_time\", y=\"credits\", color=\"warehouse_name\",color_discrete_sequence=color_scheme)\n",
    "fig.update_layout(\n",
    "    title={\n",
    "        'text': \"Timeseries of cost by warehouse\",\n",
    "        'y':0.95,\n",
    "        'x':0.5,\n",
    "        'xanchor': 'center',\n",
    "        'yanchor': 'top'},\n",
    "    xaxis_title=\"Hourly start time (UTC)\",\n",
    "    yaxis_title=\"Credits used\"\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "<div class=\"alert alert-info\">    \n",
    "  \n",
    "* PROD_WH incurs highest and > 50% of the costs at ~ 3110 credits.\n",
    "* DEV_WH consumes next highest ~30% at ~ 1872 credits.\n",
    "* DAILY_REFRESH_WH and ML_WH consume 14.6% and 3.6% respectively.\n",
    "\n",
    "* The increased credit consumption from 8th-10th Oct. is completely in DEV_WH where credit consumption increases by 200%. \n",
    "    * This increase is similar in magnitude to 11-2 pm everyday for this WH - investigate if it was the same process run intentionally or not.\n",
    "    \n",
    "* PROD_WH shows a +200% increase on 1st Oct. from 1-5 pm which shows as a spike in other views as well but was not as prominent.\n",
    "    \n",
    "* Fewer credits consumed by DAILY_REFRESH_WH on Monday and Tuesday - would have expected that to be for weekend.\n",
    "    \n",
    "* Increased noisiness starting 24th is almost entirely coming from ML_WH. This is consistent with users we identified as generating the noisiness in cost by user.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actions and Recommendations\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "    \n",
    "* Similar trend analysis as total cost by usage can be provided for each user and especially for high usage users to capture increases in cost trends and set monitors against.\n",
    "* For better visibility grant MONITOR USAGE global privilege to all users. (Or look for alternatives to make warehouse credit consumption consistent with total credit consumption).\n",
    "* Set WH level alerts for each WH - especially for DEV_WH and PROD_WH.\n",
    "* DEV_WH and DAILY_REFRESH_WH have very bursty usage - consider dithering workloads. \n",
    "* DEV_WH is constantly using credits at night - warrants further investigation. (##TODO: Optiml, Warehouse analysis notebook) \n",
    "* Tag jobs by function in PROD_WH and DEV_WH.\n",
    "* Set up a time varying resource monitor at warehouse level based on usage patterns to flag any anomalous usage.\n",
    "* Investigate increased noisiness in the ML_WH starting 24th Sept.    \n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost by Partner Tools\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=qlib.cost_by_partner_tool_ts(sdate, edate)\n",
    "# df.to_csv('/home/manas/DS_data/partner_tools.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_by_pt = df.groupby(['client_application_name']).sum('numeric_only').reset_index()\n",
    "df_by_pt = df_by_pt.round(2)\n",
    "df_by_pt.loc[len(df.index)] = ['Total', df_by_pt['approximate_credits_used'].sum()]\n",
    "print(tabulate(df_by_pt, headers='keys', tablefmt='rounded_outline', showindex=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the last row of totals for the plot\n",
    "df_by_pt.drop(df_by_pt.tail(1).index,inplace=True)\n",
    "\n",
    "fig = make_subplots(\n",
    "    rows=1, cols=1,\n",
    "    specs=[[{\"type\": \"pie\"}]],\n",
    ")\n",
    "\n",
    "fig.add_trace(go.Pie(labels=df_by_pt['client_application_name'].tolist(), values=df_by_pt['approximate_credits_used'].tolist(),name='credits',marker_colors=color_scheme, rotation=45),row=1,col=1)\n",
    "\n",
    "fig.update_layout(\n",
    "    title={\n",
    "        'text': \"Breakdown of total cost by partner tools\",\n",
    "        'y':0.1,\n",
    "        'x':0.5,\n",
    "        'xanchor': 'center',\n",
    "        'yanchor': 'top'})\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_by_pt_ts = df.groupby(['client_application_name','hourly_start_time']).sum('numeric_only').reset_index()\n",
    "# df_by_pt_ts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.area(df_by_pt_ts, x=\"hourly_start_time\", y=\"approximate_credits_used\", color=\"client_application_name\",color_discrete_sequence=color_scheme)\n",
    "fig.update_layout(\n",
    "    title={\n",
    "        'text': \"Timeseries of cost by partner tools\",\n",
    "        'y':0.95,\n",
    "        'x':0.5,\n",
    "        'xanchor': 'center',\n",
    "        'yanchor': 'top'},\n",
    "    xaxis_title=\"Hourly start time (UTC)\",\n",
    "    yaxis_title=\"Credits used (approx.)\"\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_titles=sorted(df[\"warehouse_name\"].unique())\n",
    "df_warehouse = [d for _, d in df.groupby(['warehouse_name'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(df_warehouse)):\n",
    "    fig = px.area(df_warehouse[i], x=\"hourly_start_time\", y=\"approximate_credits_used\", color=\"client_application_name\",color_discrete_sequence=color_scheme,title=df_titles[i])\n",
    "    \n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "<div class=\"alert alert-info\">\n",
    "    \n",
    "* Tools connecting through JDBC driver consume most of the credits ~1477 almost ~50%.\n",
    "* 1st Oct. usage spike is correlated with FIVETRAN_USER also using JDBC driver on PROD_WH  \n",
    "* Increased usage on 9th and 10th Oct. on DEV_WH is correlated with Python, JDBC and ODBC usage as follows based on magnitude of changes:\n",
    "    * Python: Correlates with username ML_SERVICE_DEV\n",
    "    * JDBC & ODBC: Correlates with username FIVETRAN_USER_DEV and VERTEX_API_PROD\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actions and Recommendations\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "\n",
    "* Similar trend analysis as total cost by usage can be provided for each partner tool and especially for high usage users to capture increases in cost trends and set monitors against.\n",
    "* Tag jobs associated with each partner tool better to discern which physical tool is connecting to optimize usage across entire lineage\n",
    "* Set up a time varying resource monitor at warehouse level based on usage patterns to flag any anomalous usage.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost of data transfers: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further Actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "* Implement cost of data transfers query using https://docs.snowflake.com/en/user-guide/cost-exploring-data-transfer.html\n",
    "    * Will need access to:\n",
    "        * DATA_TRANSFER_DAILY_HISTORY\n",
    "        * DATA_TRANSFER_HISTORY\n",
    "        * ORGANIZATION_USAGE ACCOUNT_USAGE\n",
    "        * DATABASE_REPLICATION_USAGE_HISTORY\n",
    "        * REPLICATION_USAGE_HISTORY\n",
    "        * REPLICATION_GROUP_USAGE_HISTORY\n",
    "        * USAGE_IN_CURRENCY_DAILY\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "fcbd4ad66d969ea49516a1cf27383420b67e9e950ebdd1bbb64e01b736f968b6"
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
