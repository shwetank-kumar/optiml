{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding system path\n",
    "import sys, pathlib\n",
    "sys.path.append(str(pathlib.Path.cwd().parent.parent))\n",
    "# sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set to show warnings only once\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up displays\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "# from dash import Dash,html,dcc,Input,Output\n",
    "# app = Dash(__name__)\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from tabulate import tabulate\n",
    "color_scheme=[\"red\",\"blue\",\"green\",\"orange\",\"purple\",\"brown\",\"pink\",\"gray\",\"olive\",\"cyan\",\"darkviolet\",\"goldenrod\",\"darkgreen\",\"chocolate\",\"lawngreen\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from dash import Dash,html,dcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##center allign all the figure outputs.\n",
    "# from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "# from IPython.display import display, HTML\n",
    "# from plotly.graph_objs import *\n",
    "# import numpy as np\n",
    "# init_notebook_mode(connected=True)\n",
    "\n",
    "# display(HTML(\"\"\"\n",
    "# <style>\n",
    "# .output {\n",
    "#     display: flex;\n",
    "#     align-items: center;\n",
    "#     text-align: center;\n",
    "# }\n",
    "# </style>\n",
    "# \"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up autoreload for libs\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%aimport optiml.queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize connection to Snowflake and set analysis date\n",
    "from optiml.connection import SnowflakeConnConfig\n",
    "connection = SnowflakeConnConfig(accountname='jg84276.us-central1.gcp',warehousename=\"XSMALL_WH\").create_connection()\n",
    "# Initialize local environment\n",
    "import os\n",
    "cache_dir = os.path.expanduser('~/data/kiva')\n",
    "# Initialize query library\n",
    "from optiml.queries import SNFLKQuery\n",
    "qlib = SNFLKQuery(connection, 'KIV', cache_dir)\n",
    "sdate = '2022-09-12'\n",
    "edate = '2022-10-12'\n",
    "print(f\"The analysis is carried our for date range {sdate} to {edate}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Most expensive queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=qlib.n_expensive_queries(sdate,edate,5)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Most expensive queries\n",
    "\n",
    "trace1  = go.Scatter(\n",
    "        mode='lines+markers',\n",
    "        x = df['query_id'],\n",
    "        y = df['credits'],\n",
    "        name=\"Credits\",\n",
    "        marker_color='crimson'\n",
    "    )\n",
    "\n",
    "# trace2 = go.Bar(\n",
    "#         x = df['query_id'],\n",
    "#         y = df['warehouse_size'],\n",
    "#         name=\"warehouse size\",\n",
    "#         yaxis='y2',\n",
    "#         marker_color ='green',\n",
    "#         marker_line_width=1.5,\n",
    "#         marker_line_color='rgb(8,48,107)',\n",
    "#         opacity=0.5\n",
    "#     )\n",
    "\n",
    "# data = [trace1, trace2]\n",
    "data = [trace1]\n",
    "\n",
    "layout = go.Layout(\n",
    "    title_text='Most expensive queries',\n",
    "    yaxis=dict(\n",
    "        # range = [0, 100],\n",
    "        title=\"Credits\"\n",
    "    ),\n",
    "    xaxis=dict(\n",
    "        title=\"Query ID\"\n",
    "    )\n",
    ")\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##TODO: @Saravana to check if query 0 and query 1 were repeated using query hash"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Cost of running a query depends on the time to execute the query and the warehouse it is run on.\n",
    "* Best practice to optimize would be to either reduce the execution time of the query or use efficient virtual warehouse practices.\n",
    "* Establishing resource monitors to estimate credit usage would be beneficial."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: @Saravana To take a look at expensive queries \n",
    "#### Saravana notes\n",
    "\n",
    "* Remove nodes column \n",
    "* Q1 - more credits, small warehouse (double wh when query hits and then decrease it)\n",
    "* Check if Q1, Q2 are running how many times\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Query 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.iloc[0][\"query_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANALYSIS\n",
    "\n",
    "* Has an execution time of 670 mins.\n",
    "* Consumes 22.33 credits.\n",
    "* Multiple JOIN statements in query.\n",
    "* Most partitions are scanned.\n",
    "* Executed using a SMALL warehouse.\n",
    "* Scans 6.3 GB of data\n",
    "* Scans 91% of data from cache\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Query 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.iloc[1][\"query_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANALYSIS\n",
    "* Execution time of 357 mins.\n",
    "* Consumes 11.92 credits\n",
    "* FAILED query.\n",
    "* Large number of bytes are spilled to both local (4719 GB) and remote (4491 GB) storage.\n",
    "* Multiple JOIN statements in query.\n",
    "* Most partitions are scanned.\n",
    "* Executed using SMALL warehouse.\n",
    "* Scans 83% of the data from cache.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Query 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.iloc[2][\"query_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANALYIS\n",
    "* Same query ran multiple times.\n",
    "* This UPDATE query also is the longest running query.\n",
    "* Has execution time of 37 mins.\n",
    "* Execution time remains constant.\n",
    "* 146 GB of bytes scanned\n",
    "* Scans 63% of total partitions\n",
    "* Only 10% of data from cache is scanned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANALYSIS AND RECOMMENDATIONS\n",
    "\n",
    "* PROD_WH can be scaled out as multicluster as large amount of data is being computed with several users using them.\n",
    "* Defining clustering keys will help with reducing number of partitions in the table being scanned.\n",
    "* Scaling up is also required for PROD_WH as bytes are being spilled causing failure of query.\n",
    "* Query 3 is expensive because it has the longest execution time.\n",
    "\n",
    "Query Optimization:\n",
    "* Querys 1 and 2 use multiple JOIN statements and WHERE predicaments. This can be eliminated and made into a Materialized View/Table.\n",
    "* QUery 3 recommendations are given under \"LONGEST RUNNING QUERIES\" section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Queries that spill to storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=qlib.n_queries_spill_to_storage(sdate,edate,5)\n",
    "# df.to_csv('/home/manas/DS_data/spill_storage.csv')\n",
    "df.head()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Most expensive queries\n",
    "\n",
    "trace1  = go.Scatter(\n",
    "        mode='lines+markers',\n",
    "        x = df['query_id'],\n",
    "        y = df['bytes_spilled_to_remote_storage'],\n",
    "        name=\"Bytes Spilled Remote\",\n",
    "        marker_color='crimson'\n",
    "    )\n",
    "\n",
    "trace2  = go.Scatter(\n",
    "        mode='lines+markers',\n",
    "        x = df['query_id'],\n",
    "        y = df['bytes_spilled_to_local_storage'],\n",
    "        name=\"Bytes Spilled Local\",\n",
    "        marker_color='purple'\n",
    "    )\n",
    "\n",
    "\n",
    "data = [trace1, trace2]\n",
    "\n",
    "\n",
    "layout = go.Layout(\n",
    "    title_text='Queries that spilled the most to storage',\n",
    "    yaxis=dict(\n",
    "        # range = [0, 100],\n",
    "        side = 'left',\n",
    "        title=\"Bytes spilled\"\n",
    "        \n",
    "    ),\n",
    "    xaxis=dict(\n",
    "        title=\"Query ID\"\n",
    "\n",
    "    )\n",
    ")\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Query 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.iloc[0][\"query_text\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANALYSIS\n",
    "* Execution time of 29 minutes.\n",
    "* Failed Query\n",
    "* 300 GB spilling to local storage and 200 GB to remote storage.\n",
    "* Most partitions are scanned in tha table.\n",
    "* 155 GB of data scanned.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Query 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.iloc[1][\"query_text\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANALYSIS\n",
    "\n",
    "* Execution time of 29 mins.\n",
    "* Failed query.\n",
    "* 170 GB splilling to local storge and 90 GB to remote.\n",
    "* Most partitions are scanned in the table.\n",
    "* 43 GB of data is scanned.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Query 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.iloc[2][\"query_text\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "### ANALYSIS\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "##TODO: @Saravana sum local and remote query spillage and plot a graph against warehouse to see which warehouse should be upgraded.\n",
    "    \n",
    " Queries 1,2 and 3 fail after execution time for each being 29 minutes.\n",
    "* Query 1 (query_id: 01a78b77-0604-f44d-0000-08d15ddd870e) - 300 GB spilling to local storage and 200 GB to remote storage.\n",
    "* Query 2 (query_id: 01a78b55-0604-f44e-0000-08d15ddd55da) - 170 GB splilling to local storge and 90 GB to remote.\n",
    "* Query 3 (query_id: 01a78b55-0604-f44e-0000-08d15ddd55d6) - 245 GB spilling to local and 78 GB to remote storage.\n",
    "\n",
    "Most partitions are scanned in the table for these 3 queries.\n",
    "There are also multiple join and select statements for each query.\n",
    "</div>\n",
    "\n",
    "\n"
=======
    "#### ANALYSIS\n",
    "* Execution time of 29 mins.\n",
    "* Failed query.\n",
    "* 245 GB spilling to local and 78 GB to remote storage.\n",
    "* Most partitions are scanned in this table.\n",
    "* 186 GB of data is scanned.\n",
    "* Same JOIN statements as QUERY 1."
>>>>>>> 8ac786d (Made changes to query notebook for Saravanan to take a look at)
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optiml TODO\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "* Check if best virtual warehouse practices can be applied to most queries in this warehouse.\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actions and Recommendation\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"alert alert-success\">\n",
    "\n",
    "* Utilize a larger warehouse - Warehouse sizes of Xsmall and Small are used. 100's of GB are spilled due to insufficient resources. \n",
    "\n",
    "* Reduce the data that is being processed (Ex: Redunant columns used in computation).\n",
    "\n",
    "* Split processing into multiple steps.\n",
    "\n",
    "* Inefficient pruning as well. Cluster keys can be defined for tables\n",
    "\n",
    "Query Optimization\n",
    "\n",
    "* Consider eliminating JOINS by making data used in JOINS as a table.\n",
    "* Use UNION ALL instead of OR for filter predicaments (WHERE clause).\n",
    "* Use UNION ALL instead of UNION.\n",
    "* Use WHERE instead of HAVING (revisit)\n",
    "\n",
    "* Instead of creating temporary tables WITH AS clause can be utilized.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: @Saravana Take a look at the queries and the analysis, combine with warehouse utilization\n",
    "#### Saravana Notes\n",
    "\n",
    "* Continous remote spill in each hour, warehouse is heavily used\n",
    "* Remote spillage is high - increase war\n",
    "* Optimize remote spilling \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Queries that scanned the most data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=qlib.n_queries_scanned_most_data(sdate,edate,5)\n",
    "# df.to_csv('/home/manas/DS_data/scanned_date.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##TODO: Put labels on the axis\n",
    "## Queries that scanned most data\n",
    "\n",
    "trace1  = go.Scatter(\n",
    "        mode='lines+markers',\n",
    "        x = df['query_id'],\n",
    "        y = df['partitions_scanned'],\n",
    "        name=\"Partitions Scanned\",\n",
    "        marker_color='crimson'\n",
    "    )\n",
    "\n",
    "data = [trace1]\n",
    "\n",
    "layout = go.Layout(\n",
    "    title_text='Queries that scanned the most partitions',\n",
    "    yaxis=dict(\n",
    "        # range = [0, 100],\n",
    "        side = 'left',\n",
    "        title=\"Bytes spilled\"\n",
    "        \n",
    "    ),\n",
    "    xaxis=dict(\n",
    "        title=\"Query ID\"\n",
    "\n",
    "    )\n",
    "    \n",
    ")\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Query 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.iloc[0][\"query_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANALYSIS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Query 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.iloc[1][\"query_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANALYSIS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Query 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.iloc[2][\"query_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANALYSIS"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Queries are the same\n",
    "* Inadequte pruning is observed. 99% of the partitions on the table are scanned.\n",
    "* Credits consumed are constant throughout, irrespective of warehouse size.\n",
    "* Execution time is 3.5 minutes for MEDIUM warehouse and is 7 minutes for SMALL warehouse.\n",
    "* When this query is run on SMALL warehouse, 17 GB is spilled onto local storage but when it is run on MEDIUM only 2GB is spilled.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actions and Recommendations"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "##TODO: @saravana Look at the wording on the recommendations and update them approrpriately - create cluster keys on time stamps\n",
    "* Queries are the same.\n",
=======
    "\n",
>>>>>>> 8ac786d (Made changes to query notebook for Saravanan to take a look at)
    "* Cluster keys can be defined to reduce partitions scanned. \n",
    "* Choose cluster key that appears frequently in a WHERE clause.\n",
    "* Create cluster key for table KIVA_PROD.DBT_MATERIALIZED.dbt_stg_snowplow_event.\n",
    "* Scaling up warehouse benifits the query as it runs faster and with same credits consumed.\n",
    "* Query can be optimization by reducing the number of JOIN statements. \n",
    "* Can create cluster key on timestamp fields"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: @Saravana To provide recommendations for reducing partitions % for specific query to improve speed of query\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Most cached queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=qlib.n_most_cached_queries(sdate,edate,5)\n",
    "df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Queries that scanned the most from cache\n",
    "\n",
    "trace1  = go.Scatter(\n",
    "        mode='lines+markers',\n",
    "        x = df['query_id'],\n",
    "        y = df['percent_scanned_from_cache'],\n",
    "        name=\"Percent Scanned From Cache\",\n",
    "        marker_color='crimson'\n",
    "    )\n",
    "\n",
    "data = [trace1]\n",
    "\n",
    "layout = go.Layout(\n",
    "    title_text='Queries that scanned the most percent from cache',\n",
    "    yaxis=dict(\n",
    "        # range = [0, 100],\n",
    "        side = 'left',\n",
    "        title=\"Percent Scanned From Cache\"\n",
    "        \n",
    "    ),\n",
    "    xaxis=dict(\n",
    "        title=\"Query ID\"\n",
    "\n",
    "    )\n",
    ")\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Query 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.iloc[2][\"query_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Query 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.iloc[1][\"query_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Query 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.iloc[2][\"query_text\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "* Most cached queries are from tables dbt_materialized.dbt_dim_login_pref_query and dbt_materialized.dbt_dim_direct_loan dl\n",
    "* Most cached queries are not the most executed ones."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actions and Recommendations\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Data from these tables should be accurate.\n",
    "* High trust on tables is necessary.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saravana notes\n",
    "\n",
    "* Make sure data is accurate, is fresh, data quality is high (query)\n",
    "* Most viewed tables (High trust on tables) - by people or downstream processes (can ask)\n",
    "* Query count \n",
    "* Check if most executed queries are in cached queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recommendations:\n",
    "\n",
    "##TODO: @saravana to word recommendations appropriately and also recommend further analysis if any.\n",
    "\n",
    "1) We need high reliability on most cached queries - since they are most used\n",
    "2) Also same holds for most executed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Most executed 'select' queries -- update this for select statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "df=qlib.n_most_executed_select_queries(sdate,edate,10)\n",
    "# df.to_csv(\"/home/manas/DS_data/most_executed_select.csv\")\n",
    "df.head() "
=======
    "df=qlib.n_most_frequently_executed_select_queries(sdate,edate,10)\n",
    "df.head()\n"
>>>>>>> 8ac786d (Made changes to query notebook for Saravanan to take a look at)
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##TODO: @saravana to put query filter rules so that this particular section surfaces the right reelvant queries for most executed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Queries that scanned the most from cache\n",
    "\n",
    "trace1  = go.Scatter(\n",
    "        mode='lines+markers',\n",
    "        x = df.index,\n",
    "        y = df['number_of_times_executed'],\n",
    "        name=\"Number of times executed\",\n",
    "        marker_color='crimson'\n",
    "    )\n",
    "\n",
    "data = [trace1]\n",
    "\n",
    "layout = go.Layout(\n",
    "    title_text='Queries that were executed the most number of times',\n",
    "    yaxis=dict(\n",
    "        # range = [0, 100],\n",
    "        side = 'left',\n",
    "        title=\"Number of times executed\"\n",
    "        \n",
    "    ),\n",
    "    xaxis=dict(\n",
    "        title=\"Query ID\"\n",
    "\n",
    "    )\n",
    ")\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Query 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(df.iloc[0][\"query_text\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANALYSIS\n",
    "* Query 1 was executed 116028 cumulatively in 30 days. It takes approximately 43 milliseconds to run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Query 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.iloc[1][\"query_text\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANALYSIS\n",
    "* Query 2 was executed 19770 in 30 days. It takes approximately 52 milliseconds to run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Query 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.iloc[3][\"query_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANALYSIS\n",
    "* Query 3 was executed 13815 in 30 days. It takes approximately 47 milliseconds to run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OPTIML Recommendations\n",
    "* Can convert SELECT Queries as Materialized Views"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saravana notes\n",
    "* Query takes this milliseconds, runs these many times, if it runs at high peak time dither it to low peak time.\n",
    "* SELECT CS, REGION - find users - most frequent queries are executed by these users\n",
    "* How many times, type of users, link cost analysis dashboard here \n",
    "* If more credits are consumed by users for these queries, build MV and caching\n",
    "* If average time is in milliseconds then it doesnt matter\n",
    "* Credits these queries consime"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MOST EXECUTED QUERIES\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=qlib.n_most_executed_queries(sdate,edate,5)\n",
    "df.to_c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LONGEST RUNNING QUERIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=qlib.longest_running_queries(sdate,edate,5)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### QUERY 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.iloc[0][\"query_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### QUERY 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.iloc[1][\"query_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### QUERY 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.iloc[2][\"query_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANALYSIS\n",
    "\n",
    "##TODO: @Saravana make a recommendation on how we recommend them to use SCD type 2 architecture"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Queries are the same.\n",
    "* Updating event query.\n",
    "* 150 GB of data is scanned by this query.\n",
    "* On average 220,000 rows are updated.\n",
    "* Execution time for query is approximately 135 minutes.\n",
    "* 65% of total partitions of table are scanned.\n",
    "* No spillage onto local or remote storage.\n",
    "* No Query overloading.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OPTIML to do"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Need to run update query to see if performance tuning can be done."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actions and Recommendations\n",
    "* Cluster keys can be defined to reduce execution time. WHERE clause present in query can be utilized.\n",
    "* Consider replacing update statement with a BULK-INSERT operation. (Delete and Insert)(Valid from and Valid To columns)(SCD type modelling)\n",
    "(Mark column as invalid)\n",
    "* If UPDATE is used, UPDATE in batches or bulk rather than row by row\n",
    "* Do updates during low peak usage times to minimize blocking of other processes.\n",
    "* Combining both JOIN and UPDATE together would prove inefficient. Do them seperately (Ask)\n",
    "* Remove index key columns from update statement. You can add them later after execution of query.\n",
    "* Try minimizing transaction size for optimizing query.\n",
    "* SCD 2 model can be implemented to preserve historical data\n",
    "* Data can be archived based on business policy.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notes from Saravanan:\n",
    "\n",
    "* SCD 2 - to keep the history (instead of update\n",
    "* General rule in DWH is not to delete/update in order not to lose history\n",
    "* We can archive the data later based on the business policy\n",
    "https://www.sqlshack.com/implementing-slowly-changing-dimensions-scds-in-data-warehouses/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes\n",
    "\n",
    "* For most expensive queries i.e. consuming most credits steps are:\n",
    "  * Look at the amount of data the queries are operating on to see if it passes the sniff test\n",
    "  * Idenitfy causes of credit consumption e.g. using INSERT instead of COPY_TO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "ba286fab723f3beadf2310fa20d811dff5d5606e9e08a95b52f51185bcb3e19b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
