{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding system path\n",
    "import sys, pathlib\n",
    "sys.path.append(str(pathlib.Path.cwd().parent.parent))\n",
    "# sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set to show warnings only once\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up displays\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "\n",
    "import math\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from tabulate import tabulate\n",
    "color_scheme=[\"red\",\"blue\",\"green\",\"orange\",\"purple\",\"brown\",\"pink\",\"gray\",\"olive\",\"cyan\",\"darkviolet\",\"goldenrod\",\"darkgreen\",\"chocolate\",\"lawngreen\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up autoreload for libs\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%aimport optiml.queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize connection to Snowflake and set analysis date\n",
    "from optiml.connection import SnowflakeConnConfig\n",
    "connection = SnowflakeConnConfig(accountname='jg84276.us-central1.gcp',warehousename=\"XSMALL_WH\").create_connection()\n",
    "# Initialize local environment\n",
    "import os\n",
    "cache_dir = os.path.expanduser('~/data/kiva')\n",
    "# Initialize query library\n",
    "from optiml.queries import SNFLKQuery\n",
    "qlib = SNFLKQuery(connection, 'KIV', cache_dir)\n",
    "sdate = '2022-09-12'\n",
    "edate = '2022-10-12'\n",
    "print(f\"The analysis is carried our for date range {sdate} to {edate}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis setup\n",
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "* Analysis date range: '2022-09-12' to '2022-10-12': last rolling month in the data we collected.\n",
    "\n",
    "* Type of Snowflake account: Standard Edition\n",
    "\n",
    "* Credit to dollar conversion: `$`2 per credit\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function for query text hashing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_query_hash(txt):\n",
    "    hash_object = hashlib.md5(txt)\n",
    "    return hash_object\n",
    "\n",
    "def get_unique_query(df):\n",
    "    queries = df[\"query_text\"].values.tolist()\n",
    "    query_hashes = [get_query_hash(q.encode()).hexdigest() for q in queries]\n",
    "    df[\"query_hash\"] = query_hashes\n",
    "    df_unique_queries = df.drop_duplicates(subset=['query_hash']).reset_index(drop=True)\n",
    "    df2 = df.groupby('query_hash',as_index=False)[['credits','execution_success','execution_fail']].sum()\n",
    "    df_unique_queries.drop(columns=[\"execution_status\",\"execution_success\",\"execution_fail\"], inplace=True)\n",
    "#     df2.drop(columns=[\"execution_status\"], inplace=True)\n",
    "    df_new = pd.merge(df_unique_queries, df2, on=[\"query_hash\"],suffixes=('_individual_max', '_total'))\n",
    "    df_new = df_new.sort_values(by='credits_total', ascending=False)\n",
    "    return df_new\n",
    "    return df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Most expensive queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = qlib.n_expensive_queries(sdate,edate,200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"execution_success\"] = (df[\"execution_status\"] == \"SUCCESS\").astype(int)\n",
    "df[\"execution_fail\"] = (df[\"execution_status\"] == \"FAIL\").astype(int)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##TODO: Temporal distribution of expensive queries by warehouse - DEV_WH 8-10: 954a79a28187b69c99ca3de1d8a106ce\n",
    "df_unique = get_unique_query(df).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Most expensive queries\n",
    "\n",
    "trace1  = go.Scatter(\n",
    "        mode='lines+markers',\n",
    "        x = df['query_id'][0:50],\n",
    "        y = df['credits'][0:50],\n",
    "        name=\"Credits\",\n",
    "        line=dict(color='black'),\n",
    "        marker=dict(\n",
    "            size=10,\n",
    "            color=(df_unique[\"execution_fail\"] >0 ).astype(int).astype('int'),\n",
    "            colorscale=[[0, 'green'], [1, 'red']]\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "data = [trace1]\n",
    "\n",
    "layout = go.Layout(\n",
    "    title_text='Naive Most expensive queries',\n",
    "    yaxis=dict(\n",
    "        title=\"Credits\"\n",
    "    ),\n",
    "    xaxis=dict(\n",
    "        title=\"Query ID\"\n",
    "    )\n",
    ")\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Most expensive queries\n",
    "\n",
    "trace1 = go.Bar(\n",
    "        x = df_unique['query_hash'],\n",
    "        y = df_unique['execution_success'],\n",
    "        name=\"Execution success count\",\n",
    "        yaxis='y2',\n",
    "        marker=dict(color='green'),\n",
    "        opacity=0.5\n",
    "    )\n",
    "\n",
    "trace2 = go.Bar(\n",
    "        x = df_unique['query_hash'],\n",
    "        y = df_unique['execution_fail'],\n",
    "        name=\"Execution fail count\",\n",
    "        yaxis='y2',\n",
    "        marker=dict(color='red'),\n",
    "        opacity=0.5\n",
    "    )\n",
    "\n",
    "trace3  = go.Scatter(\n",
    "        mode='lines+markers',\n",
    "        x = df_unique['query_hash'],\n",
    "        y = df_unique['credits_total'],\n",
    "        name=\"Credits\",\n",
    "        line=dict(color='black'),\n",
    "    )\n",
    "\n",
    "\n",
    "data = [trace1, trace2, trace3]\n",
    "\n",
    "layout = go.Layout(\n",
    "    title_text='Most expensive queries',\n",
    "    yaxis=dict(\n",
    "        title=\"Total Credits\",\n",
    "        showgrid=False,\n",
    "        range=[0, math.ceil(max(df_unique[\"credits_total\"]))+10]\n",
    "    ),\n",
    "    yaxis2=dict(\n",
    "        title=\"Number of times ran\", overlaying=\"y\",\n",
    "        side=\"right\",\n",
    "#         position=0.98,\n",
    "        showgrid=False,\n",
    "        range=[0, math.ceil(max(df_unique[\"execution_success\"] + df_unique[\"execution_fail\"]))+10]\n",
    "    ),\n",
    "    xaxis=dict(\n",
    "        title=\"Query Hash\"\n",
    "    ),\n",
    "    legend=dict(\n",
    "        yanchor=\"top\",\n",
    "        y=0.99,\n",
    "        xanchor=\"right\",\n",
    "        x=0.99\n",
    "    ),\n",
    "    barmode=\"stack\"\n",
    ")\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unique.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "    \n",
    "* Naively optimizing the most expensive query is not an effective strategy. Some queries:\n",
    "    * Fail to run to completion generating no ROI but use resources\n",
    "    * Run more frequently than others so optimizing them will lead to better ROI\n",
    "    * Queries that fail to run or cumulatively consume more credits should be optimized first\n",
    "    \n",
    "* Queries aggregated by query hash (i.e. query text matching exactly) show:\n",
    "    * Query IDs: 01a78358-0604-ec7d-0000-08d15dc9f6fe, 01a78350-0604-ec7d-0000-08d15dc9f016  might be responsible for increase in compute consumption for DEV_WH on 8th Oct. (alternatively could be WH scaling/size change as well)\n",
    "    * Query Hash: 6ef334ed61427031d52acf53a055ab57 cumulatively contributes to the most number of credits (~28) and runs to completion 12 times \n",
    "    * Query Hash: 49539ab6df6b8a1df712dbbd70efe3e3 fails 3 times it runs and costs ~3 credits\n",
    "    * Multiple queries that fail once during the analysis period\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actions and Recommendations\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "\n",
    "* High spillage -> consider a larger instance\n",
    "* Large number of partitions scanned:\n",
    "    * Consider Search Optimization enable for selective queries\n",
    "    * Consider Autoclustering enable with thoughtfully chosen cluster keys\n",
    "* SCD2 for update queries (https://community.snowflake.com/s/article/Building-a-Type-2-Slowly-Changing-Dimension-in-Snowflake-Using-Streams-and-Tasks-Part-1)\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Query 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_unique.iloc[0][\"query_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Query 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_unique.iloc[1][\"query_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Query 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_unique.iloc[2][\"query_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_unique.iloc[12][\"query_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Queries that spill to storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=qlib.n_queries_spill_to_storage(sdate,edate,5)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Most expensive queries\n",
    "\n",
    "trace1  = go.Scatter(\n",
    "        mode='lines+markers',\n",
    "        x = df['query_id'],\n",
    "        y = df['bytes_spilled_to_remote_storage'],\n",
    "        name=\"Bytes Spilled Remote\",\n",
    "        marker_color='crimson'\n",
    "    )\n",
    "\n",
    "trace2  = go.Scatter(\n",
    "        mode='lines+markers',\n",
    "        x = df['query_id'],\n",
    "        y = df['bytes_spilled_to_local_storage'],\n",
    "        name=\"Bytes Spilled Local\",\n",
    "        marker_color='purple'\n",
    "    )\n",
    "\n",
    "\n",
    "data = [trace1, trace2]\n",
    "\n",
    "\n",
    "layout = go.Layout(\n",
    "    title_text='Queries that spilled the most to storage',\n",
    "    yaxis=dict(\n",
    "        # range = [0, 100],\n",
    "        side = 'left',\n",
    "        title=\"Bytes spilled\"\n",
    "        \n",
    "    ),\n",
    "    xaxis=dict(\n",
    "        title=\"Query ID\"\n",
    "\n",
    "    )\n",
    ")\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Query 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.iloc[0][\"query_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Query 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.iloc[1][\"query_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Query 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.iloc[2][\"query_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANALYSIS\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "##TODO: @Saravana sum local and remote query spillage and plot a graph against warehouse to see which warehouse should be upgraded.\n",
    "    \n",
    " Queries 1,2 and 3 fail after execution time for each being 29 minutes.\n",
    "* Query 1 (query_id: 01a78b77-0604-f44d-0000-08d15ddd870e) - 300 GB spilling to local storage and 200 GB to remote storage.\n",
    "* Query 2 (query_id: 01a78b55-0604-f44e-0000-08d15ddd55da) - 170 GB splilling to local storge and 90 GB to remote.\n",
    "* Query 3 (query_id: 01a78b55-0604-f44e-0000-08d15ddd55d6) - 245 GB spilling to local and 78 GB to remote storage.\n",
    "\n",
    "Most partitions are scanned in the table for these 3 queries.\n",
    "There are also multiple join and select statements for each query.\n",
    "</div>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optiml TODO\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "* Check if best virtual warehouse practices can be applied to most queries in this warehouse\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optiml Recommendation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"alert alert-success\">\n",
    "\n",
    "* Utilize a larger warehouse - Warehouse sizes of Xsmall and Small are used. 100's of GB are spilled due to insufficient resources. \n",
    "\n",
    "* Optimize the query - Reducing the number of JOINS, SELECT and UNION statements would improve the performance.\n",
    "\n",
    "* Reduce the data that is being processed (Ex: Redunant columns used in computation).\n",
    "\n",
    "* Split processing into multiple steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Queries that scanned the most data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=qlib.n_queries_scanned_most_data(sdate,edate,5)\n",
    "df.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##TODO: Put labels on the axis\n",
    "## Queries that scanned most data\n",
    "\n",
    "trace1  = go.Scatter(\n",
    "        mode='lines+markers',\n",
    "        x = df['query_id'],\n",
    "        y = df['partitions_scanned'],\n",
    "        name=\"Partitions Scanned\",\n",
    "        marker_color='crimson'\n",
    "    )\n",
    "\n",
    "data = [trace1]\n",
    "\n",
    "layout = go.Layout(\n",
    "    title_text='Queries that scanned the most partitions',\n",
    "    yaxis=dict(\n",
    "        # range = [0, 100],\n",
    "        side = 'left',\n",
    "        title=\"Bytes spilled\"\n",
    "        \n",
    "    ),\n",
    "    xaxis=dict(\n",
    "        title=\"Query ID\"\n",
    "\n",
    "    )\n",
    "    \n",
    ")\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Query 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.iloc[0][\"query_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANALYSIS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Inadequte pruning is observed. 99% of the partitions on the table are scanned.\n",
    "* Execution time is 3.5 minutes.\n",
    "* 2 GB is spilled onto local storage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Query 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.iloc[1][\"query_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANALYSIS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Inadequte pruning is observed. 99% of the partitions on the table are scanned.\n",
    "* Execution time is 7.7 minutes.\n",
    "* 16.9 GB is spilled onto local storage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Query 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.iloc[2][\"query_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANALYSIS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Inadequte pruning is observed. 99% of the partitions on the table are scanned.\n",
    "* Execution time is 3.5 minutes.\n",
    "* 2 GB is spilled onto local storage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OPTIML Recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##TODO: @saravana Look at the wording on the recommendations and update them approrpriately - create cluster keys on time stamps\n",
    "* Queries are the same.\n",
    "* Cluster keys can be defined to reduce partitions scanned. \n",
    "* Choose cluster key that appears frequently in a WHERE clause\n",
    "* Scaling up warehouse would reduce bytes splilling to local storage\n",
    "* Query can be optimized by reducing number of JOIN statements, eliminating redundant SELECT statements and using DISTINCT clauses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Most cached queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=qlib.n_most_cached_queries(sdate,edate,5)\n",
    "df.head() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Queries that scanned the most from cache\n",
    "\n",
    "trace1  = go.Scatter(\n",
    "        mode='lines+markers',\n",
    "        x = df['query_id'],\n",
    "        y = df['percent_scanned_from_cache'],\n",
    "        name=\"Percent Scanned From Cache\",\n",
    "        marker_color='crimson'\n",
    "    )\n",
    "\n",
    "data = [trace1]\n",
    "\n",
    "layout = go.Layout(\n",
    "    title_text='Queries that scanned the most percent from cache',\n",
    "    yaxis=dict(\n",
    "        # range = [0, 100],\n",
    "        side = 'left',\n",
    "        title=\"Percent Scanned From Cache\"\n",
    "        \n",
    "    ),\n",
    "    xaxis=dict(\n",
    "        title=\"Query ID\"\n",
    "\n",
    "    )\n",
    ")\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Query 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.iloc[2][\"query_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Query 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.iloc[1][\"query_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Query 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.iloc[2][\"query_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recommendations:\n",
    "\n",
    "##TODO: @saravana to word recommendations appropriately and also recommend further analysis if any.\n",
    "\n",
    "1) We need high reliability on most cached queries - since they are most used\n",
    "2) Also same holds for most executed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Most executed 'select' queries -- update this for select statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=qlib.n_most_executed_select_queries(sdate,edate,10)\n",
    "# df.to_csv(\"/home/manas/DS_data/most_executed_select.csv\")\n",
    "df.head() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##TODO: @saravana to put query filter rules so that this particular section surfaces the right reelvant queries for most executed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Queries that scanned the most from cache\n",
    "\n",
    "trace1  = go.Scatter(\n",
    "        mode='lines+markers',\n",
    "        x = df.index,\n",
    "        y = df['number_of_times_executed'],\n",
    "        name=\"Number of times executed\",\n",
    "        marker_color='crimson'\n",
    "    )\n",
    "\n",
    "data = [trace1]\n",
    "\n",
    "layout = go.Layout(\n",
    "    title_text='Queries that were executed the most number of times',\n",
    "    yaxis=dict(\n",
    "        # range = [0, 100],\n",
    "        side = 'left',\n",
    "        title=\"Number of times executed\"\n",
    "        \n",
    "    ),\n",
    "    xaxis=dict(\n",
    "        title=\"Query ID\"\n",
    "\n",
    "    )\n",
    ")\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Query 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(df.iloc[0][\"query_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Query 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.iloc[1][\"query_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Query 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.iloc[3][\"query_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANALYSIS\n",
    "* Query 1 was executed 116028 in 30 days\n",
    "* Query 2 was executed 19770 in 30 days\n",
    "* Query 3 was executed 13815 in 30 days."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OPTIML Recommendations\n",
    "* Can convert SELECT Queries as Materialized Views"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MOST EXECUTED QUERIES\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=qlib.caching_warehouse(sdate,edate,5)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LONGEST RUNNING QUERIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=qlib.longest_running_queries(sdate,edate,5)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### QUERY 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.iloc[0][\"query_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### QUERY 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.iloc[1][\"query_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### QUERY 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.iloc[2][\"query_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANALYSIS\n",
    "\n",
    "##TODO: @Saravana make a recommendation on how we recommend them to use SCD type 2 architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Queries are the same.\n",
    "* Updating event query.\n",
    "* Execution time for query is approximately 135 minutes.\n",
    "* 65% of total partitions of table are scanned.\n",
    "* No spillage onto local or remote storage.\n",
    "* No Query overloading.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OPTIML to do"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Need to run update query to see if performance tuning can be done."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OPTIML Recommendations\n",
    "* Cluster keys can be defined to reduce execution time. WHERE clause present in query can be utilized.\n",
    "* Warehouse scaling up won't help with performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes\n",
    "\n",
    "* For most expensive queries i.e. consuming most credits steps are:\n",
    "  * Look at the amount of data the queries are operating on to see if it passes the sniff test\n",
    "  * Idenitfy causes of credit consumption e.g. using INSERT instead of COPY_TO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "ba286fab723f3beadf2310fa20d811dff5d5606e9e08a95b52f51185bcb3e19b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
