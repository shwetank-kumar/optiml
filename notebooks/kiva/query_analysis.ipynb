{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
    "    return false;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding system path\n",
    "import sys, pathlib, os\n",
    "sys.path.append(str(pathlib.Path.cwd().parent.parent))\n",
    "# sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set to show warnings only once\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setup connection to DWH\n",
    "# customer = 'KIVA'\n",
    "# schema = 'KIVA_PROD.OPTIML'\n",
    "customer = 'OPTIML' # Use this for testing\n",
    "schema = 'KIV.ACCOUNT_USAGE' # Use this for testing\n",
    "username = customer + '_USERNAME'\n",
    "password = customer + '_PASSWORD'\n",
    "account = customer + '_ACCOUNT'\n",
    "\n",
    "user = os.getenv(username)\n",
    "password = os.getenv(password)\n",
    "account = os.getenv(account)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setup pandas\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from tabulate import tabulate\n",
    "color_scheme=[\"red\",\"blue\",\"green\",\"orange\",\"purple\",\"brown\",\"pink\",\"gray\",\"olive\",\"cyan\",\"darkviolet\",\"goldenrod\",\"darkgreen\",\"chocolate\",\"lawngreen\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize connection to Snowflake and set analysis date\n",
    "from optiml.connection import SnowflakeConnConfig\n",
    "# connection = SnowflakeConnConfig(accountname='jg84276.us-central1.gcp',warehousename=\"XSMALL_WH\").create_connection()\n",
    "connection = SnowflakeConnConfig(username=user,password=password,accountname=account).create_connection()\n",
    "\n",
    "# Initialize query library\n",
    "from optiml.backend.query_profile import QueryProfile\n",
    "from optiml.backend.cost_profile import CostProfile, get_previous_dates\n",
    "qqlib = QueryProfile(connection, schema)\n",
    "cqlib = CostProfile(connection, schema)\n",
    "\n",
    "# Initialize dates\n",
    "import datetime \n",
    "# edate = datetime.date.today() - datetime.timedelta(days=1)\n",
    "# sdate = edate - datetime.timedelta(days=8)\n",
    "# edate = str(edate)\n",
    "# sdate = str(sdate)\n",
    "edate = '2022-10-12'\n",
    "sdate = '2022-10-05'\n",
    "print('Customer:', customer)\n",
    "print('Schema:', schema)\n",
    "print(str(sdate), str(edate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up autoreload for libs\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%aimport optiml.backend.query_profile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query Analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Failures"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Total by day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = qqlib.queries_stats_by_execution_status(sdate,edate)\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_by_day = df.groupby(['day']).agg({'n_success': 'sum', 'n_fail': 'sum', 'credits_success': 'sum', 'credits_fail': 'sum'}).reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace1 = go.Bar(\n",
    "        x = df_by_day['day'],\n",
    "        y = df_by_day['n_fail'],\n",
    "        name=\"Execution fail count\",\n",
    "    )\n",
    "\n",
    "\n",
    "trace2  = go.Scatter(\n",
    "        mode='lines+markers',\n",
    "        x = df_by_day['day'],\n",
    "        y = df_by_day['credits_fail'],\n",
    "        name=\"Credits\",\n",
    "        yaxis='y2',\n",
    "    )\n",
    "\n",
    "data = [trace1, trace2]\n",
    "\n",
    "\n",
    "layout = go.Layout(\n",
    "    title_text='Query success, fail, credits per day',\n",
    "    yaxis=dict(\n",
    "        title=\"Count number\",\n",
    "        showgrid=False,\n",
    "    ),\n",
    "     yaxis2=dict(\n",
    "        title=\"Credits\", \n",
    "        overlaying=\"y\",\n",
    "        side=\"right\",\n",
    "        showgrid=False,\n",
    "    ),\n",
    "    xaxis=dict(\n",
    "        title=\"Date (UTC)\"\n",
    "    ),\n",
    "    legend=dict(\n",
    "        yanchor=\"top\",\n",
    "        y=0.99,\n",
    "        xanchor=\"left\",\n",
    "        x=0.99\n",
    "    ),\n",
    "    barmode=\"stack\"\n",
    ")\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_success = sum(df_by_day['n_success'])\n",
    "total_fail = sum(df_by_day['n_fail'])\n",
    "pct_fail = round(total_fail/(total_fail + total_success) * 100,2)\n",
    "credits_success = round(sum(df_by_day['credits_success']),2)\n",
    "credits_fail = sum(df_by_day['credits_fail'])\n",
    "pct_credits_fail = round(credits_fail/(credits_fail + credits_success) * 100,2)\n",
    "print('Summary Stats: Credits and counts')\n",
    "print('---------------------------------')\n",
    "print('Number of queries that ran to success: ', total_success)\n",
    "print('Number of queries that ran to failure: ', total_fail)\n",
    "print('% failed queries: ', pct_fail)\n",
    "print('Credits used by queries that ran to success: ', credits_success)\n",
    "print('Credits used by queries that ran to failure: ', credits_fail)\n",
    "print('% credits due to failed queries: ', pct_credits_fail)\n",
    "print('Credits per successful query: ', round(credits_success/total_success,4))\n",
    "print('Credits per failed query: ', round(credits_fail/total_fail,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### By warehouse by day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_by_wh = df.groupby([\"warehouse_name\", \"day\"]).agg({'n_success': 'sum', 'n_fail': 'sum', 'credits_success': 'sum', 'credits_fail': 'sum'}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.bar(df_by_wh, x=\"day\", y=\"n_fail\", color=\"warehouse_name\", title=\"Number failed by warehouse\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.bar(df_by_wh, x=\"day\", y=\"credits_fail\", color=\"warehouse_name\", title=\"Credits failed by warehouse\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### By user by day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_by_user = df.groupby([\"user_name\", \"day\"]).agg({'n_success': 'sum', 'n_fail': 'sum', 'credits_success': 'sum', 'credits_fail': 'sum'}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.bar(df_by_user, x=\"day\", y=\"n_fail\", color=\"user_name\", title=\"Number failed by user\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.bar(df_by_user, x=\"day\", y=\"credits_fail\", color=\"user_name\", title=\"Credits failed by user\")\n",
    "fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 10 expensive failing queries of past week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_expensive_queries_failed = qqlib.queries_by_execution_status(sdate,edate,'FAIL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Query details for top 20 expensive failing queries')\n",
    "print('-------------------------------------------------')\n",
    "for n in range(0,20):\n",
    "    if n < len(df_expensive_queries_failed):\n",
    "        # print()\n",
    "        print('Query id:', df_expensive_queries_failed.iloc[n][\"query_id\"])\n",
    "        print('User name:', df_expensive_queries_failed.iloc[n][\"user_name\"])\n",
    "        print('Warehouse name:', df_expensive_queries_failed.iloc[n][\"warehouse_name\"])\n",
    "        print('Query credits:', df_expensive_queries_failed.iloc[n][\"credits\"])\n",
    "        print('Query text snippet:', df_expensive_queries_failed.iloc[n][\"query_text\"][0:75],'...')\n",
    "        print('---------------------------------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expensive queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = 'credits'\n",
    "df_v2 = qqlib.n_inefficient_queries_v2(sdate,edate,10,metric=metric)\n",
    "df_v2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metric = 'bytes_scanned'\n",
    "# metric = 'percentage_scanned_from_cache'\n",
    "# metric = 'bytes_spilled_to_local_storage'\n",
    "# metric = 'bytes_spilled_to_remote_storage' -- division by 0\n",
    "# metric = 'percentage_partitions_scanned' -- division by 0\n",
    "# metric = 'partitions_total'\n",
    "# metric = 'compilation_time_sec' #-- division by 0\n",
    "# metric = 'execution_time_sec'\n",
    "# metric = 'queued_provisioning_time_sec' -- division by 0\n",
    "# metric = 'queued_repair_time_sec' -- division by 0\n",
    "# metric = 'queued_overload_time_sec' -- division by 0\n",
    "# metric = 'list_external_files_time_sec' -- division by 0\n",
    "# metric = 'total_time_elapsed_sec'\n",
    "metric = 'credits'\n",
    "df = qqlib.n_inefficient_queries(sdate,edate,250,metric=metric)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unique = qqlib.get_unique_queries_with_metrics_ordered(df, metric)\n",
    "df_unique.reset_index(inplace=True)\n",
    "df_unique.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 20\n",
    "trace1 = go.Bar(\n",
    "        x = df_unique.iloc[0:n]['query_hash'],\n",
    "        y = df_unique.iloc[0:n]['n_success'],\n",
    "        name=\"Execution success count\",\n",
    "    )\n",
    "\n",
    "trace2 = go.Bar(\n",
    "        x = df_unique.iloc[0:n]['query_hash'],\n",
    "        y = df_unique.iloc[0:n]['n_fail'],\n",
    "        name=\"Execution fail count\",\n",
    "    )\n",
    "\n",
    "trace3  = go.Scatter(\n",
    "        mode='markers+lines',\n",
    "        x = df_unique.iloc[0:n]['query_hash'],\n",
    "        y = df_unique.iloc[0:n][metric],\n",
    "        name=metric,\n",
    "        yaxis='y2',\n",
    "        line=dict(color='black'),\n",
    "    )\n",
    "\n",
    "data = [trace1, trace2, trace3]\n",
    "\n",
    "layout = go.Layout(\n",
    "    title_text='Query count',\n",
    "    yaxis=dict(\n",
    "        title=\"Count number\",\n",
    "        showgrid=False,\n",
    "    ),\n",
    "     yaxis2=dict(\n",
    "        title=metric, \n",
    "        overlaying=\"y\",\n",
    "        side=\"right\",\n",
    "        showgrid=False,\n",
    "    ),\n",
    "    xaxis=dict(\n",
    "        title=\"Date (UTC)\"\n",
    "    ),\n",
    "    legend=dict(\n",
    "        yanchor=\"top\",\n",
    "        y=0.99,\n",
    "        xanchor=\"right\",\n",
    "        x=0.99\n",
    "    ),\n",
    "    barmode=\"stack\"\n",
    ")\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "fig.update_yaxes(rangemode=\"tozero\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unique.iloc[17]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query execution by user per day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = qqlib.queries_by_execution_status(sdate,edate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_by_user = df.groupby(['user_name','day']).agg({'n_success': 'sum', 'n_fail': 'sum'})\n",
    "df_by_user.reset_index(inplace=True)\n",
    "user_list = list(df_by_user[\"user_name\"].unique())\n",
    "# user_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user = 'ANALYTICS_EXERCISE_USER' #significant failures\n",
    "# user =  'BAMBOO' #significant failures\n",
    "# user =  'DBT_DEV'\n",
    "# user = 'DBT_PROD'\n",
    "# user = 'FIVETRAN_USER'\n",
    "# user = 'FIVETRAN_USER_DEV'\n",
    "# user = 'GREGORYW' #some failures\n",
    "# user = 'GREGORYW_DEV'\n",
    "user = 'KIVA_API_SNAPSHOT_PROD' #all failures\n",
    "# user = 'LOOKER_DEV_ADMIN' #significant failures\n",
    "# user = 'LOOKER_PROD' #some failures\n",
    "# user = 'LOOKER_PROD_ADMIN' #some failures\n",
    "# user = 'LOOKER_RAW_DEV' #significant failures\n",
    "# user = 'LOOKER_RAW_PROD' #significant failures\n",
    "# user = 'MAXH_DEV' #some failures\n",
    "# user = 'ML_SERVICE_DEV'\n",
    "# user = 'ML_SERVICE_PROD'\n",
    "# user = 'PATRICKL'\n",
    "# user = 'PATT' #significant failures\n",
    "# user = 'ROBS' #significant failures\n",
    "# user = 'TEST_EXERCISE_AS' #some failures\n",
    "# user = 'TEST_EXERCISE_CM' #some failures\n",
    "# user = 'TEST_EXERCISE_JAR' #some failures\n",
    "# user = 'TEST_EXERCISE_LAZ' #some failures\n",
    "# user = 'TEST_EXERCISE_RDN' #some failures\n",
    "# user = 'VERTEX_API_DEV'\n",
    "# user = 'VERTEX_API_DEV_JENKINS'\n",
    "# user = 'VERTEX_API_PROD'\n",
    "# user = 'WORKSHEETS_APP_USER'\n",
    "df_user = df_by_user[df_by_user[\"user_name\"] == user]\n",
    "df_user.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace1 = go.Bar(\n",
    "        x = df_user['day'],\n",
    "        y = df_user['n_success'],\n",
    "        name=\"Execution success count\",\n",
    "    )\n",
    "\n",
    "trace2 = go.Bar(\n",
    "        x = df_user['day'],\n",
    "        y = df_user['n_fail'],\n",
    "        name=\"Execution fail count\",\n",
    "    )\n",
    "\n",
    "data = [trace1, trace2]\n",
    "\n",
    "layout = go.Layout(\n",
    "    title_text='Query count',\n",
    "    yaxis=dict(\n",
    "        title=\"Count number\",\n",
    "        showgrid=False,\n",
    "    ),\n",
    "     yaxis2=dict(\n",
    "        title=\"Number of times ran\", \n",
    "        overlaying=\"y\",\n",
    "        side=\"right\",\n",
    "        showgrid=False,\n",
    "    ),\n",
    "    xaxis=dict(\n",
    "        title=\"Date (UTC)\"\n",
    "    ),\n",
    "    legend=dict(\n",
    "        yanchor=\"top\",\n",
    "        y=0.99,\n",
    "        xanchor=\"left\",\n",
    "        x=0.99\n",
    "    ),\n",
    "    barmode=\"stack\"\n",
    ")\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## all failures for a user between start and end date\n",
    "df=qqlib.get_queries(start_date=sdate,end_date=edate,user=user,es=\"FAIL\", n=1500)\n",
    "df_unique = qqlib.get_unique_queries(df)\n",
    "df_unique.sort_values('execution_status', inplace=True, ascending=False)\n",
    "df_unique.reset_index(inplace=True)\n",
    "df_unique.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query execution by warehouse per day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = qqlib.queries_by_execution_status(sdate,edate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_by_wh = df.groupby(['warehouse_name','day']).agg({'n_success': 'sum', 'n_fail': 'sum'})\n",
    "df_by_wh.reset_index(inplace=True)\n",
    "wh_list = list(df_by_wh[\"warehouse_name\"].unique())\n",
    "# wh_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wh = 'DAILY_REFRESH_WH'\n",
    "# wh = 'DEV_WH'\n",
    "# wh = 'ML_WH'\n",
    "# wh ='PROD_WH'\n",
    "wh = 'Unassigned'\n",
    "df_wh = df_by_wh[df_by_wh[\"warehouse_name\"] == wh]\n",
    "df_wh.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace1 = go.Bar(\n",
    "        x = df_wh['day'],\n",
    "        y = df_wh['n_success'],\n",
    "        name=\"Execution success count\",\n",
    "    )\n",
    "\n",
    "trace2 = go.Bar(\n",
    "        x = df_wh['day'],\n",
    "        y = df_wh['n_fail'],\n",
    "        name=\"Execution fail count\",\n",
    "    )\n",
    "\n",
    "data = [trace1, trace2]\n",
    "\n",
    "layout = go.Layout(\n",
    "    title_text='Query count',\n",
    "    yaxis=dict(\n",
    "        title=\"Count number\",\n",
    "        showgrid=False,\n",
    "    ),\n",
    "     yaxis2=dict(\n",
    "        title=\"Number of times ran\", \n",
    "        overlaying=\"y\",\n",
    "        side=\"right\",\n",
    "        showgrid=False,\n",
    "    ),\n",
    "    xaxis=dict(\n",
    "        title=\"Date (UTC)\"\n",
    "    ),\n",
    "    legend=dict(\n",
    "        yanchor=\"top\",\n",
    "        y=0.99,\n",
    "        xanchor=\"left\",\n",
    "        x=0.99\n",
    "    ),\n",
    "    barmode=\"stack\"\n",
    ")\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get n queries given start_date, and end_date, or user or warehouse, execution_status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdate = '2022-10-11'\n",
    "edate = '2022-10-12'\n",
    "es = 'FAIL'\n",
    "n = 1500\n",
    "wh = 'Unassigned'\n",
    "df = qqlib.get_queries(start_date=sdate,end_date=edate,es=es, n=n)\n",
    "df = df.fillna('Unassigned')\n",
    "df = df[df['warehouse_name']==wh]\n",
    "df_unique = qqlib.get_unique_queries(df)\n",
    "df_fails = df_unique.sort_values('execution_status', ascending=False).reset_index()\n",
    "df_fails.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get most inefficient query by metric - credits, total time elapsed etc. as query_id and details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metric = 'bytes_scanned'\n",
    "# metric = 'percentage_scanned_from_cache'\n",
    "# metric = 'bytes_spilled_to_local_storage'\n",
    "# metric = 'bytes_spilled_to_remote_storage' -- division by 0\n",
    "# metric = 'percentage_partitions_scanned' -- division by 0\n",
    "# metric = 'partitions_total'\n",
    "# metric = 'compilation_time_sec' #-- division by 0\n",
    "# metric = 'execution_time_sec'\n",
    "# metric = 'queued_provisioning_time_sec' -- division by 0\n",
    "# metric = 'queued_repair_time_sec' -- division by 0\n",
    "# metric = 'queued_overload_time_sec' -- division by 0\n",
    "# metric = 'list_external_files_time_sec' -- division by 0\n",
    "# metric = 'total_time_elapsed_sec'\n",
    "metric = 'credits'\n",
    "sdate = \"2022-08-12\"\n",
    "edate = \"2022-10-12\"\n",
    "df = qqlib.n_inefficient_queries(sdate,edate,250,metric=metric)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get most inefficient query by metric - credits, total time elapsed etc. with details and grouped by query text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metric = 'bytes_scanned'\n",
    "# metric = 'percentage_scanned_from_cache'\n",
    "# metric = 'bytes_spilled_to_local_storage'\n",
    "# metric = 'bytes_spilled_to_remote_storage' -- division by 0\n",
    "# metric = 'percentage_partitions_scanned' -- division by 0\n",
    "# metric = 'partitions_total'\n",
    "# metric = 'compilation_time_sec' #-- division by 0\n",
    "# metric = 'execution_time_sec'\n",
    "# metric = 'queued_provisioning_time_sec' -- division by 0\n",
    "# metric = 'queued_repair_time_sec' -- division by 0\n",
    "# metric = 'queued_overload_time_sec' -- division by 0\n",
    "# metric = 'list_external_files_time_sec' -- division by 0\n",
    "# metric = 'total_time_elapsed_sec'\n",
    "metric = 'credits'\n",
    "df = qqlib.n_inefficient_queries(sdate,edate,250,metric=metric)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unique = qqlib.get_unique_queries_with_metrics_ordered(df, metric)\n",
    "df_unique.reset_index(inplace=True)\n",
    "df_unique.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 20\n",
    "trace1 = go.Bar(\n",
    "        x = df_unique.iloc[0:n]['query_hash'],\n",
    "        y = df_unique.iloc[0:n]['n_success'],\n",
    "        name=\"Execution success count\",\n",
    "    )\n",
    "\n",
    "trace2 = go.Bar(\n",
    "        x = df_unique.iloc[0:n]['query_hash'],\n",
    "        y = df_unique.iloc[0:n]['n_fail'],\n",
    "        name=\"Execution fail count\",\n",
    "    )\n",
    "\n",
    "trace3  = go.Scatter(\n",
    "        mode='markers+lines',\n",
    "        x = df_unique.iloc[0:n]['query_hash'],\n",
    "        y = df_unique.iloc[0:n][metric],\n",
    "        name=metric,\n",
    "        yaxis='y2',\n",
    "        line=dict(color='black'),\n",
    "    )\n",
    "\n",
    "data = [trace1, trace2, trace3]\n",
    "\n",
    "layout = go.Layout(\n",
    "    title_text='Query count',\n",
    "    yaxis=dict(\n",
    "        title=\"Count number\",\n",
    "        showgrid=False,\n",
    "    ),\n",
    "     yaxis2=dict(\n",
    "        title=metric, \n",
    "        overlaying=\"y\",\n",
    "        side=\"right\",\n",
    "        showgrid=False,\n",
    "    ),\n",
    "    xaxis=dict(\n",
    "        title=\"Date (UTC)\"\n",
    "    ),\n",
    "    legend=dict(\n",
    "        yanchor=\"top\",\n",
    "        y=0.99,\n",
    "        xanchor=\"right\",\n",
    "        x=0.99\n",
    "    ),\n",
    "    barmode=\"stack\"\n",
    ")\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "fig.update_yaxes(rangemode=\"tozero\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Warning note\n",
    "\n",
    "<div class=\"alert alert-danger\">\n",
    "    \n",
    "* We do not recommend the naive approach since it can have very limited ROI\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##TODO: Temporal distribution of expensive queries by warehouse\n",
    "df_hash = qqlib.get_unique_query(df)\n",
    "# df_hash.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_by_execution_status = df_hash.groupby([\"execution_status\"])[\"execution_success\",\"execution_fail\",\"execution_count\",\"credits\"].sum().reset_index()\n",
    "\n",
    "# Pie charts for expensive queries by execution status\n",
    "fig = make_subplots(\n",
    "    rows=1, cols=2,\n",
    "    specs=[[{\"type\": \"pie\"},{\"type\": \"pie\"}]],\n",
    "    subplot_titles=(\"Count\", \"Credits\")\n",
    ")\n",
    "\n",
    "fig.add_trace(go.Pie(labels=df_by_execution_status['execution_status'].tolist(), values=df_by_execution_status['execution_count'].tolist(), \n",
    "                     name=\"Execution Count\", rotation=45, marker_colors=[\"red\",\"green\"]),row=1,col=1)\n",
    "fig.add_trace(go.Pie(labels=df_by_execution_status['execution_status'].tolist(), values=df_by_execution_status['credits'].tolist(),\n",
    "                     name='Credits', rotation=45, marker_colors=[\"red\",\"green\"]),row=1,col=2)\n",
    "\n",
    "fig.update_layout(\n",
    "    title={\n",
    "        'text': \"Expensive queries breakdown by execution status\",\n",
    "        'y':0.95,\n",
    "        'x':0.5,\n",
    "        'xanchor': 'center',\n",
    "        'yanchor': 'top'})\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_by_user = df_hash.groupby([\"user_name\"])[\"execution_success\",\"execution_fail\",\"execution_count\",\"credits\"].sum().reset_index()\n",
    "print('Credit and count of expensive queries by user')\n",
    "print('---------------------------------------------')\n",
    "print(tabulate(df_by_user, headers='keys', tablefmt='rounded_outline', showindex=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pie charts for expensive queries by execution status\n",
    "fig = make_subplots(\n",
    "    rows=1, cols=2,\n",
    "    specs=[[{\"type\": \"pie\"},{\"type\": \"pie\"}]],\n",
    "    subplot_titles=(\"Count\", \"Credits\")\n",
    ")\n",
    "\n",
    "fig.add_trace(go.Pie(labels=df_by_user['user_name'].tolist(), values=df_by_user['execution_count'].tolist(), \n",
    "                     name=\"Execution Count\", rotation=45, marker_colors=color_scheme),row=1,col=1)\n",
    "fig.add_trace(go.Pie(labels=df_by_user['user_name'].tolist(), values=df_by_user['credits'].tolist(),\n",
    "                     name='Credits', rotation=45, marker_colors=color_scheme),row=1,col=2)\n",
    "\n",
    "fig.update_layout(\n",
    "    title={\n",
    "        'text': \"Expensive queries breakdown by user\",\n",
    "        'y':0.95,\n",
    "        'x':0.5,\n",
    "        'xanchor': 'center',\n",
    "        'yanchor': 'top'})\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_by_wh = df_hash.groupby([\"warehouse_name\"])[\"execution_success\",\"execution_fail\",\"execution_count\",\"credits\"].sum().reset_index()\n",
    "print('Credit and count of expensive queries by user')\n",
    "print('---------------------------------------------')\n",
    "print(tabulate(df_by_wh, headers='keys', tablefmt='rounded_outline', showindex=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pie charts for expensive queries by execution status\n",
    "fig = make_subplots(\n",
    "    rows=1, cols=2,\n",
    "    specs=[[{\"type\": \"pie\"},{\"type\": \"pie\"}]],\n",
    "    subplot_titles=(\"Count\", \"Credits\")\n",
    ")\n",
    "\n",
    "fig.add_trace(go.Pie(labels=df_by_wh['warehouse_name'].tolist(), values=df_by_user['execution_count'].tolist(), \n",
    "                     name=\"Execution Count\", rotation=45, marker_colors=color_scheme),row=1,col=1)\n",
    "fig.add_trace(go.Pie(labels=df_by_wh['warehouse_name'].tolist(), values=df_by_user['credits'].tolist(),\n",
    "                     name='Credits', rotation=45, marker_colors=color_scheme),row=1,col=2)\n",
    "\n",
    "fig.update_layout(\n",
    "    title={\n",
    "        'text': \"Expensive queries breakdown by warehouse\",\n",
    "        'y':0.95,\n",
    "        'x':0.5,\n",
    "        'xanchor': 'center',\n",
    "        'yanchor': 'top'})\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Most expensive queries\n",
    "df_by_hash = df_hash.groupby([\"query_hash\"])[\"execution_success\",\"execution_fail\",\"credits\"].sum().reset_index().sort_values('credits', ascending=False)\n",
    "trace1 = go.Bar(\n",
    "        x = df_by_hash['query_hash'],\n",
    "        y = df_by_hash['execution_success'],\n",
    "        name=\"Execution success count\",\n",
    "        yaxis='y2',\n",
    "        marker=dict(color='green'),\n",
    "        opacity=0.5\n",
    "    )\n",
    "\n",
    "trace2 = go.Bar(\n",
    "        x = df_by_hash['query_hash'],\n",
    "        y = df_by_hash['execution_fail'],\n",
    "        name=\"Execution fail count\",\n",
    "        yaxis='y2',\n",
    "        marker=dict(color='red'),\n",
    "        opacity=0.5\n",
    "    )\n",
    "\n",
    "trace3  = go.Scatter(\n",
    "        mode='lines+markers',\n",
    "        x = df_by_hash['query_hash'],\n",
    "        y = df_by_hash['credits'],\n",
    "        name=\"Credits\",\n",
    "        line=dict(color='black'),\n",
    "    )\n",
    "\n",
    "\n",
    "data = [trace1, trace2, trace3]\n",
    "\n",
    "layout = go.Layout(\n",
    "    title_text='Most expensive queries',\n",
    "    yaxis=dict(\n",
    "        title=\"Total Credits\",\n",
    "        showgrid=False,\n",
    "        range=[0, math.ceil(max(df_by_hash[\"credits\"]))+10]\n",
    "    ),\n",
    "    yaxis2=dict(\n",
    "        title=\"Number of times ran\", overlaying=\"y\",\n",
    "        side=\"right\",\n",
    "#         position=0.98,\n",
    "        showgrid=False,\n",
    "        range=[0, math.ceil(max(df_by_hash[\"execution_success\"] + df_by_hash[\"execution_fail\"]))+10]\n",
    "    ),\n",
    "    xaxis=dict(\n",
    "        title=\"Query Hash\"\n",
    "    ),\n",
    "    legend=dict(\n",
    "        yanchor=\"top\",\n",
    "        y=0.99,\n",
    "        xanchor=\"right\",\n",
    "        x=0.99\n",
    "    ),\n",
    "    barmode=\"stack\"\n",
    ")\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_by_hash = df_hash.groupby([\"query_hash\"])[\"execution_success\",\"execution_fail\",\"execution_count\",\"credits\"].sum().reset_index()\n",
    "df_by_hash = df_by_hash.sort_values('credits', ascending=False)\n",
    "print(tabulate(df_by_hash, headers='keys', tablefmt='rounded_outline', showindex=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check times of 5 most expensive queries with WH credit usage peaks\n",
    "# q1 = df_hash[df_hash['query_hash'] == '6ef334ed61427031d52acf53a055ab57'].sort_values('start_time')\n",
    "# q1[[\"warehouse_name\",\"start_time\",\"end_time\"]]\n",
    "q2 = df_hash[df_hash['query_hash'] == '295230eae35c6270a0e240fca4a0be6f'].sort_values('start_time')\n",
    "q2[[\"query_id\", \"warehouse_name\",\"start_time\",\"end_time\"]]\n",
    "# q3 = df_hash[df_hash['query_hash'] == '233655c56c9344d5c303bdd8f4b9f264'].sort_values('start_time')\n",
    "# q3[[\"warehouse_name\",\"start_time\",\"end_time\"]]\n",
    "# q4 = df_hash[df_hash['query_hash'] == 'd154d97ba4dbd4fd677baa48d6b47b71'].sort_values('start_time')\n",
    "# q4[[\"warehouse_name\",\"start_time\",\"end_time\"]]\n",
    "# q5 = df_hash[df_hash['query_hash'] == '5cb178243f7dc6005e8141a72f9bc895'].sort_values('start_time')\n",
    "# q5[[\"warehouse_name\",\"start_time\",\"end_time\"]]\n",
    "# q6 = df_hash[df_hash['query_hash'] == '535f3564c40d364959615108ceee4ea0'].sort_values('start_time')\n",
    "# q6[[\"warehouse_name\",\"start_time\",\"end_time\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weekly change of 10 top expensive queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_n_expensive_query_hashes(sdate, edate, n):\n",
    "    df_expensive = qqlib.n_expensive_queries(sdate, edate, 200)\n",
    "    df_hash = get_unique_query(df_expensive).sort_values('credits', ascending=False)\n",
    "    hash_list = df_hash[\"query_hash\"].unique()\n",
    "    return hash_list[0:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "sdate = '2022-01-01'\n",
    "n_periods = 9\n",
    "frequency = '1M'\n",
    "sdates = pd.date_range(sdate, periods=n_periods, freq=frequency)\n",
    "edates = pd.date_range(sdates[1], periods=n_periods, freq=frequency)\n",
    "start_dates = [d.strftime('%Y-%m-%d') for d in sdates]\n",
    "end_dates = [d.strftime('%Y-%m-%d') for d in edates]\n",
    "df_hash = pd.DataFrame()\n",
    "\n",
    "for idx, s in enumerate(start_dates):\n",
    "    df_hash[s] = pd.DataFrame(get_n_expensive_query_hashes(start_dates[idx], end_dates[idx], 10), columns=[start_dates[0]])\n",
    "\n",
    "\n",
    "df_hash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = df_hash.columns.values\n",
    "repeat = []\n",
    "for idx, col in enumerate(cols[:-1]):\n",
    "    repeat.append(sum(el in df_hash[cols[idx]].to_list() for el in df_hash[cols[idx+1]].to_list()))\n",
    "\n",
    "print(repeat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 14 days\n",
    "from statistics import mean, median\n",
    "mean(repeat), median(repeat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "    \n",
    "* Naively optimizing the most expensive query is not an effective strategy. Some queries:\n",
    "    * Fail to run to completion generating no ROI but use resources\n",
    "    * Run more frequently than others so optimizing them will lead to better ROI\n",
    "* 5% of queries fail to run to completion by count but they consume ~ 6% of credits\n",
    "* Majority of expensive queries come from DEV_WH - which makes sense, but larger fraction comes from DAILY_REFRESH_WH by credits than by count which might have optimization opportunity.\n",
    "* DBT_DEV, DBT_PROD write > 90% of the top 200 most expensive queries. This is inspite of the fact that their total credit consumption is <30% and much less than that of the most expensive user i.e. FIVETRAN_USER. There may be opportunity to do DBT specific optimization for Snowflake.\n",
    "* Manually checking 5 most expensive queries showed 4 of them are consistent with peaks in warehouse usage:\n",
    "    * Query Hash: 6ef334ed61427031d52acf53a055ab57 (DAILY_REFRESH_WH, 2 pm), 295230eae35c6270a0e240fca4a0be6f (DEV_WH, 12 pm), 233655c56c9344d5c303bdd8f4b9f264 (DAILY_REFRESH_WH, 9 am), d154d97ba4dbd4fd677baa48d6b47b71 (DAILY_REFRESH_WH, 2:50 pm), 5cb178243f7dc6005e8141a72f9bc895 (DEV_WH, 1 pm)\n",
    "                                                                                                                                               \n",
    "* Queries aggregated by query hash (i.e. query text matching exactly) show:                                                                                                 \n",
    "    * Query IDs: 01a78358-0604-ec7d-0000-08d15dc9f6fe, 01a78350-0604-ec7d-0000-08d15dc9f016  might be responsible for increase in compute consumption for DEV_WH on 8th Oct. (alternatively could be WH scaling/size change as well)\n",
    "    * Query Hash: 6ef334ed61427031d52acf53a055ab57 cumulatively contributes to the most number of credits (~28) and runs to completion 12 times \n",
    "    * Query Hash: 49539ab6df6b8a1df712dbbd70efe3e3 fails 3 times it runs and costs ~3 credits\n",
    "    * Multiple queries that fail once during the analysis period\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actions and Recommendations\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "\n",
    "* ##TODO:\n",
    "    * Are these queries all current and serving a business purpose - if not remove them\n",
    "    * Look at other queries that feed reports that no one is looking at and remove them\n",
    "    * Optimize queries that fail to run or cumulatively consume more credits than the queries that consume the most credits but run fewer time\n",
    "    * For highest ROI generate query hashes using templates so that queries can be grouped irrespective of parameter values\n",
    "* For queries with higher spillage consider a larger instance\n",
    "* For queries that scan a large number of partitions consider:\n",
    "    * Search Optimization enable for selective queries\n",
    "    * Autoclustering enable with thoughtfully chosen cluster keys\n",
    "* SCD2 for update queries (https://community.snowflake.com/s/article/Building-a-Type-2-Slowly-Changing-Dimension-in-Snowflake-Using-Streams-and-Tasks-Part-1)\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hash_merged = pd.merge(df_by_hash, df_hash[['query_hash', 'query_text','user_name','warehouse_name']], on=[\"query_hash\"])\n",
    "df_hash_merged = df_hash_merged.drop_duplicates(subset=['query_hash']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search for queries by any column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_iterable = df[df['query_text'].str.contains(\"merge into \", case=False)][\"query_text\"].reset_index(drop=True)\n",
    "print(update_iterable.iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start and end time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Query 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_hash_merged.iloc[0][\"query_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Query 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_hash_merged.iloc[1][\"query_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Query 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_hash_merged.iloc[12][\"query_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Queries that spill to storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=qqlib.n_queries_spill_to_storage(sdate,edate,5)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Most expensive queries\n",
    "\n",
    "trace1  = go.Scatter(\n",
    "        mode='lines+markers',\n",
    "        x = df['query_id'],\n",
    "        y = df['bytes_spilled_to_remote_storage'],\n",
    "        name=\"Bytes Spilled Remote\",\n",
    "        marker_color='crimson'\n",
    "    )\n",
    "\n",
    "trace2  = go.Scatter(\n",
    "        mode='lines+markers',\n",
    "        x = df['query_id'],\n",
    "        y = df['bytes_spilled_to_local_storage'],\n",
    "        name=\"Bytes Spilled Local\",\n",
    "        marker_color='purple'\n",
    "    )\n",
    "\n",
    "\n",
    "data = [trace1, trace2]\n",
    "\n",
    "\n",
    "layout = go.Layout(\n",
    "    title_text='Queries that spilled the most to storage',\n",
    "    yaxis=dict(\n",
    "        # range = [0, 100],\n",
    "        side = 'left',\n",
    "        title=\"Bytes spilled\"\n",
    "        \n",
    "    ),\n",
    "    xaxis=dict(\n",
    "        title=\"Query ID\"\n",
    "\n",
    "    )\n",
    ")\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Query 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.iloc[0][\"query_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Query 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.iloc[1][\"query_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Query 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.iloc[2][\"query_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANALYSIS\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "##TODO: @Saravana sum local and remote query spillage and plot a graph against warehouse to see which warehouse should be upgraded.\n",
    "    \n",
    " Queries 1,2 and 3 fail after execution time for each being 29 minutes.\n",
    "* Query 1 (query_id: 01a78b77-0604-f44d-0000-08d15ddd870e) - 300 GB spilling to local storage and 200 GB to remote storage.\n",
    "* Query 2 (query_id: 01a78b55-0604-f44e-0000-08d15ddd55da) - 170 GB splilling to local storge and 90 GB to remote.\n",
    "* Query 3 (query_id: 01a78b55-0604-f44e-0000-08d15ddd55d6) - 245 GB spilling to local and 78 GB to remote storage.\n",
    "\n",
    "Most partitions are scanned in the table for these 3 queries.\n",
    "There are also multiple join and select statements for each query.\n",
    "</div>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optiml TODO\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "* Check if best virtual warehouse practices can be applied to most queries in this warehouse\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optiml Recommendation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"alert alert-success\">\n",
    "\n",
    "* Utilize a larger warehouse - Warehouse sizes of Xsmall and Small are used. 100's of GB are spilled due to insufficient resources. \n",
    "\n",
    "* Optimize the query - Reducing the number of JOINS, SELECT and UNION statements would improve the performance.\n",
    "\n",
    "* Reduce the data that is being processed (Ex: Redunant columns used in computation).\n",
    "\n",
    "* Split processing into multiple steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Queries that scanned the most data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=qqlib.n_queries_scanned_most_data(sdate,edate,5)\n",
    "df.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##TODO: Put labels on the axis\n",
    "## Queries that scanned most data\n",
    "\n",
    "trace1  = go.Scatter(\n",
    "        mode='lines+markers',\n",
    "        x = df['query_id'],\n",
    "        y = df['partitions_scanned'],\n",
    "        name=\"Partitions Scanned\",\n",
    "        marker_color='crimson'\n",
    "    )\n",
    "\n",
    "data = [trace1]\n",
    "\n",
    "layout = go.Layout(\n",
    "    title_text='Queries that scanned the most partitions',\n",
    "    yaxis=dict(\n",
    "        # range = [0, 100],\n",
    "        side = 'left',\n",
    "        title=\"Bytes spilled\"\n",
    "        \n",
    "    ),\n",
    "    xaxis=dict(\n",
    "        title=\"Query ID\"\n",
    "\n",
    "    )\n",
    "    \n",
    ")\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Query 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.iloc[0][\"query_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANALYSIS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Inadequte pruning is observed. 99% of the partitions on the table are scanned.\n",
    "* Execution time is 3.5 minutes.\n",
    "* 2 GB is spilled onto local storage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Query 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.iloc[1][\"query_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANALYSIS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Inadequte pruning is observed. 99% of the partitions on the table are scanned.\n",
    "* Execution time is 7.7 minutes.\n",
    "* 16.9 GB is spilled onto local storage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Query 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.iloc[2][\"query_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANALYSIS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Inadequte pruning is observed. 99% of the partitions on the table are scanned.\n",
    "* Execution time is 3.5 minutes.\n",
    "* 2 GB is spilled onto local storage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OPTIML Recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##TODO: @saravana Look at the wording on the recommendations and update them approrpriately - create cluster keys on time stamps\n",
    "* Queries are the same.\n",
    "* Cluster keys can be defined to reduce partitions scanned. \n",
    "* Choose cluster key that appears frequently in a WHERE clause\n",
    "* Scaling up warehouse would reduce bytes splilling to local storage\n",
    "* Query can be optimized by reducing number of JOIN statements, eliminating redundant SELECT statements and using DISTINCT clauses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Most cached queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=qqlib.n_most_cached_queries(sdate,edate,5)\n",
    "df.head() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Queries that scanned the most from cache\n",
    "\n",
    "trace1  = go.Scatter(\n",
    "        mode='lines+markers',\n",
    "        x = df['query_id'],\n",
    "        y = df['percent_scanned_from_cache'],\n",
    "        name=\"Percent Scanned From Cache\",\n",
    "        marker_color='crimson'\n",
    "    )\n",
    "\n",
    "data = [trace1]\n",
    "\n",
    "layout = go.Layout(\n",
    "    title_text='Queries that scanned the most percent from cache',\n",
    "    yaxis=dict(\n",
    "        # range = [0, 100],\n",
    "        side = 'left',\n",
    "        title=\"Percent Scanned From Cache\"\n",
    "        \n",
    "    ),\n",
    "    xaxis=dict(\n",
    "        title=\"Query ID\"\n",
    "\n",
    "    )\n",
    ")\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Query 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.iloc[2][\"query_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Query 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.iloc[1][\"query_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Query 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.iloc[2][\"query_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recommendations:\n",
    "\n",
    "##TODO: @saravana to word recommendations appropriately and also recommend further analysis if any.\n",
    "\n",
    "1) We need high reliability on most cached queries - since they are most used\n",
    "2) Also same holds for most executed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Most executed 'select' queries -- update this for select statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=qqlib.n_most_executed_select_queries(sdate,edate,10)\n",
    "# df.to_csv(\"/home/manas/DS_data/most_executed_select.csv\")\n",
    "df.head() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##TODO: @saravana to put query filter rules so that this particular section surfaces the right reelvant queries for most executed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Queries that scanned the most from cache\n",
    "\n",
    "trace1  = go.Scatter(\n",
    "        mode='lines+markers',\n",
    "        x = df.index,\n",
    "        y = df['number_of_times_executed'],\n",
    "        name=\"Number of times executed\",\n",
    "        marker_color='crimson'\n",
    "    )\n",
    "\n",
    "data = [trace1]\n",
    "\n",
    "layout = go.Layout(\n",
    "    title_text='Queries that were executed the most number of times',\n",
    "    yaxis=dict(\n",
    "        # range = [0, 100],\n",
    "        side = 'left',\n",
    "        title=\"Number of times executed\"\n",
    "        \n",
    "    ),\n",
    "    xaxis=dict(\n",
    "        title=\"Query ID\"\n",
    "\n",
    "    )\n",
    ")\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Query 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(df.iloc[0][\"query_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Query 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.iloc[1][\"query_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Query 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.iloc[3][\"query_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANALYSIS\n",
    "* Query 1 was executed 116028 in 30 days\n",
    "* Query 2 was executed 19770 in 30 days\n",
    "* Query 3 was executed 13815 in 30 days."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OPTIML Recommendations\n",
    "* Can convert SELECT Queries as Materialized Views"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MOST EXECUTED QUERIES\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=qqlib.caching_warehouse(sdate,edate,5)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LONGEST RUNNING QUERIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=qqlib.longest_running_queries(sdate,edate,5)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### QUERY 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.iloc[0][\"query_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### QUERY 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.iloc[1][\"query_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### QUERY 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.iloc[2][\"query_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANALYSIS\n",
    "\n",
    "##TODO: @Saravana make a recommendation on how we recommend them to use SCD type 2 architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Queries are the same.\n",
    "* Updating event query.\n",
    "* Execution time for query is approximately 135 minutes.\n",
    "* 65% of total partitions of table are scanned.\n",
    "* No spillage onto local or remote storage.\n",
    "* No Query overloading.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OPTIML to do"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Need to run update query to see if performance tuning can be done."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OPTIML Recommendations\n",
    "* Cluster keys can be defined to reduce execution time. WHERE clause present in query can be utilized.\n",
    "* Warehouse scaling up won't help with performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes\n",
    "\n",
    "* For most expensive queries i.e. consuming most credits steps are:\n",
    "  * Look at the amount of data the queries are operating on to see if it passes the sniff test\n",
    "  * Idenitfy causes of credit consumption e.g. using INSERT instead of COPY_TO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "fcbd4ad66d969ea49516a1cf27383420b67e9e950ebdd1bbb64e01b736f968b6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
