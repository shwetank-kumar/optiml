{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
    "    return false;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding system path\n",
    "import sys, pathlib\n",
    "sys.path.append(str(pathlib.Path.cwd().parent.parent))\n",
    "# sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set to show warnings only once\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up displays\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from tabulate import tabulate\n",
    "color_scheme=[\"red\",\"blue\",\"green\",\"orange\",\"purple\",\"brown\",\"pink\",\"gray\",\"olive\",\"cyan\",\"darkviolet\",\"goldenrod\",\"darkgreen\",\"chocolate\",\"lawngreen\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize connection to Snowflake and set analysis date\n",
    "from optiml.connection import SnowflakeConnConfig\n",
    "connection = SnowflakeConnConfig(accountname='jg84276.us-central1.gcp',warehousename=\"XSMALL_WH\").create_connection()\n",
    "# Initialize local environment\n",
    "import os\n",
    "cache_dir = os.path.expanduser('~/data/kiva')\n",
    "# Initialize query library\n",
    "from optiml.backend.query_profile import QueryProfile\n",
    "from optiml.backend.cost_profile import CostProfile, get_previous_dates\n",
    "qqlib = QueryProfile(connection, 'KIV', cache_dir)\n",
    "cqlib = CostProfile(connection, 'KIV', cache_dir)\n",
    "# Initialize analysis dates\n",
    "sdate = \"2022-08-12\"\n",
    "edate = \"2022-10-12\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up autoreload for libs\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%aimport optiml.backend.query_profile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis setup\n",
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "* Analysis date range: '2022-09-12' to '2022-10-12': last rolling month in the data we collected.\n",
    "\n",
    "* Type of Snowflake account: Standard Edition\n",
    "\n",
    "* Credit to dollar conversion: `$`2 per credit\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query execution per day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = qqlib.queries_by_execution_status(sdate,edate)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_count = df.groupby(['day']).agg({'n_success': 'sum', 'n_fail': 'sum'}).reset_index()\n",
    "df_count.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_credits = cqlib.credits_by_day(sdate, edate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace1 = go.Bar(\n",
    "        x = df_count['day'],\n",
    "        y = df_count['n_success'],\n",
    "        name=\"Execution success count\",\n",
    "    )\n",
    "\n",
    "trace2 = go.Bar(\n",
    "        x = df_count['day'],\n",
    "        y = df_count['n_fail'],\n",
    "        name=\"Execution fail count\",\n",
    "    )\n",
    "\n",
    "trace3  = go.Scatter(\n",
    "        mode='lines+markers',\n",
    "        x = df_credits['date'],\n",
    "        y = df_credits['credits'],\n",
    "        name=\"Credits\",\n",
    "        yaxis='y2',\n",
    "        line=dict(color='black'),\n",
    "    )\n",
    "\n",
    "\n",
    "data = [trace1, trace2, trace3]\n",
    "\n",
    "layout = go.Layout(\n",
    "    title_text='Query success, fail, credits per day',\n",
    "    yaxis=dict(\n",
    "        title=\"Count number\",\n",
    "        showgrid=False,\n",
    "    ),\n",
    "     yaxis2=dict(\n",
    "        title=\"Credits\", \n",
    "        overlaying=\"y\",\n",
    "        side=\"right\",\n",
    "        showgrid=False,\n",
    "    ),\n",
    "    xaxis=dict(\n",
    "        title=\"Date (UTC)\"\n",
    "    ),\n",
    "    legend=dict(\n",
    "        yanchor=\"top\",\n",
    "        y=0.99,\n",
    "        xanchor=\"left\",\n",
    "        x=0.99\n",
    "    ),\n",
    "    barmode=\"stack\"\n",
    ")\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Failures by user by day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fail = df[df['execution_status'] == 'FAIL'].reset_index(drop=True)\n",
    "df_fail_user = df_fail.groupby([\"user_name\",\"day\"]).sum(\"warehouse_name\").reset_index()\n",
    "df_fail_user.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.bar(df_fail_user, x=\"day\", y=\"n_fail\", color=\"user_name\", title=\"Number failed by user\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Failures by warehouse by day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fail_wh = df_fail.groupby([\"warehouse_name\",\"day\"]).sum(\"user_name\").reset_index()\n",
    "df_fail_wh.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.bar(df_fail_wh, x=\"day\", y=\"n_fail\", color=\"warehouse_name\", title=\"Number failed by warehouse\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 17th Aug DEV_WH execution_status fail anomaly\n",
    "df=qqlib.get_queries(start_date=\"2022-08-17\",end_date=\"2022-08-18\",wh=\"DEV_WH\",es=\"FAIL\", n=1500)\n",
    "df_unique = qqlib.append_query_hash(df)\n",
    "df_unique_group = df_unique.groupby(['query_hash','query_text']).agg({'execution_status':'count'}).reset_index()\n",
    "df_fails = df.groupby(['query_hash','query_text']).agg({'execution_status':'count'}).reset_index().sort_values('execution_status', ascending=False)\n",
    "df_fails.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 3rd Oct DEV_WH execution_status fail anomaly\n",
    "df=qqlib.get_queries(start_date=\"2022-10-03\",end_date=\"2022-10-04\",wh=\"DEV_WH\",es=\"FAIL\", n=1500)\n",
    "df_unique = qqlib.append_query_hash(df)\n",
    "df_unique_group = df_unique.groupby(['query_hash','query_text']).agg({'execution_status':'count'})\n",
    "df_fails = df.groupby(['query_hash','query_text']).agg({'execution_status':'count'}).sort_values('execution_status', ascending=False).reset_index()\n",
    "df_fails.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 11th Oct Unassigned execution_status fail anomaly\n",
    "df=qqlib.get_queries(start_date=\"2022-10-11\",end_date=\"2022-10-12\",es=\"FAIL\", n=1500)\n",
    "df_unique = qqlib.append_query_hash(df)\n",
    "df_unique_group = df_unique.groupby(['query_hash','query_text']).agg({'execution_status':'count'})\n",
    "df_fails = df.groupby(['query_hash','query_text']).agg({'execution_status':'count'}).sort_values('execution_status', ascending=False).reset_index()\n",
    "df_fails.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query execution by user per day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = qqlib.queries_by_execution_status(sdate,edate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_by_user = df.groupby(['user_name','day']).agg({'n_success': 'sum', 'n_fail': 'sum'})\n",
    "df_by_user.reset_index(inplace=True)\n",
    "user_list = list(df_by_user[\"user_name\"].unique())\n",
    "# user_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user = 'ANALYTICS_EXERCISE_USER' #significant failures\n",
    "# user =  'BAMBOO' #significant failures\n",
    "# user =  'DBT_DEV'\n",
    "# user = 'DBT_PROD'\n",
    "# user = 'FIVETRAN_USER'\n",
    "# user = 'FIVETRAN_USER_DEV'\n",
    "# user = 'GREGORYW' #some failures\n",
    "# user = 'GREGORYW_DEV'\n",
    "user = 'KIVA_API_SNAPSHOT_PROD' #all failures\n",
    "# user = 'LOOKER_DEV_ADMIN' #significant failures\n",
    "# user = 'LOOKER_PROD' #some failures\n",
    "# user = 'LOOKER_PROD_ADMIN' #some failures\n",
    "# user = 'LOOKER_RAW_DEV' #significant failures\n",
    "# user = 'LOOKER_RAW_PROD' #significant failures\n",
    "# user = 'MAXH_DEV' #some failures\n",
    "# user = 'ML_SERVICE_DEV'\n",
    "# user = 'ML_SERVICE_PROD'\n",
    "# user = 'PATRICKL'\n",
    "# user = 'PATT' #significant failures\n",
    "# user = 'ROBS' #significant failures\n",
    "# user = 'TEST_EXERCISE_AS' #some failures\n",
    "# user = 'TEST_EXERCISE_CM' #some failures\n",
    "# user = 'TEST_EXERCISE_JAR' #some failures\n",
    "# user = 'TEST_EXERCISE_LAZ' #some failures\n",
    "# user = 'TEST_EXERCISE_RDN' #some failures\n",
    "# user = 'VERTEX_API_DEV'\n",
    "# user = 'VERTEX_API_DEV_JENKINS'\n",
    "# user = 'VERTEX_API_PROD'\n",
    "# user = 'WORKSHEETS_APP_USER'\n",
    "df_user = df_by_user[df_by_user[\"user_name\"] == user]\n",
    "df_user.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace1 = go.Bar(\n",
    "        x = df_user['day'],\n",
    "        y = df_user['n_success'],\n",
    "        name=\"Execution success count\",\n",
    "    )\n",
    "\n",
    "trace2 = go.Bar(\n",
    "        x = df_user['day'],\n",
    "        y = df_user['n_fail'],\n",
    "        name=\"Execution fail count\",\n",
    "    )\n",
    "\n",
    "data = [trace1, trace2]\n",
    "\n",
    "layout = go.Layout(\n",
    "    title_text='Query count',\n",
    "    yaxis=dict(\n",
    "        title=\"Count number\",\n",
    "        showgrid=False,\n",
    "    ),\n",
    "     yaxis2=dict(\n",
    "        title=\"Number of times ran\", \n",
    "        overlaying=\"y\",\n",
    "        side=\"right\",\n",
    "        showgrid=False,\n",
    "    ),\n",
    "    xaxis=dict(\n",
    "        title=\"Date (UTC)\"\n",
    "    ),\n",
    "    legend=dict(\n",
    "        yanchor=\"top\",\n",
    "        y=0.99,\n",
    "        xanchor=\"left\",\n",
    "        x=0.99\n",
    "    ),\n",
    "    barmode=\"stack\"\n",
    ")\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## all failures for a user between start and end date\n",
    "df=qqlib.get_queries(start_date=sdate,end_date=edate,user=user,es=\"FAIL\", n=1500)\n",
    "df_unique = qqlib.append_query_hash(df)\n",
    "df_unique_group = df_unique.groupby(['query_hash','query_text']).agg({'execution_status':'count'})\n",
    "df_fails = df.groupby(['query_hash','query_text']).agg({'execution_status':'count'}).sort_values('execution_status', ascending=False).reset_index()\n",
    "df_fails.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query execution by warehouse per day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = qqlib.queries_by_execution_status(sdate,edate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_by_wh = df.groupby(['warehouse_name','day']).agg({'n_success': 'sum', 'n_fail': 'sum'})\n",
    "df_by_wh.reset_index(inplace=True)\n",
    "wh_list = list(df_by_wh[\"warehouse_name\"].unique())\n",
    "# wh_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wh = 'DAILY_REFRESH_WH'\n",
    "# wh = 'DEV_WH'\n",
    "# wh = 'ML_WH'\n",
    "# wh ='PROD_WH'\n",
    "wh = 'Unassigned'\n",
    "df_wh = df_by_wh[df_by_wh[\"warehouse_name\"] == wh]\n",
    "df_wh.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace1 = go.Bar(\n",
    "        x = df_wh['day'],\n",
    "        y = df_wh['n_success'],\n",
    "        name=\"Execution success count\",\n",
    "    )\n",
    "\n",
    "trace2 = go.Bar(\n",
    "        x = df_wh['day'],\n",
    "        y = df_wh['n_fail'],\n",
    "        name=\"Execution fail count\",\n",
    "    )\n",
    "\n",
    "data = [trace1, trace2]\n",
    "\n",
    "layout = go.Layout(\n",
    "    title_text='Query count',\n",
    "    yaxis=dict(\n",
    "        title=\"Count number\",\n",
    "        showgrid=False,\n",
    "    ),\n",
    "     yaxis2=dict(\n",
    "        title=\"Number of times ran\", \n",
    "        overlaying=\"y\",\n",
    "        side=\"right\",\n",
    "        showgrid=False,\n",
    "    ),\n",
    "    xaxis=dict(\n",
    "        title=\"Date (UTC)\"\n",
    "    ),\n",
    "    legend=dict(\n",
    "        yanchor=\"top\",\n",
    "        y=0.99,\n",
    "        xanchor=\"left\",\n",
    "        x=0.99\n",
    "    ),\n",
    "    barmode=\"stack\"\n",
    ")\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Find the queries given the conditions - with options to omit certain constraints\n",
    "df = qqlib.get_queries(start_date='2022-10-11',end_date='2022-10-12',es='FAIL', n=1500)\n",
    "df = df.fillna('Unassigned')\n",
    "# df.head()\n",
    "df = df[df['warehouse_name']=='Unassigned']\n",
    "df_unique = qqlib.append_query_hash(df)\n",
    "df_unique_group = df_unique.groupby(['query_hash','query_text']).agg({'execution_status':'count'})\n",
    "df_fails = df.groupby(['query_hash','query_text']).agg({'execution_status':'count'}).sort_values('execution_status', ascending=False).reset_index()\n",
    "df_fails.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get most inefficient query by metric - credits, total time elapsed etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metric = 'bytes_scanned'\n",
    "# metric = 'percentage_scanned_from_cache'\n",
    "# metric = 'bytes_spilled_to_local_storage'\n",
    "# metric = 'bytes_spilled_to_remote_storage' -- division by 0\n",
    "# metric = 'percentage_partitions_scanned' -- division by 0\n",
    "# metric = 'partitions_total'\n",
    "# metric = 'compilation_time_sec' -- division by 0\n",
    "# metric = 'execution_time_sec'\n",
    "# metric = 'queued_provisioning_time_sec' -- division by 0\n",
    "# metric = 'queued_repair_time_sec' -- division by 0\n",
    "# metric = 'queued_overload_time_sec' -- division by 0\n",
    "# metric = 'list_external_files_time_sec' -- division by 0\n",
    "# metric = 'total_time_elapsed_sec'\n",
    "# metric = 'credits'\n",
    "df = qqlib.n_inefficient_queries(sdate,edate,250,metric=metric)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unique = qqlib.get_unique_queries(df,metric)\n",
    "df_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df[df['query_hash']=='6ef334ed61427031d52acf53a055ab57']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Average is not there and total credits are different\n",
    "metric = 'credits'\n",
    "df = qqlib.n_inefficient_queries(sdate,edate,250,metric=metric)\n",
    "df = qqlib.get_unique_queries(df,metric)\n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace1 = go.Bar(\n",
    "        x = df['query_hash'],\n",
    "        y = df['n_success'],\n",
    "        name=\"Execution success count\",\n",
    "    )\n",
    "\n",
    "trace2 = go.Bar(\n",
    "        x = df['query_hash'],\n",
    "        y = df['n_fail'],\n",
    "        name=\"Execution fail count\",\n",
    "    )\n",
    "\n",
    "trace3  = go.Scatter(\n",
    "        mode='markers+lines',\n",
    "        x = df['query_hash'],\n",
    "        y = df['credits'],\n",
    "        name=\"Credits\",\n",
    "        yaxis='y2',\n",
    "        line=dict(color='black'),\n",
    "    )\n",
    "\n",
    "data = [trace1, trace2, trace3]\n",
    "\n",
    "layout = go.Layout(\n",
    "    title_text='Query count',\n",
    "    yaxis=dict(\n",
    "        title=\"Count number\",\n",
    "        showgrid=False,\n",
    "    ),\n",
    "     yaxis2=dict(\n",
    "        title=\"Number of times ran\", \n",
    "        overlaying=\"y\",\n",
    "        side=\"right\",\n",
    "        showgrid=False,\n",
    "    ),\n",
    "    xaxis=dict(\n",
    "        title=\"Date (UTC)\"\n",
    "    ),\n",
    "    legend=dict(\n",
    "        yanchor=\"top\",\n",
    "        y=0.99,\n",
    "        xanchor=\"left\",\n",
    "        x=0.99\n",
    "    ),\n",
    "    barmode=\"stack\"\n",
    ")\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "fig.update_yaxes(rangemode=\"tozero\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[0].query_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now plot these grouped queries as success and fail bar charts and show what numbers are different between success and fail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Queries succeeded / failed by hash in an analysis period\n",
    "## Individual query inefficient by a metric\n",
    "## Query hashes inefficient by metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plot most expensive queries by query id - showing both success, failures credit consumed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plot this by hash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most expensive queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = qqlib.n_inefficient_queries(sdate,edate,10,metric='credits')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = qqlib.n_expensive_queries(sdate,edate,200)\n",
    "df[\"execution_success\"] = (df[\"execution_status\"] == \"SUCCESS\").astype(int)\n",
    "df[\"execution_fail\"] = (df[\"execution_status\"] == \"FAIL\").astype(int)\n",
    "df[\"execution_count\"] = df[\"execution_success\"] + df[\"execution_fail\"]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_by_execution_status = df.groupby([\"execution_status\"])[\"execution_success\",\"execution_fail\",\"execution_count\",\"credits\"].sum().reset_index()\n",
    "df_by_execution_status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Most expensive queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = qqlib.n_inefficient_queries(sdate,edate,200,metric='credits')\n",
    "df[\"execution_success\"] = (df[\"execution_status\"] == \"SUCCESS\").astype(int)\n",
    "df[\"execution_fail\"] = (df[\"execution_status\"] == \"FAIL\").astype(int)\n",
    "df[\"execution_count\"] = df[\"execution_success\"] + df[\"execution_fail\"]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_by_execution_status = df.groupby([\"execution_status\"])[\"execution_success\",\"execution_fail\",\"execution_count\",\"credits\"].sum().reset_index()\n",
    "df_by_execution_status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Naive most expensive queries\n",
    "n = 100\n",
    "trace1 = go.Scatter(\n",
    "        mode='lines+markers',\n",
    "        x = df['query_id'][0:n],\n",
    "        y = df['credits'][0:n],\n",
    "        name=\"Credits\",\n",
    "        marker=dict(\n",
    "            size=10,\n",
    "            color=(df[\"execution_fail\"] > 0 ).astype(int).astype('int'),\n",
    "            colorscale=[[0, 'green'], [1, 'red']]\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "data = [trace1]\n",
    "\n",
    "layout = go.Layout(\n",
    "    title_text='Naive Most expensive queries',\n",
    "    yaxis=dict(\n",
    "        title=\"Credits\"\n",
    "    ),\n",
    "    xaxis=dict(\n",
    "        title=\"Query ID\"\n",
    "    )\n",
    ")\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_by_execution_status = df.groupby([\"execution_status\"])[\"execution_success\",\"execution_fail\",\"execution_count\",\"credits\"].sum().reset_index()\n",
    "df_by_execution_status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Warning note\n",
    "\n",
    "<div class=\"alert alert-danger\">\n",
    "    \n",
    "* We do not recommend the naive approach since it can have very limited ROI\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##TODO: Temporal distribution of expensive queries by warehouse\n",
    "df_hash = get_unique_query(df)\n",
    "# df_hash.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_by_execution_status = df_hash.groupby([\"execution_status\"])[\"execution_success\",\"execution_fail\",\"execution_count\",\"credits\"].sum().reset_index()\n",
    "\n",
    "# Pie charts for expensive queries by execution status\n",
    "fig = make_subplots(\n",
    "    rows=1, cols=2,\n",
    "    specs=[[{\"type\": \"pie\"},{\"type\": \"pie\"}]],\n",
    "    subplot_titles=(\"Count\", \"Credits\")\n",
    ")\n",
    "\n",
    "fig.add_trace(go.Pie(labels=df_by_execution_status['execution_status'].tolist(), values=df_by_execution_status['execution_count'].tolist(), \n",
    "                     name=\"Execution Count\", rotation=45, marker_colors=[\"red\",\"green\"]),row=1,col=1)\n",
    "fig.add_trace(go.Pie(labels=df_by_execution_status['execution_status'].tolist(), values=df_by_execution_status['credits'].tolist(),\n",
    "                     name='Credits', rotation=45, marker_colors=[\"red\",\"green\"]),row=1,col=2)\n",
    "\n",
    "fig.update_layout(\n",
    "    title={\n",
    "        'text': \"Expensive queries breakdown by execution status\",\n",
    "        'y':0.95,\n",
    "        'x':0.5,\n",
    "        'xanchor': 'center',\n",
    "        'yanchor': 'top'})\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_by_user = df_hash.groupby([\"user_name\"])[\"execution_success\",\"execution_fail\",\"execution_count\",\"credits\"].sum().reset_index()\n",
    "print('Credit and count of expensive queries by user')\n",
    "print('---------------------------------------------')\n",
    "print(tabulate(df_by_user, headers='keys', tablefmt='rounded_outline', showindex=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pie charts for expensive queries by execution status\n",
    "fig = make_subplots(\n",
    "    rows=1, cols=2,\n",
    "    specs=[[{\"type\": \"pie\"},{\"type\": \"pie\"}]],\n",
    "    subplot_titles=(\"Count\", \"Credits\")\n",
    ")\n",
    "\n",
    "fig.add_trace(go.Pie(labels=df_by_user['user_name'].tolist(), values=df_by_user['execution_count'].tolist(), \n",
    "                     name=\"Execution Count\", rotation=45, marker_colors=color_scheme),row=1,col=1)\n",
    "fig.add_trace(go.Pie(labels=df_by_user['user_name'].tolist(), values=df_by_user['credits'].tolist(),\n",
    "                     name='Credits', rotation=45, marker_colors=color_scheme),row=1,col=2)\n",
    "\n",
    "fig.update_layout(\n",
    "    title={\n",
    "        'text': \"Expensive queries breakdown by user\",\n",
    "        'y':0.95,\n",
    "        'x':0.5,\n",
    "        'xanchor': 'center',\n",
    "        'yanchor': 'top'})\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_by_wh = df_hash.groupby([\"warehouse_name\"])[\"execution_success\",\"execution_fail\",\"execution_count\",\"credits\"].sum().reset_index()\n",
    "print('Credit and count of expensive queries by user')\n",
    "print('---------------------------------------------')\n",
    "print(tabulate(df_by_wh, headers='keys', tablefmt='rounded_outline', showindex=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pie charts for expensive queries by execution status\n",
    "fig = make_subplots(\n",
    "    rows=1, cols=2,\n",
    "    specs=[[{\"type\": \"pie\"},{\"type\": \"pie\"}]],\n",
    "    subplot_titles=(\"Count\", \"Credits\")\n",
    ")\n",
    "\n",
    "fig.add_trace(go.Pie(labels=df_by_wh['warehouse_name'].tolist(), values=df_by_user['execution_count'].tolist(), \n",
    "                     name=\"Execution Count\", rotation=45, marker_colors=color_scheme),row=1,col=1)\n",
    "fig.add_trace(go.Pie(labels=df_by_wh['warehouse_name'].tolist(), values=df_by_user['credits'].tolist(),\n",
    "                     name='Credits', rotation=45, marker_colors=color_scheme),row=1,col=2)\n",
    "\n",
    "fig.update_layout(\n",
    "    title={\n",
    "        'text': \"Expensive queries breakdown by warehouse\",\n",
    "        'y':0.95,\n",
    "        'x':0.5,\n",
    "        'xanchor': 'center',\n",
    "        'yanchor': 'top'})\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Most expensive queries\n",
    "df_by_hash = df_hash.groupby([\"query_hash\"])[\"execution_success\",\"execution_fail\",\"credits\"].sum().reset_index().sort_values('credits', ascending=False)\n",
    "trace1 = go.Bar(\n",
    "        x = df_by_hash['query_hash'],\n",
    "        y = df_by_hash['execution_success'],\n",
    "        name=\"Execution success count\",\n",
    "        yaxis='y2',\n",
    "        marker=dict(color='green'),\n",
    "        opacity=0.5\n",
    "    )\n",
    "\n",
    "trace2 = go.Bar(\n",
    "        x = df_by_hash['query_hash'],\n",
    "        y = df_by_hash['execution_fail'],\n",
    "        name=\"Execution fail count\",\n",
    "        yaxis='y2',\n",
    "        marker=dict(color='red'),\n",
    "        opacity=0.5\n",
    "    )\n",
    "\n",
    "trace3  = go.Scatter(\n",
    "        mode='lines+markers',\n",
    "        x = df_by_hash['query_hash'],\n",
    "        y = df_by_hash['credits'],\n",
    "        name=\"Credits\",\n",
    "        line=dict(color='black'),\n",
    "    )\n",
    "\n",
    "\n",
    "data = [trace1, trace2, trace3]\n",
    "\n",
    "layout = go.Layout(\n",
    "    title_text='Most expensive queries',\n",
    "    yaxis=dict(\n",
    "        title=\"Total Credits\",\n",
    "        showgrid=False,\n",
    "        range=[0, math.ceil(max(df_by_hash[\"credits\"]))+10]\n",
    "    ),\n",
    "    yaxis2=dict(\n",
    "        title=\"Number of times ran\", overlaying=\"y\",\n",
    "        side=\"right\",\n",
    "#         position=0.98,\n",
    "        showgrid=False,\n",
    "        range=[0, math.ceil(max(df_by_hash[\"execution_success\"] + df_by_hash[\"execution_fail\"]))+10]\n",
    "    ),\n",
    "    xaxis=dict(\n",
    "        title=\"Query Hash\"\n",
    "    ),\n",
    "    legend=dict(\n",
    "        yanchor=\"top\",\n",
    "        y=0.99,\n",
    "        xanchor=\"right\",\n",
    "        x=0.99\n",
    "    ),\n",
    "    barmode=\"stack\"\n",
    ")\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_by_hash = df_hash.groupby([\"query_hash\"])[\"execution_success\",\"execution_fail\",\"execution_count\",\"credits\"].sum().reset_index()\n",
    "df_by_hash = df_by_hash.sort_values('credits', ascending=False)\n",
    "print(tabulate(df_by_hash, headers='keys', tablefmt='rounded_outline', showindex=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check times of 5 most expensive queries with WH credit usage peaks\n",
    "# q1 = df_hash[df_hash['query_hash'] == '6ef334ed61427031d52acf53a055ab57'].sort_values('start_time')\n",
    "# q1[[\"warehouse_name\",\"start_time\",\"end_time\"]]\n",
    "q2 = df_hash[df_hash['query_hash'] == '295230eae35c6270a0e240fca4a0be6f'].sort_values('start_time')\n",
    "q2[[\"query_id\", \"warehouse_name\",\"start_time\",\"end_time\"]]\n",
    "# q3 = df_hash[df_hash['query_hash'] == '233655c56c9344d5c303bdd8f4b9f264'].sort_values('start_time')\n",
    "# q3[[\"warehouse_name\",\"start_time\",\"end_time\"]]\n",
    "# q4 = df_hash[df_hash['query_hash'] == 'd154d97ba4dbd4fd677baa48d6b47b71'].sort_values('start_time')\n",
    "# q4[[\"warehouse_name\",\"start_time\",\"end_time\"]]\n",
    "# q5 = df_hash[df_hash['query_hash'] == '5cb178243f7dc6005e8141a72f9bc895'].sort_values('start_time')\n",
    "# q5[[\"warehouse_name\",\"start_time\",\"end_time\"]]\n",
    "# q6 = df_hash[df_hash['query_hash'] == '535f3564c40d364959615108ceee4ea0'].sort_values('start_time')\n",
    "# q6[[\"warehouse_name\",\"start_time\",\"end_time\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weekly change of 10 top expensive queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_n_expensive_query_hashes(sdate, edate, n):\n",
    "    df_expensive = qqlib.n_expensive_queries(sdate, edate, 200)\n",
    "    df_hash = get_unique_query(df_expensive).sort_values('credits', ascending=False)\n",
    "    hash_list = df_hash[\"query_hash\"].unique()\n",
    "    return hash_list[0:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "sdate = '2022-01-01'\n",
    "n_periods = 9\n",
    "frequency = '1M'\n",
    "sdates = pd.date_range(sdate, periods=n_periods, freq=frequency)\n",
    "edates = pd.date_range(sdates[1], periods=n_periods, freq=frequency)\n",
    "start_dates = [d.strftime('%Y-%m-%d') for d in sdates]\n",
    "end_dates = [d.strftime('%Y-%m-%d') for d in edates]\n",
    "df_hash = pd.DataFrame()\n",
    "\n",
    "for idx, s in enumerate(start_dates):\n",
    "    df_hash[s] = pd.DataFrame(get_n_expensive_query_hashes(start_dates[idx], end_dates[idx], 10), columns=[start_dates[0]])\n",
    "\n",
    "\n",
    "df_hash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = df_hash.columns.values\n",
    "repeat = []\n",
    "for idx, col in enumerate(cols[:-1]):\n",
    "    repeat.append(sum(el in df_hash[cols[idx]].to_list() for el in df_hash[cols[idx+1]].to_list()))\n",
    "\n",
    "print(repeat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 14 days\n",
    "from statistics import mean, median\n",
    "mean(repeat), median(repeat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "    \n",
    "* Naively optimizing the most expensive query is not an effective strategy. Some queries:\n",
    "    * Fail to run to completion generating no ROI but use resources\n",
    "    * Run more frequently than others so optimizing them will lead to better ROI\n",
    "* 5% of queries fail to run to completion by count but they consume ~ 6% of credits\n",
    "* Majority of expensive queries come from DEV_WH - which makes sense, but larger fraction comes from DAILY_REFRESH_WH by credits than by count which might have optimization opportunity.\n",
    "* DBT_DEV, DBT_PROD write > 90% of the top 200 most expensive queries. This is inspite of the fact that their total credit consumption is <30% and much less than that of the most expensive user i.e. FIVETRAN_USER. There may be opportunity to do DBT specific optimization for Snowflake.\n",
    "* Manually checking 5 most expensive queries showed 4 of them are consistent with peaks in warehouse usage:\n",
    "    * Query Hash: 6ef334ed61427031d52acf53a055ab57 (DAILY_REFRESH_WH, 2 pm), 295230eae35c6270a0e240fca4a0be6f (DEV_WH, 12 pm), 233655c56c9344d5c303bdd8f4b9f264 (DAILY_REFRESH_WH, 9 am), d154d97ba4dbd4fd677baa48d6b47b71 (DAILY_REFRESH_WH, 2:50 pm), 5cb178243f7dc6005e8141a72f9bc895 (DEV_WH, 1 pm)\n",
    "                                                                                                                                               \n",
    "* Queries aggregated by query hash (i.e. query text matching exactly) show:                                                                                                 \n",
    "    * Query IDs: 01a78358-0604-ec7d-0000-08d15dc9f6fe, 01a78350-0604-ec7d-0000-08d15dc9f016  might be responsible for increase in compute consumption for DEV_WH on 8th Oct. (alternatively could be WH scaling/size change as well)\n",
    "    * Query Hash: 6ef334ed61427031d52acf53a055ab57 cumulatively contributes to the most number of credits (~28) and runs to completion 12 times \n",
    "    * Query Hash: 49539ab6df6b8a1df712dbbd70efe3e3 fails 3 times it runs and costs ~3 credits\n",
    "    * Multiple queries that fail once during the analysis period\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actions and Recommendations\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "\n",
    "* ##TODO:\n",
    "    * Are these queries all current and serving a business purpose - if not remove them\n",
    "    * Look at other queries that feed reports that no one is looking at and remove them\n",
    "    * Optimize queries that fail to run or cumulatively consume more credits than the queries that consume the most credits but run fewer time\n",
    "    * For highest ROI generate query hashes using templates so that queries can be grouped irrespective of parameter values\n",
    "* For queries with higher spillage consider a larger instance\n",
    "* For queries that scan a large number of partitions consider:\n",
    "    * Search Optimization enable for selective queries\n",
    "    * Autoclustering enable with thoughtfully chosen cluster keys\n",
    "* SCD2 for update queries (https://community.snowflake.com/s/article/Building-a-Type-2-Slowly-Changing-Dimension-in-Snowflake-Using-Streams-and-Tasks-Part-1)\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hash_merged = pd.merge(df_by_hash, df_hash[['query_hash', 'query_text','user_name','warehouse_name']], on=[\"query_hash\"])\n",
    "df_hash_merged = df_hash_merged.drop_duplicates(subset=['query_hash']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search for queries by any column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_iterable = df[df['query_text'].str.contains(\"merge into \", case=False)][\"query_text\"].reset_index(drop=True)\n",
    "print(update_iterable.iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start and end time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Query 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_hash_merged.iloc[0][\"query_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Query 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_hash_merged.iloc[1][\"query_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Query 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_hash_merged.iloc[12][\"query_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Queries that spill to storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=qqlib.n_queries_spill_to_storage(sdate,edate,5)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Most expensive queries\n",
    "\n",
    "trace1  = go.Scatter(\n",
    "        mode='lines+markers',\n",
    "        x = df['query_id'],\n",
    "        y = df['bytes_spilled_to_remote_storage'],\n",
    "        name=\"Bytes Spilled Remote\",\n",
    "        marker_color='crimson'\n",
    "    )\n",
    "\n",
    "trace2  = go.Scatter(\n",
    "        mode='lines+markers',\n",
    "        x = df['query_id'],\n",
    "        y = df['bytes_spilled_to_local_storage'],\n",
    "        name=\"Bytes Spilled Local\",\n",
    "        marker_color='purple'\n",
    "    )\n",
    "\n",
    "\n",
    "data = [trace1, trace2]\n",
    "\n",
    "\n",
    "layout = go.Layout(\n",
    "    title_text='Queries that spilled the most to storage',\n",
    "    yaxis=dict(\n",
    "        # range = [0, 100],\n",
    "        side = 'left',\n",
    "        title=\"Bytes spilled\"\n",
    "        \n",
    "    ),\n",
    "    xaxis=dict(\n",
    "        title=\"Query ID\"\n",
    "\n",
    "    )\n",
    ")\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Query 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.iloc[0][\"query_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Query 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.iloc[1][\"query_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Query 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.iloc[2][\"query_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANALYSIS\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "##TODO: @Saravana sum local and remote query spillage and plot a graph against warehouse to see which warehouse should be upgraded.\n",
    "    \n",
    " Queries 1,2 and 3 fail after execution time for each being 29 minutes.\n",
    "* Query 1 (query_id: 01a78b77-0604-f44d-0000-08d15ddd870e) - 300 GB spilling to local storage and 200 GB to remote storage.\n",
    "* Query 2 (query_id: 01a78b55-0604-f44e-0000-08d15ddd55da) - 170 GB splilling to local storge and 90 GB to remote.\n",
    "* Query 3 (query_id: 01a78b55-0604-f44e-0000-08d15ddd55d6) - 245 GB spilling to local and 78 GB to remote storage.\n",
    "\n",
    "Most partitions are scanned in the table for these 3 queries.\n",
    "There are also multiple join and select statements for each query.\n",
    "</div>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optiml TODO\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "* Check if best virtual warehouse practices can be applied to most queries in this warehouse\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optiml Recommendation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"alert alert-success\">\n",
    "\n",
    "* Utilize a larger warehouse - Warehouse sizes of Xsmall and Small are used. 100's of GB are spilled due to insufficient resources. \n",
    "\n",
    "* Optimize the query - Reducing the number of JOINS, SELECT and UNION statements would improve the performance.\n",
    "\n",
    "* Reduce the data that is being processed (Ex: Redunant columns used in computation).\n",
    "\n",
    "* Split processing into multiple steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Queries that scanned the most data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=qqlib.n_queries_scanned_most_data(sdate,edate,5)\n",
    "df.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##TODO: Put labels on the axis\n",
    "## Queries that scanned most data\n",
    "\n",
    "trace1  = go.Scatter(\n",
    "        mode='lines+markers',\n",
    "        x = df['query_id'],\n",
    "        y = df['partitions_scanned'],\n",
    "        name=\"Partitions Scanned\",\n",
    "        marker_color='crimson'\n",
    "    )\n",
    "\n",
    "data = [trace1]\n",
    "\n",
    "layout = go.Layout(\n",
    "    title_text='Queries that scanned the most partitions',\n",
    "    yaxis=dict(\n",
    "        # range = [0, 100],\n",
    "        side = 'left',\n",
    "        title=\"Bytes spilled\"\n",
    "        \n",
    "    ),\n",
    "    xaxis=dict(\n",
    "        title=\"Query ID\"\n",
    "\n",
    "    )\n",
    "    \n",
    ")\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Query 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.iloc[0][\"query_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANALYSIS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Inadequte pruning is observed. 99% of the partitions on the table are scanned.\n",
    "* Execution time is 3.5 minutes.\n",
    "* 2 GB is spilled onto local storage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Query 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.iloc[1][\"query_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANALYSIS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Inadequte pruning is observed. 99% of the partitions on the table are scanned.\n",
    "* Execution time is 7.7 minutes.\n",
    "* 16.9 GB is spilled onto local storage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Query 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.iloc[2][\"query_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANALYSIS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Inadequte pruning is observed. 99% of the partitions on the table are scanned.\n",
    "* Execution time is 3.5 minutes.\n",
    "* 2 GB is spilled onto local storage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OPTIML Recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##TODO: @saravana Look at the wording on the recommendations and update them approrpriately - create cluster keys on time stamps\n",
    "* Queries are the same.\n",
    "* Cluster keys can be defined to reduce partitions scanned. \n",
    "* Choose cluster key that appears frequently in a WHERE clause\n",
    "* Scaling up warehouse would reduce bytes splilling to local storage\n",
    "* Query can be optimized by reducing number of JOIN statements, eliminating redundant SELECT statements and using DISTINCT clauses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Most cached queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=qqlib.n_most_cached_queries(sdate,edate,5)\n",
    "df.head() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Queries that scanned the most from cache\n",
    "\n",
    "trace1  = go.Scatter(\n",
    "        mode='lines+markers',\n",
    "        x = df['query_id'],\n",
    "        y = df['percent_scanned_from_cache'],\n",
    "        name=\"Percent Scanned From Cache\",\n",
    "        marker_color='crimson'\n",
    "    )\n",
    "\n",
    "data = [trace1]\n",
    "\n",
    "layout = go.Layout(\n",
    "    title_text='Queries that scanned the most percent from cache',\n",
    "    yaxis=dict(\n",
    "        # range = [0, 100],\n",
    "        side = 'left',\n",
    "        title=\"Percent Scanned From Cache\"\n",
    "        \n",
    "    ),\n",
    "    xaxis=dict(\n",
    "        title=\"Query ID\"\n",
    "\n",
    "    )\n",
    ")\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Query 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.iloc[2][\"query_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Query 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.iloc[1][\"query_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Query 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.iloc[2][\"query_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recommendations:\n",
    "\n",
    "##TODO: @saravana to word recommendations appropriately and also recommend further analysis if any.\n",
    "\n",
    "1) We need high reliability on most cached queries - since they are most used\n",
    "2) Also same holds for most executed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Most executed 'select' queries -- update this for select statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=qqlib.n_most_executed_select_queries(sdate,edate,10)\n",
    "# df.to_csv(\"/home/manas/DS_data/most_executed_select.csv\")\n",
    "df.head() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##TODO: @saravana to put query filter rules so that this particular section surfaces the right reelvant queries for most executed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Queries that scanned the most from cache\n",
    "\n",
    "trace1  = go.Scatter(\n",
    "        mode='lines+markers',\n",
    "        x = df.index,\n",
    "        y = df['number_of_times_executed'],\n",
    "        name=\"Number of times executed\",\n",
    "        marker_color='crimson'\n",
    "    )\n",
    "\n",
    "data = [trace1]\n",
    "\n",
    "layout = go.Layout(\n",
    "    title_text='Queries that were executed the most number of times',\n",
    "    yaxis=dict(\n",
    "        # range = [0, 100],\n",
    "        side = 'left',\n",
    "        title=\"Number of times executed\"\n",
    "        \n",
    "    ),\n",
    "    xaxis=dict(\n",
    "        title=\"Query ID\"\n",
    "\n",
    "    )\n",
    ")\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Query 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(df.iloc[0][\"query_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Query 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.iloc[1][\"query_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Query 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.iloc[3][\"query_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANALYSIS\n",
    "* Query 1 was executed 116028 in 30 days\n",
    "* Query 2 was executed 19770 in 30 days\n",
    "* Query 3 was executed 13815 in 30 days."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OPTIML Recommendations\n",
    "* Can convert SELECT Queries as Materialized Views"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MOST EXECUTED QUERIES\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=qqlib.caching_warehouse(sdate,edate,5)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LONGEST RUNNING QUERIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=qqlib.longest_running_queries(sdate,edate,5)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### QUERY 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.iloc[0][\"query_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### QUERY 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.iloc[1][\"query_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### QUERY 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.iloc[2][\"query_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANALYSIS\n",
    "\n",
    "##TODO: @Saravana make a recommendation on how we recommend them to use SCD type 2 architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Queries are the same.\n",
    "* Updating event query.\n",
    "* Execution time for query is approximately 135 minutes.\n",
    "* 65% of total partitions of table are scanned.\n",
    "* No spillage onto local or remote storage.\n",
    "* No Query overloading.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OPTIML to do"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Need to run update query to see if performance tuning can be done."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OPTIML Recommendations\n",
    "* Cluster keys can be defined to reduce execution time. WHERE clause present in query can be utilized.\n",
    "* Warehouse scaling up won't help with performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes\n",
    "\n",
    "* For most expensive queries i.e. consuming most credits steps are:\n",
    "  * Look at the amount of data the queries are operating on to see if it passes the sniff test\n",
    "  * Idenitfy causes of credit consumption e.g. using INSERT instead of COPY_TO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "ba286fab723f3beadf2310fa20d811dff5d5606e9e08a95b52f51185bcb3e19b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
