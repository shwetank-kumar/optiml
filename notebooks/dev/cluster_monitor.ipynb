{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92fe9cab-eea5-40c4-b396-28b5c6d60242",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys, pathlib\n",
    "sys.path.append(str(pathlib.Path.cwd().parent.parent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d472020-1bc3-4001-836f-08a7036c4b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext dotenv\n",
    "\n",
    "%dotenv ../../env/.env\n",
    "\n",
    "import warnings\n",
    "from pandas import Timedelta\n",
    "# from optiml.utils import sf\n",
    "import time\n",
    "from optiml.utils.sf import logger, sql_to_df, run_sql, conn, session\n",
    "import pandas as pd\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "try:\n",
    "    %load_ext autotime\n",
    "except:\n",
    "    !pip install ipython-autotime\n",
    "    %load_ext autotime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc268fe9-922b-4ae8-8cb6-299c155b0f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1: if Cluster 1 and 3 are active, 2 is idle, and num_clusters is set to 2, does #2 shutoff?\n",
    "#    1b) are credits saved?\n",
    "# Q2: if cluster 1 and 2 are active, and 2 is shutoff before 60 seconds of uptime, are we billed\n",
    "#     at minimum of 60 secs like base warehouse cluster? or charged for usage?\n",
    "#     - yes, minimum of 60 secs billed\n",
    "\n",
    "# algos:\n",
    "# 1) if warehouse is idle, shut down warehouse\n",
    "# 2) estimate total number of active clusters\n",
    "#   a) use total number of running queries\n",
    "#   b) use total pct of provisioned \n",
    "# set max_clusters to that number of active clusters\n",
    "# 3) if multicluster type is STANDARD and warehouse is idle set to ECONOMY\n",
    "#    if not idle and total active clusters > 1 (or total active clusters = max clusters)\n",
    "#        then set to ECONOMY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4cbded-68af-4a2f-87bc-36ce8a2e20a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def monitor():\n",
    "    show_df = sql_to_df(\"show warehouses\")\n",
    "    now = pd.Timestamp.now()\n",
    "    show_df['ts'] = now\n",
    "    something_running = 'STARTED' in show_df.state.to_list()\n",
    "    all_show_results = []\n",
    "    all_show_results.append(show_df)\n",
    "    \n",
    "    while something_running:\n",
    "        time.sleep(1)\n",
    "        show_df = sql_to_df(\"show warehouses\")\n",
    "        now = pd.Timestamp.now()\n",
    "        show_df['ts'] = now\n",
    "        something_running = 'STARTED' in show_df.state.to_list()\n",
    "        print(something_running)\n",
    "        all_show_results.append(show_df)\n",
    "    return pd.concat(all_show_results)\n",
    "#     def show():\n",
    "#         run_sql(f\"USE WAREHOUSE {using_wh}\", ctx=using_conn)\n",
    "#         show_df = sql_to_df(\"show warehouses\")\n",
    "\n",
    "    \n",
    "#     show()\n",
    "    print(something_running)\n",
    "\n",
    "df = monitor()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c6dc4d-e1f3-485e-a0ea-c7db398c7b67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c78b84c-23f4-4ea4-9b14-c6bb744b9a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "wh_name = 'test_control_a67fc85c_9de9_400e_8e49_6c93dfcef9db'\n",
    "\n",
    "# ['test_control_4d79538c_94ab_464d_bb1e_3320be370837',\n",
    "#  'test_suspend_idle_after_one_minute_11404fa8_0e29_40d7_8eee_9a7dc8414539',\n",
    "#  'test_suspend_after_one_minute_a3185ebe_291c_4b5e_aeed_cd22efb8457c']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14154269-9460-4334-abc0-66b8dccdd9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "\n",
    "current_time = datetime.datetime.now().strftime(\"%Y_%m%d_%H%M%S\")\n",
    "print(current_time)\n",
    "# Assuming your DataFrame is called 'df'\n",
    "file_name = f'{wh_name}_{current_time}.pkl'\n",
    "print(file_name)\n",
    "df.to_pickle(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e6896b-603a-4af3-aed9-5ef2fe435277",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = 'test_bc9a9691_c770_4eb7_96e9_278aac7db0c6_2023_0609_143717.pkl'\n",
    "# df = pd.read_pickle(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c990ec50-2c09-42bb-a28d-ecabc44b1de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wh_name='DEMO'\n",
    "print(wh_name.upper())\n",
    "import plotly.express as px\n",
    "fig = px.line(df[df.name == wh_name.upper()], x='ts', y='started_clusters')\n",
    "fig.show()\n",
    "fig = px.line(df[df.name == wh_name.upper()], x='ts', y=['running', 'queued'])\n",
    "fig.show()\n",
    "fig = px.line(df[df.name == wh_name.upper()], x='ts', y=['available', 'provisioning', 'quiescing'])\n",
    "fig.update_yaxes(autorange=\"reversed\")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d3c59c-2e64-44ef-82d4-ef4f019ef6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "wh_status = f\"\"\"with qh as (\n",
    "select \n",
    "    *,\n",
    "    md5(warehouse_name) as warehouse_id,\n",
    "      warehouse_name || ':' || cluster_number || ':' || query_id as warehouse_query,\n",
    "    TIMESTAMPADD( millisecond , -execution_time,end_time) as execution_start_time\n",
    "from table(information_schema.query_history_by_warehouse('{wh_name}', result_limit=>10000))\n",
    "where execution_status != 'RUNNING'\n",
    "and warehouse_size is not null\n",
    "and start_time > '2023-06-10 11:00'\n",
    "order by start_time desc\n",
    "),\n",
    "seconds as (\n",
    "    select\n",
    "        dateadd(\n",
    "            'second',\n",
    "            '-' || row_number() over (order by seq4() asc),\n",
    "            dateadd('day', '+1', date_trunc('second', current_timestamp))\n",
    "        ) as second_start,\n",
    "        dateadd('second', '+1', second_start) as timestamp\n",
    "    from table(generator(rowcount => (3600*24 * 100)))\n",
    "    qualify second_start between (select min(start_time) from qh) and (select max(end_time) from qh)\n",
    "),\n",
    "warehouse_cluster_seconds as (\n",
    "    select * from \n",
    "    seconds\n",
    "    cross join \n",
    "    (select distinct md5(warehouse_name) as warehouse_id, warehouse_name, cluster_number from qh) w\n",
    "\n",
    ")\n",
    "-- select * from warehouse_seconds;\n",
    "select\n",
    "    seconds.timestamp,\n",
    "    seconds.warehouse_name,\n",
    "    seconds.warehouse_id,\n",
    "    seconds.cluster_number,\n",
    "    count(queries.query_id) as num_queries\n",
    "from warehouse_cluster_seconds seconds\n",
    "left join qh as queries\n",
    "    -- on date_trunc('hour', seconds.timestamp)=queries.query_hour -- NEW: equi-join condition\n",
    "    on seconds.warehouse_id = queries.warehouse_id\n",
    "    and seconds.cluster_number = queries.cluster_number\n",
    "    and seconds.timestamp -- range join condition\n",
    "      between date_trunc('second', queries.execution_start_time) and date_trunc('second', queries.end_time)\n",
    "group by 1,2,3,4\n",
    "\"\"\"\n",
    "\n",
    "wdf = sql_to_df(wh_status)\n",
    "display(wdf)\n",
    "\n",
    "\n",
    "events = f\"\"\"\n",
    "select *\n",
    "from snowflake.account_usage.warehouse_events_history\n",
    "where lower(warehouse_name) = '{wh_name}'\n",
    "and timestamp > '2023-06-08 11:00'\n",
    "\"\"\"\n",
    "\n",
    "whedf = sql_to_df(events)\n",
    "display(whedf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "736a63d5-19e0-47c3-9801-7918ef34f7de",
   "metadata": {},
   "outputs": [],
   "source": [
    "wdf.cluster_number = wdf.cluster_number.map(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd85bf8-0e43-43de-b6cf-ce5e351b73dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "fig = px.scatter(whedf.sort_values('warehouse_name'), x=\"timestamp\", y=\"event_name\", color='event_name', hover_data=['cluster_number', 'event_name', 'event_reason'])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a3025f-d09a-492c-be17-1e53043bbc55",
   "metadata": {},
   "outputs": [],
   "source": [
    "wdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722fd861-cc4e-48dc-ad6e-e89a1d9d2c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "wdf['timestamp_est'] = wdf.timestamp + pd.Timedelta(hours=3)\n",
    "\n",
    "fig = px.area(wdf.sort_values(['timestamp', 'cluster_number']), x='timestamp_est', y=['num_queries'], color = 'cluster_number')\n",
    "# fig.update_yaxes(autorange=\"reversed\")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846bd8f6-0a24-44c0-8f3e-dd8469a26b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "wdf.clus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6985e583-f122-4e13-9d2c-1e5c88811f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "[df.ts.min(), df.ts.max()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "994b958d-dc7a-428a-be01-3d1079babd3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# qdf['warehouse_query'] = qdf.warehouse_name + \" : \" + qdf.query_id\n",
    "\n",
    "wdf['timestamp_est'] = wdf.timestamp + pd.Timedelta(hours=3)\n",
    "whedf['timestamp_est'] = whedf.timestamp + pd.Timedelta(hours=3)\n",
    "\n",
    "\n",
    "# wh_name='DEMO'\n",
    "print(wh_name.upper())\n",
    "import plotly.express as px\n",
    "# fig = px.line(df[df.name == wh_name.upper()], x='ts', y='started_clusters')\n",
    "# fig.show()\n",
    "# fig = px.line(df[df.name == wh_name.upper()], x='ts', y=['running', 'queued'])\n",
    "# fig.show()\n",
    "# fig = px.line(df[df.name == wh_name.upper()], x='ts', y=['available', 'provisioning', 'quiescing'])\n",
    "# fig.update_yaxes(autorange=\"reversed\")\n",
    "\n",
    "# fig.show()\n",
    "\n",
    "\n",
    "figures = [\n",
    "    px.line(df[df.name == wh_name.upper()], x='ts', y='started_clusters'),\n",
    "    px.line(df[df.name == wh_name.upper()], x='ts', y=['running', 'queued']),\n",
    "    px.line(df[df.name == wh_name.upper()], x='ts', y=['available', 'provisioning', 'quiescing']),\n",
    "    px.scatter(whedf.sort_values('warehouse_name'), x=\"timestamp_est\", y=\"event_name\", color='event_name', hover_data=['cluster_number', 'event_name', 'event_reason']),\n",
    "    px.area(wdf.sort_values(['timestamp_est', 'cluster_number']), x='timestamp_est', y=['num_queries'], color='cluster_number')\n",
    "    ]\n",
    "\n",
    "fig = make_subplots(rows=len(figures), cols=1, shared_xaxes=True, vertical_spacing=0.05) \n",
    "\n",
    "for i, figure in enumerate(figures):\n",
    "    for trace in range(len(figure[\"data\"])):\n",
    "        fig.append_trace(figure[\"data\"][trace], row=i+1, col=1)\n",
    "\n",
    "\n",
    "fig.update_xaxes(showgrid=True,minor=dict(showgrid=True))\n",
    "fig.update_yaxes(showgrid=True,minor=dict(showgrid=True))\n",
    "fig.update_xaxes(autorange=True)\n",
    "\n",
    "fig.update_xaxes(range=[df.ts.min(), df.ts.max()])\n",
    "fig.update_layout(\n",
    "    height=800,\n",
    ")\n",
    "fig.update_xaxes(type='date', autorange=True)\n",
    "fig.update_yaxes(row=3, col=1, autorange='reversed')\n",
    "\n",
    "# fig.update_layout(\n",
    "#     xaxis_range=[cqe.era_start.min(), cqe.era_end.max()]  # Specify your desired minimum and maximum range\n",
    "# )\n",
    "fig.show()\n",
    "\n",
    "# next: try just one layer of test query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b0d5bb-92f1-46db-ae5b-9ea97e41e1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.name == wh_name.upper()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a70cdf3-43c0-4839-b49c-6d94c22c87ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8ca3da-69b9-4bed-bd3e-d0dd61d2acb9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
