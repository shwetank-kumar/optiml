{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8dea559-e626-4833-bfa2-30845ac00485",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import collections\n",
    "import os\n",
    "# os.chdir('/Users/jaitoor/Documents/dev/aero/aero/dbt/dbt-snowflake-usage/')\n",
    "\n",
    "project_path = \"../../../aero/dbt/dbt-snowflake-usage/\"\n",
    "manifest_path = os.path.join(project_path, 'target/manifest.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9469c7c6-711a-4082-abe9-38143f01a0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import collections\n",
    "import os\n",
    "\n",
    "def get_sql_file_path(node):\n",
    "    # Adjust this function based on your project's structure\n",
    "    # Here, assuming that all SQL files are in a directory named 'models'\n",
    "    return os.path.join('models', node + '.sql')\n",
    "\n",
    "def order_nodes_by_dependency(nodes):\n",
    "    ordered_nodes = []\n",
    "    while nodes:\n",
    "        for node in list(nodes):\n",
    "            if nodes[node].get('depends_on', {}).get('nodes'):\n",
    "                nodes[node]['depends_on']['nodes'] = [n for n in nodes[node]['depends_on']['nodes'] if n in nodes]\n",
    "            else:\n",
    "                ordered_nodes.append(node)\n",
    "                del nodes[node]\n",
    "    return ordered_nodes\n",
    "\n",
    "def order_nodes():\n",
    "    # Adjust the path to your manifest.json file\n",
    "    manifest_file = manifest_path\n",
    "    with open(manifest_file, 'r') as file:\n",
    "        manifest_data = json.load(file)\n",
    "        # Get the dictionary of nodes from manifest_data\n",
    "        nodes = manifest_data.get('nodes', {})\n",
    "        # Order the nodes based on dependencies\n",
    "        ordered_nodes = order_nodes_by_dependency(nodes)\n",
    "        # Combine SQLs into one file\n",
    "        # combined_sql = combine_sql_files(ordered_nodes)\n",
    "        # Write the combined SQL to a new file\n",
    "        # write_to_file(combined_sql, 'combined.sql')\n",
    "        return ordered_nodes\n",
    "\n",
    "with open(manifest_path, 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "node_list = order_nodes()\n",
    "nodes = data['nodes']\n",
    "\n",
    "# counter=1\n",
    "\n",
    "node_sql = {}\n",
    "for node in node_list:\n",
    "    sql_list = []\n",
    "    # print(node)\n",
    "    create_type = nodes[node]['config']['materialized']\n",
    "    if ('compiled_code' in nodes[node].keys()) and create_type in ['view','table','incremental']:   \n",
    "        # print(\"hello\")\n",
    "        create_type = nodes[node]['config']['materialized']\n",
    "        # create_types.append(create_type)\n",
    "        table_name = nodes[node]['name']\n",
    "        schema_name = nodes[node]['schema']\n",
    "        if create_type == 'view':\n",
    "            create_string = f'CREATE OR REPLACE VIEW {schema_name}.{table_name} AS '\n",
    "        if create_type in ['table','incremental']:\n",
    "            create_string = f'CREATE OR REPLACE TRANSIENT TABLE {schema_name}.{table_name} AS '\n",
    "        if create_type in ['test']:\n",
    "            create_string = ''\n",
    "        # sql = nodes[node]['compiled_code']\n",
    "        # print(sql)\n",
    "        if 'unrendered_config' in nodes[node].keys() and 'pre-hook' in nodes[node]['unrendered_config'].keys():\n",
    "            if isinstance(nodes[node]['unrendered_config']['pre-hook'], list):\n",
    "                for hook in nodes[node]['unrendered_config']['pre-hook']:\n",
    "                    sql_list.append(f\"\"\"--{table_name} pre-hook\\n  {hook} ;\\n\"\"\")\n",
    "            else:\n",
    "                sql_list.append(f\"\"\"--{table_name} pre-hook\\n {nodes[node]['unrendered_config']['pre-hook']} ;\\n\"\"\")\n",
    "        if 'compiled_code' in nodes[node].keys():\n",
    "            sql_list.append(f\"\"\"--{table_name} compiled_code \\n {create_string} {nodes[node]['compiled_code']} ;\\n\"\"\")\n",
    "        if 'unrendered_config' in nodes[node].keys() and 'post-hook' in nodes[node]['unrendered_config'].keys():\n",
    "            # print('unrendered_config post-hook')\n",
    "            sql_list.append(f\"\"\"--{table_name} post-hook\\n {nodes[node]['unrendered_config']['post-hook']} ;\\n\"\"\")\n",
    "        node_sql[node] = sql_list\n",
    "        \n",
    "\n",
    "def format_query(query):\n",
    "    query = query.replace(\"\\\\'\", \"''\")\n",
    "    return query\n",
    "\n",
    "# combined_sql\n",
    "# with open('data/queries.py', 'w') as file:\n",
    "#     file.write('def get_native_app_query_list():\\n\\treturn [')\n",
    "#     for query in sql_list:\n",
    "#         file.write('\"\"\"\\n' + format_query(query) + '\"\"\",\\n')\n",
    "#     file.write(']\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9f644b-1c3c-44e3-95e5-3069a13238ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import networkx as nx\n",
    "\n",
    "# Read the dbt manifest file\n",
    "with open(manifest_path) as f:\n",
    "    manifest = json.load(f)\n",
    "\n",
    "# Create a NetworkX graph\n",
    "graph = nx.DiGraph()\n",
    "\n",
    "# Add nodes to the graph\n",
    "for model_name, model_data in manifest['nodes'].items():\n",
    "    if model_data['resource_type'] == 'model':\n",
    "        graph.add_node(model_name)\n",
    "\n",
    "# Add edges between models based on their dependencies\n",
    "for model_name, model_data in manifest['nodes'].items():\n",
    "    for dep in model_data.get('depends_on', {}).get('nodes', []):\n",
    "        if model_data['resource_type'] == 'model':\n",
    "            if not \"source\" in dep:\n",
    "                graph.add_edge(dep, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63758618-bc5a-4db7-a8f1-f8b5cdd4bf05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the node for which to get the subgraph\n",
    "target_node = \"model.dbt_snowflake_usage.warehouse_era\"\n",
    "\n",
    "# Get the ancestors of the target node\n",
    "ancestors = nx.ancestors(graph, target_node)\n",
    "\n",
    "# Add the target node itself to include it in the subgraph\n",
    "ancestors.add(target_node)\n",
    "\n",
    "# Extract the subgraph containing only the target node and its ancestors\n",
    "subgraph = graph.subgraph(ancestors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb04e64-80a4-4c85-9306-44a1f25c0131",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nodes['model.dbt_snowflake_usage.test_results_60d']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ffb0575-63c8-4ecc-b2e6-ca657b619528",
   "metadata": {},
   "outputs": [],
   "source": [
    "execution_order = list(nx.topological_sort(subgraph))\n",
    "execution_order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c09d978-9681-4806-8b84-883581843fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "snowflake_tasks = []\n",
    "\n",
    "def get_task_name(node):\n",
    "    return f\"task_{node.replace('.', '_')}\"\n",
    "for node in execution_order:\n",
    "    print(node)\n",
    "    sql_statements = node_sql[node]\n",
    "\n",
    "\n",
    "    # Find dependencies for the current node\n",
    "    dependencies = list(subgraph.predecessors(node))\n",
    "    print(dependencies)\n",
    "    # Generate \"after\" parameter value\n",
    "    if len(dependencies) > 0:\n",
    "        after_parameter = 'AFTER ' + ', '.join(get_task_name(dep) for dep in dependencies)\n",
    "    else:\n",
    "        after_parameter = ''\n",
    "\n",
    "    for index, sql_statement in enumerate(sql_statements):\n",
    "        sql_statement_without_schema = sql_statement.replace(\"SNOWFLAKE_USAGE.SANDBOX.\",\"\").replace(\";\",\"\").replace(\"SANDBOX.\",\"\")\n",
    "        task_name = get_task_name(node)\n",
    "        snowflake_task = f\"\"\"\n",
    "            CREATE OR REPLACE TASK {task_name} \n",
    "            WAREHOUSE = DEMO\n",
    "            --STATEMENT_EXECUTION_TIMEOUT_IN_SECONDS = <timeout> \n",
    "            {after_parameter} \n",
    "            AS\\n{sql_statement_without_schema};\n",
    "            \"\"\"\n",
    "        after_parameter = f\"AFTER {task_name}\"\n",
    "        snowflake_tasks.append(snowflake_task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c72aea7-0c89-4d99-9d98-46ee1a2241e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\".join(snowflake_tasks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726414b7-646e-4be9-af37-b3d93bec96ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
