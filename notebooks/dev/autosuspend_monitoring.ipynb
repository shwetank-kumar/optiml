{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92fe9cab-eea5-40c4-b396-28b5c6d60242",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys, pathlib\n",
    "sys.path.append(str(pathlib.Path.cwd().parent.parent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d472020-1bc3-4001-836f-08a7036c4b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext dotenv\n",
    "\n",
    "%dotenv ../../env/.env\n",
    "\n",
    "import warnings\n",
    "from pandas import Timedelta\n",
    "# from optiml.utils import sf\n",
    "import time\n",
    "from optiml.utils.sf import logger, sql_to_df, run_sql, conn, session\n",
    "import pandas as pd\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "try:\n",
    "    %load_ext autotime\n",
    "except:\n",
    "    !pip install ipython-autotime\n",
    "    %load_ext autotime\n",
    "\n",
    "import plotly\n",
    "plotly.offline.init_notebook_mode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c87c96-124b-43dd-911d-1cb798ab0178",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to rerun this analysis:\n",
    "\n",
    "# in the *staging* app, rerun dbt in knot account in order to pull latest staging data [<=10min]\n",
    "# run `grant select on all tables in schema optiml_share.optiml to database role optiml_share_role` in the query admin console\n",
    "# run this notebook, connecting to KNOT_SHARE.OPTIML\n",
    "\n",
    "# for the counterfactual analysis, additional steps are needed:\n",
    "\n",
    "# use the knot-dba notebook to copy all *tables* from the share into a target schema [in our account]\n",
    "# run dbt: dbt seed && dbt run --exclude staging daily_rates\n",
    "# run the cluster autosuspend simulation smart-suspend-simulate on that schema\n",
    "# rerun dbt # dbt run -s warehouse_era_simulated+\n",
    "# run this notebook connected to the target schema:\n",
    "\n",
    "# rerun dbt, just selecting views dbt run -s config.materialized:view?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c181df4-9321-47ab-91d0-1fbbe60016d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_sql(\"set lookback_days=60\")\n",
    "\n",
    "# wh_name = 'XOGRP_DEV_WH'\n",
    "wh_name = 'SEGMENT_LOAD_WH'\n",
    "wh_profile = f\"\"\"\n",
    "select \n",
    "    *,\n",
    "    (active_hours - idle_hours)*60 as query_minutes,\n",
    "    idle_hours * 60 as idle_minutes\n",
    "from warehouse_profile_by_hour\n",
    "where warehouse_name = '{wh_name}' \n",
    "and hour_start > dateadd('days',-$lookback_days, current_timestamp())\n",
    "order by hour_start desc;\n",
    "\"\"\"\n",
    "\n",
    "import itables\n",
    "\n",
    "\n",
    "wpdf = sql_to_df(wh_profile)\n",
    "itables.show(wpdf)\n",
    "\n",
    "\n",
    "wh_profile_sim = f\"\"\"\n",
    "\n",
    "with sim_snowflake_suspend as (\n",
    "select \n",
    "    *,\n",
    "    (active_hours - idle_hours)*60 as query_minutes,\n",
    "    idle_hours * 60 as idle_minutes\n",
    "from warehouse_profile_by_hour_sim\n",
    "where warehouse_name = '{wh_name}' \n",
    "and hour_start > dateadd('days',-$lookback_days, current_timestamp())\n",
    "and strategy = {{'autosuspend_sec': 60,   'engine': 'sql',   'type': 'snowflake' }}\n",
    "),\n",
    "sim_aero_managed as (\n",
    "select \n",
    "    *,\n",
    "    (active_hours - idle_hours)*60 as query_minutes,\n",
    "    idle_hours * 60 as idle_minutes\n",
    "from warehouse_profile_by_hour_sim\n",
    "where warehouse_name = '{wh_name}' \n",
    "and hour_start > dateadd('days',-$lookback_days, current_timestamp())\n",
    "and strategy = {{'engine': 'python',   'polling_sec': 1,   'type': 'greedy_after_one_min' }}\n",
    "),\n",
    "\n",
    "actual as (\n",
    "    select \n",
    "        *,\n",
    "        (active_hours - idle_hours)*60 as query_minutes,\n",
    "        idle_hours * 60 as idle_minutes\n",
    "    from warehouse_profile_by_hour\n",
    "    where warehouse_name = '{wh_name}' \n",
    "    and hour_start > dateadd('days',-$lookback_days, current_timestamp())\n",
    ")\n",
    "select\n",
    "    a.*,\n",
    "    a.query_minutes as query_minutes_actual,\n",
    "    a.idle_minutes as idle_minutes_actual,\n",
    "    s.query_minutes as query_minutes_sim_snowflake,\n",
    "    s.idle_minutes as idle_minutes_sim_snowflake,\n",
    "    m.query_minutes as query_minutes_sim_aero,\n",
    "    m.idle_minutes as idle_minutes_sim_aero,\n",
    "    \n",
    "   (abs(idle_minutes_sim_aero - idle_minutes_actual) < abs(idle_minutes_sim_snowflake - idle_minutes_actual)) as aero_presumed_on,\n",
    "    sum( case when aero_presumed_on then  (idle_minutes_sim_snowflake - idle_minutes_actual ) else 0 end) over (order by a.hour_start asc) as cum_idle_minutes_saved,\n",
    "   sum( case when aero_presumed_on then  a.max_cluster_number * (idle_minutes_sim_snowflake - idle_minutes_actual ) else 0  end) over (order by a.hour_start asc) as cum_cluster_idle_minutes_saved,\n",
    "   sum(aero_presumed_on::int) over (order by a.hour_start asc) as hours_on,\n",
    "   (cum_idle_minutes_saved / 60) * wc.credits_per_hour as cum_credits_saved_pessimistic,\n",
    "   (cum_cluster_idle_minutes_saved / 60) * wc.credits_per_hour as cum_credits_saved_upperbound,\n",
    "   div0(cum_credits_saved_pessimistic * (365 * 24), hours_on) as annualized_credits_saved_projected_pessimistic,\n",
    "   div0(cum_credits_saved_upperbound * (365 * 24), hours_on) as annualized_credits_saved_projected_upperbound\n",
    "        \n",
    "    \n",
    "from actual a\n",
    "left join sim_snowflake_suspend s\n",
    "on a.hour_start = s.hour_start\n",
    "left join sim_aero_managed m\n",
    "on a.hour_start = m.hour_start\n",
    "left join warehouse_credits wc\n",
    "on a.warehouse_size = wc.size\n",
    "order by hour_start desc;\n",
    "\n",
    "\"\"\"\n",
    "wpsimdf = sql_to_df(wh_profile_sim)\n",
    "\n",
    "\n",
    "wh_events = f\"\"\"\n",
    "select \n",
    "    current_timestamp(), \n",
    "    *,\n",
    "    event_reason || ':' || nvl(role_name, 'null') as source\n",
    "from stg_warehouse_events_history \n",
    "where event_name = 'SUSPEND_WAREHOUSE'\n",
    "and event_state = 'COMPLETED'\n",
    "and warehouse_name='{wh_name}' \n",
    "and timestamp > dateadd('days',-$lookback_days, current_timestamp())\n",
    "order by timestamp desc;\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "wedf = sql_to_df(wh_events)\n",
    "itables.show(wedf)\n",
    "\n",
    "\n",
    "op_queries = \"\"\"\n",
    "select\n",
    "    start_time,\n",
    "    query_text,\n",
    "    credits_used_cloud_services,\n",
    "    cloud_services_cost,\n",
    "    cloud_services_cost/credits_used_cloud_services as dollars_per_credit,\n",
    "    sum(credits_used_cloud_services) over (order by start_time asc) cum_credits_used_cloud_services,\n",
    "    sum(query_cost) over (order by start_time asc) cum_query_cost\n",
    "from query_history_enriched\n",
    "where (contains(lower(user_name), 'aero') or contains(lower(user_name), 'optiml'))\n",
    "and (contains(lower(query_text), 'show warehouses') or contains(lower(query_text), 'alter warehouse'))\n",
    "and start_time > dateadd('days',-$lookback_days, current_timestamp());\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "opdf = sql_to_df(op_queries)\n",
    "itables.show(opdf)\n",
    "\n",
    "suspension_stats = f\"\"\"\n",
    "with eras as (\n",
    "    select\n",
    "        'query' as type,\n",
    "        warehouse_id,\n",
    "        warehouse_name,\n",
    "        warehouse_sizes,\n",
    "        max_cluster_number,\n",
    "        era_start,\n",
    "        era_end\n",
    "    from query_era\n",
    "    where era_end <= (select max(era_end) from warehouse_era)\n",
    "    \n",
    "    union\n",
    "\n",
    "    select\n",
    "        'warehouse'as type,\n",
    "        warehouse_id,\n",
    "        warehouse_name,\n",
    "        null as max_cluster_number,\n",
    "        null as warehouse_sizes,\n",
    "        era_start,\n",
    "        era_end\n",
    "    from warehouse_era\n",
    "    where era_start > (select min(era_start) from query_era)\n",
    "    and era_end <= (select max(era_end) from query_era)\n",
    "),\n",
    "enriched as (\n",
    "    select \n",
    "        row_number() over(order by warehouse_id, era_end) as era_id,\n",
    "        *,\n",
    "        -- max(era_end) over (partition by warehouse_id)\n",
    "        lag(type) over (partition by warehouse_id order by era_end) as previous_ending_type,\n",
    "        lag(era_end) over (partition by warehouse_id order by era_end) as previous_ending_time,\n",
    "        lag(max_cluster_number) over (partition by warehouse_id order by era_end) as previous_max_cluster_number,\n",
    "        lag(warehouse_sizes) over (partition by warehouse_id order by era_end) as prevous_wh_sizes,\n",
    "        case when type = 'warehouse' and previous_ending_type = 'query' then timediff(milliseconds, previous_ending_time, era_end)/1000 else null end as suspend_lag,\n",
    "        case when type = 'query' and previous_ending_type = 'query' then timediff(milliseconds, previous_ending_time, era_start)/1000 else null end as time_since_last_query,\n",
    "        timediff(seconds, era_start, era_end) as era_seconds\n",
    "    from eras\n",
    ")\n",
    "-- select * from enriched order by era_end desc limit 10;\n",
    ",\n",
    "suspends as (\n",
    "select\n",
    "    warehouse_id,\n",
    "    warehouse_name,\n",
    "    prevous_wh_sizes as warehouse_sizes,\n",
    "    previous_max_cluster_number,\n",
    "    era_end as suspend_time,\n",
    "    suspend_lag\n",
    "from enriched\n",
    "where suspend_lag is not null\n",
    "),\n",
    "suspension_hour_stats as (\n",
    "-- select * from suspends limit 10;\n",
    "select\n",
    "\twarehouse_id,\n",
    "    warehouse_name,\n",
    "    date_trunc('hour', suspend_time) as hour,\n",
    "    array_union_agg(warehouse_sizes) as sizes,\n",
    "    sizes[0]::text as size,\n",
    "    max(previous_max_cluster_number) as clusters,\n",
    "    count(*) as num_suspensions,\n",
    "    sum(previous_max_cluster_number*(60 - suspend_lag)) as max_saved_idle_seconds,\n",
    "    avg(suspend_lag) suspend_lag_avg,\n",
    "    median(suspend_lag) suspend_lag_median,\n",
    "    min(suspend_lag) suspend_lag_min,\n",
    "    percentile_cont(.99) within group(order by suspend_lag) as \"99_pctile\",\n",
    "    max(suspend_lag) suspend_lag_max\n",
    "from suspends s\n",
    "where warehouse_name = '{wh_name}'\n",
    "and suspend_time > dateadd('days',-$lookback_days, current_timestamp())\n",
    "group by 1,2,3\n",
    "),\n",
    "suspension_savings as (\n",
    "select \n",
    "    s.*,\n",
    "    max_saved_idle_seconds * wc.credits_per_hour / 3600 as max_credit_diff,\n",
    "    max_credit_diff * 3 as max_dollar_diff,\n",
    "    sum(max_dollar_diff) over(order by hour asc) as cum_savings\n",
    "from \n",
    "    suspension_hour_stats s\n",
    "left join warehouse_credits wc\n",
    "on s.size = wc.size\n",
    ")\n",
    "select * from suspension_savings\n",
    "order by hour desc;\n",
    "\"\"\"\n",
    "\n",
    "sdf = sql_to_df(suspension_stats)\n",
    "itables.show(sdf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115ba1e0-3a5e-4733-86e2-6cb517a22cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "itables.show(wpsimdf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05442fb-d1d4-4161-b4fd-c5c0ccbf7608",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "fig = px.bar(wpsimdf, x=\"hour_start\", y='dollars_used_compute')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675e1e88-278b-4e6b-b232-d66cda2b995e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "fig = px.scatter(wedf, x=\"timestamp\", y=\"source\")\n",
    "fig.show()\n",
    "\n",
    "fig = px.line(sdf, x=\"hour\", y=[\"suspend_lag_avg\", 'suspend_lag_max', 'suspend_lag_min', 'suspend_lag_median'])\n",
    "fig.show()\n",
    "\n",
    "\n",
    "fig = px.bar(sdf, x=\"hour\", y='num_suspensions')\n",
    "fig.show()\n",
    "\n",
    "NO_SIMULATION_FOUND = wpsimdf.query_minutes_sim_aero.isnull().all()\n",
    "\n",
    "if NO_SIMULATION_FOUND:\n",
    "    print(\"NO AERO SIMULATION FOUND: you may need to run the warehouse simulation notebook and rerun dbt...\")\n",
    "\n",
    "fig = px.line(wpsimdf, x=\"hour_start\", y=['query_minutes_actual', 'query_minutes_sim_snowflake','query_minutes_sim_aero'])\n",
    "fig.show()\n",
    "\n",
    "fig = px.line(wpsimdf, x=\"hour_start\", y=['idle_minutes_actual', 'idle_minutes_sim_snowflake',  'idle_minutes_sim_aero'])\n",
    "fig.show()\n",
    "\n",
    "\n",
    "fig = px.line(wpsimdf, x=\"hour_start\", y=['cum_cluster_idle_minutes_saved'])\n",
    "fig.show()\n",
    "\n",
    "fig = px.line(wpsimdf, x=\"hour_start\", y=['cum_credits_saved_pessimistic', 'cum_credits_saved_upperbound'])\n",
    "fig.show()\n",
    "\n",
    "fig = px.line(wpsimdf, x=\"hour_start\", y=['annualized_credits_saved_projected_pessimistic', 'annualized_credits_saved_projected_upperbound'])\n",
    "fig.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adeb4b6f-81f0-491a-954c-2b9e08063243",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%markdown\n",
    "# Idle Time, Suspension Lag, Savings Estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0242293-69ef-4c4a-805c-0677e4704457",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%markdown\n",
    "# Latency / Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e340ed6-bcdb-42ed-8fe8-6029335e347e",
   "metadata": {},
   "outputs": [],
   "source": [
    "whload = f\"\"\"\n",
    "select \n",
    "    *\n",
    "from stg_warehouse_load_history \n",
    "where warehouse_name = '{wh_name}' \n",
    "    and start_time > dateadd('days',-$lookback_days, current_timestamp())\n",
    "    order by start_time desc;\n",
    "\"\"\"\n",
    "\n",
    "load_df = sql_to_df(whload)\n",
    "\n",
    "whload = f\"\"\"\n",
    "select \n",
    "    date_trunc(hour, start_time) start_hour,\n",
    "    avg(avg_running) as avg_running,\n",
    "    avg(avg_queued_load) as avg_queued_load,\n",
    "    avg(avg_queued_provisioning) as avg_queued_provisioning,\n",
    "    avg(avg_blocked) as avg_blocked\n",
    "from stg_warehouse_load_history \n",
    "where warehouse_name = '{wh_name}' \n",
    "    and start_time > dateadd('days',-$lookback_days, current_timestamp())\n",
    "    group by 1\n",
    "    order by start_hour desc;\n",
    "\"\"\"\n",
    "\n",
    "load_hour_df = sql_to_df(whload)\n",
    "\n",
    "exec_times = f\"\"\"\n",
    "\n",
    "with times as (\n",
    "    select\n",
    "        start_time,\n",
    "        execution_time/1000 as execution_time\n",
    "    from \n",
    "        stg_query_history\n",
    "    where start_time > dateadd('days',-$lookback_days, current_timestamp())\n",
    "    and warehouse_name = '{wh_name}'\n",
    ")\n",
    "select \n",
    "    date_trunc(hour, start_time) as start_hour,\n",
    "    count(*) as num,\n",
    "    avg(execution_time) avg,\n",
    "    median(execution_time) median,\n",
    "    min(execution_time) min,\n",
    "    max(execution_time) max,\n",
    "    percentile_cont(.90) within group(order by execution_time) as \"90_pctile\",\n",
    "    percentile_cont(.75) within group(order by execution_time) as \"75_pctile\",\n",
    "    percentile_cont(.25) within group(order by execution_time) as \"25_pctile\",\n",
    "    percentile_cont(.10) within group(order by execution_time) as \"10_pctile\"\n",
    "from\n",
    "   times\n",
    "group by 1\n",
    "having num > 1000\n",
    "order by 1 desc;\n",
    "\"\"\"\n",
    "exec_times_df = sql_to_df(exec_times)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520922b8-5a01-430e-947b-5d43e0e51ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.line(load_df, x=\"start_time\", y=['avg_running', 'avg_queued_load', 'avg_queued_provisioning', 'avg_blocked'])\n",
    "fig.show()\n",
    "\n",
    "fig = px.line(load_hour_df, x=\"start_hour\", y=['avg_running', 'avg_queued_load', 'avg_queued_provisioning', 'avg_blocked'])\n",
    "fig.show()\n",
    "\n",
    "fig = px.line(exec_times_df, x=\"start_hour\", y=['median', '90_pctile', '75_pctile', '25_pctile', '10_pctile'], title='query latency (seconds)')\n",
    "fig.show()\n",
    "fig = px.line(exec_times_df, x=\"start_hour\", y='num', title='number of queries')\n",
    "fig.show()\n",
    "\n",
    "\n",
    "# fig = px.line(wpsimdf, x=\"hour_start\", y=['query_minutes_actual', 'query_minutes_sim_snowflake','query_minutes_sim_aero'])\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ded42a-1cbe-4884-a45b-ac3dd09e7e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotly.subplots import make_subplots\n",
    "import plotly.express as px\n",
    "\n",
    "\n",
    "figures = [\n",
    "    px.line(sdf, x=\"hour\", y=[\"suspend_lag_avg\", 'suspend_lag_max', 'suspend_lag_min', 'suspend_lag_median']),\n",
    "    px.bar(wpdf, x='hour_start', y='dollars_used_compute'),\n",
    "    px.line(wpdf, x='hour_start', y=['query_minutes', 'idle_minutes']),\n",
    "    px.bar(wpdf, x='hour_start', y= 'idle_minutes'),\n",
    "    px.bar(wpdf, x='hour_start', y='pct_idle'),\n",
    "    px.scatter(wedf, x=\"timestamp\", y=\"source\"),\n",
    "    px.area(opdf, x=\"start_time\", y=\"cum_query_cost\")\n",
    "    ]\n",
    "\n",
    "fig = make_subplots(rows=len(figures), cols=1, shared_xaxes=True, vertical_spacing=0.05,\n",
    "                   subplot_titles=['suspend lag stats', 'dollars_used_compute', \n",
    "                                   'query and idle minutes', 'idle minutes', 'pct_idle', \n",
    "                                   'suspension event sources', 'cumulative operational query cost']) \n",
    "\n",
    "for i, figure in enumerate(figures):\n",
    "    for trace in range(len(figure[\"data\"])):\n",
    "        fig.append_trace(figure[\"data\"][trace], row=i+1, col=1)\n",
    "\n",
    "\n",
    "fig.update_xaxes(showgrid=True,minor=dict(showgrid=True))\n",
    "fig.update_yaxes(showgrid=True,minor=dict(showgrid=True))\n",
    "fig.update_xaxes(autorange=True)\n",
    "fig.update_layout(xaxis_showticklabels=True, \n",
    "                  xaxis2_showticklabels=True,\n",
    "                 xaxis3_showticklabels=True,\n",
    "                  xaxis4_showticklabels=True,\n",
    "                  xaxis5_showticklabels=True\n",
    "                 )\n",
    "\n",
    "# fig.update_xaxes(range=[df.ts.min(), df.ts.max()])\n",
    "fig.update_layout(\n",
    "    height=2000,\n",
    ")\n",
    "fig.update_xaxes(type='date', autorange=True)\n",
    "# fig.update_yaxes(row=3, col=1, autorange='reversed')\n",
    "\n",
    "fig.show()\n",
    "\n",
    "# next: try just one layer of test query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff82b97-7767-4dd1-a954-31ac86e62c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%sh \n",
    "# jupyter nbconvert --to html autosuspend_monitoring.ipynb --no-input --output knot-autosuspend-analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9422f406-4211-4e82-bed9-f4eab6be7a32",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
