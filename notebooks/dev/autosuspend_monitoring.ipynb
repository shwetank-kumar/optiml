{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92fe9cab-eea5-40c4-b396-28b5c6d60242",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys, pathlib\n",
    "sys.path.append(str(pathlib.Path.cwd().parent.parent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d472020-1bc3-4001-836f-08a7036c4b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext dotenv\n",
    "\n",
    "%dotenv ../../env/.env\n",
    "\n",
    "import warnings\n",
    "from pandas import Timedelta\n",
    "# from optiml.utils import sf\n",
    "import time\n",
    "from optiml.utils.sf import logger, sql_to_df, run_sql, conn, session\n",
    "import pandas as pd\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "try:\n",
    "    %load_ext autotime\n",
    "except:\n",
    "    !pip install ipython-autotime\n",
    "    %load_ext autotime\n",
    "\n",
    "import plotly\n",
    "plotly.offline.init_notebook_mode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c87c96-124b-43dd-911d-1cb798ab0178",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to rerun this analysis:\n",
    "\n",
    "# in the *staging* app, rerun dbt in knot account in order to pull latest staging data [<=10min]\n",
    "# run `grant select on all tables in schema optiml_share.optiml to database role optiml_share_role` in the query admin console\n",
    "# run this notebook, connecting to KNOT_SHARE.OPTIML\n",
    "\n",
    "# for the counterfactual analysis, additional steps are needed:\n",
    "\n",
    "# use the knot-dba notebook to copy all *tables* from the share into a target schema [in our account]\n",
    "# run dbt: dbt seed && dbt run --exclude staging daily_rates\n",
    "# run the cluster autosuspend simulation smart-suspend-simulate on that schema\n",
    "# rerun dbt # dbt run -s warehouse_era_simulated+\n",
    "# run this notebook connected to the target schema:\n",
    "\n",
    "# rerun dbt, just selecting views dbt run -s config.materialized:view?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c181df4-9321-47ab-91d0-1fbbe60016d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_sql(\"set lookback_days=30\")\n",
    "\n",
    "wh_name = 'XOGRP_DEV_WH'\n",
    "# wh_name = 'SEGMENT_LOAD_WH'\n",
    "wh_profile = f\"\"\"\n",
    "select \n",
    "    *,\n",
    "    (active_hours - idle_hours)*60 as query_minutes,\n",
    "    idle_hours * 60 as idle_minutes\n",
    "from warehouse_profile_by_hour\n",
    "where warehouse_name = '{wh_name}' \n",
    "and hour_start > dateadd('days',-$lookback_days, current_timestamp())\n",
    "order by hour_start desc;\n",
    "\"\"\"\n",
    "\n",
    "import itables\n",
    "\n",
    "\n",
    "wpdf = sql_to_df(wh_profile)\n",
    "itables.show(wpdf)\n",
    "\n",
    "\n",
    "wh_profile_sim = f\"\"\"\n",
    "\n",
    "with sim_snowflake_suspend as (\n",
    "select \n",
    "    *,\n",
    "    (active_hours - idle_hours)*60 as query_minutes,\n",
    "    idle_hours * 60 as idle_minutes\n",
    "from warehouse_profile_by_hour_sim\n",
    "where warehouse_name = '{wh_name}' \n",
    "and hour_start > dateadd('days',-$lookback_days, current_timestamp())\n",
    "and strategy = {{'autosuspend_sec': 60,   'engine': 'sql',   'type': 'snowflake' }}\n",
    "),\n",
    "sim_aero_managed as (\n",
    "select \n",
    "    *,\n",
    "    (active_hours - idle_hours)*60 as query_minutes,\n",
    "    idle_hours * 60 as idle_minutes\n",
    "from warehouse_profile_by_hour_sim\n",
    "where warehouse_name = '{wh_name}' \n",
    "and hour_start > dateadd('days',-$lookback_days, current_timestamp())\n",
    "and strategy = {{'engine': 'python',   'polling_sec': 1,   'type': 'greedy_after_one_min' }}\n",
    "),\n",
    "\n",
    "actual as (\n",
    "    select \n",
    "        *,\n",
    "        (active_hours - idle_hours)*60 as query_minutes,\n",
    "        idle_hours * 60 as idle_minutes\n",
    "    from warehouse_profile_by_hour\n",
    "    where warehouse_name = '{wh_name}' \n",
    "    and hour_start > dateadd('days',-$lookback_days, current_timestamp())\n",
    ")\n",
    "select\n",
    "    a.*,\n",
    "    a.query_minutes as query_minutes_actual,\n",
    "    a.idle_minutes as idle_minutes_actual,\n",
    "    s.query_minutes as query_minutes_sim_snowflake,\n",
    "    s.idle_minutes as idle_minutes_sim_snowflake,\n",
    "    m.query_minutes as query_minutes_sim_aero,\n",
    "    m.idle_minutes as idle_minutes_sim_aero,\n",
    "    \n",
    "   (abs(idle_minutes_sim_aero - idle_minutes_actual) < abs(idle_minutes_sim_snowflake - idle_minutes_actual)) as aero_presumed_on,\n",
    "    sum( case when aero_presumed_on then  (idle_minutes_sim_snowflake - idle_minutes_actual ) else 0 end) over (order by a.hour_start asc) as cum_idle_minutes_saved,\n",
    "   case when aero_presumed_on then  (idle_minutes_sim_snowflake - idle_minutes_actual ) else 0 end as idle_minutes_saved,\n",
    "   idle_minutes_saved*a.max_cluster_number as cluster_idle_minutes_saved,\n",
    "   sum( case when aero_presumed_on then  a.max_cluster_number * (idle_minutes_sim_snowflake - idle_minutes_actual ) else 0  end) over (order by a.hour_start asc) as cum_cluster_idle_minutes_saved,\n",
    "   sum(aero_presumed_on::int) over (order by a.hour_start asc) as hours_on,\n",
    "   (cum_idle_minutes_saved / 60) * wc.credits_per_hour as cum_credits_saved_pessimistic,\n",
    "   (cum_cluster_idle_minutes_saved / 60) * wc.credits_per_hour as cum_credits_saved_upperbound,\n",
    "   div0(cum_credits_saved_pessimistic * (365 * 24), hours_on) as annualized_credits_saved_projected_pessimistic,\n",
    "   div0(cum_credits_saved_upperbound * (365 * 24), hours_on) as annualized_credits_saved_projected_upperbound\n",
    "        \n",
    "    \n",
    "from actual a\n",
    "left join sim_snowflake_suspend s\n",
    "on a.hour_start = s.hour_start\n",
    "left join sim_aero_managed m\n",
    "on a.hour_start = m.hour_start\n",
    "left join warehouse_credits wc\n",
    "on a.warehouse_size = wc.size\n",
    "order by hour_start desc;\n",
    "\n",
    "\"\"\"\n",
    "wpsimdf = sql_to_df(wh_profile_sim)\n",
    "\n",
    "\n",
    "wh_events = f\"\"\"\n",
    "select \n",
    "    current_timestamp(), \n",
    "    *,\n",
    "    event_reason || ':' || nvl(role_name, 'null') as source\n",
    "from stg_warehouse_events_history \n",
    "where event_name = 'SUSPEND_WAREHOUSE'\n",
    "and event_state = 'COMPLETED'\n",
    "and warehouse_name='{wh_name}' \n",
    "and timestamp > dateadd('days',-$lookback_days, current_timestamp())\n",
    "order by timestamp desc;\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "wedf = sql_to_df(wh_events)\n",
    "itables.show(wedf)\n",
    "\n",
    "\n",
    "op_queries = \"\"\"\n",
    "select\n",
    "    start_time,\n",
    "    query_text,\n",
    "    credits_used_cloud_services,\n",
    "    cloud_services_cost,\n",
    "    cloud_services_cost/credits_used_cloud_services as dollars_per_credit,\n",
    "    sum(credits_used_cloud_services) over (order by start_time asc) cum_credits_used_cloud_services,\n",
    "    sum(query_cost) over (order by start_time asc) cum_query_cost\n",
    "from query_history_enriched\n",
    "where (contains(lower(user_name), 'aero') or contains(lower(user_name), 'optiml'))\n",
    "and (contains(lower(query_text), 'show warehouses') or contains(lower(query_text), 'alter warehouse'))\n",
    "and start_time > dateadd('days',-$lookback_days, current_timestamp());\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "opdf = sql_to_df(op_queries)\n",
    "itables.show(opdf)\n",
    "\n",
    "suspension_stats = f\"\"\"\n",
    "with eras as (\n",
    "    select\n",
    "        'query' as type,\n",
    "        warehouse_id,\n",
    "        warehouse_name,\n",
    "        warehouse_sizes,\n",
    "        max_cluster_number,\n",
    "        era_start,\n",
    "        era_end\n",
    "    from query_era\n",
    "    where era_end <= (select max(era_end) from warehouse_era)\n",
    "    \n",
    "    union\n",
    "\n",
    "    select\n",
    "        'warehouse'as type,\n",
    "        warehouse_id,\n",
    "        warehouse_name,\n",
    "        null as max_cluster_number,\n",
    "        null as warehouse_sizes,\n",
    "        era_start,\n",
    "        era_end\n",
    "    from warehouse_era\n",
    "    where era_start > (select min(era_start) from query_era)\n",
    "    and era_end <= (select max(era_end) from query_era)\n",
    "),\n",
    "enriched as (\n",
    "    select \n",
    "        row_number() over(order by warehouse_id, era_end) as era_id,\n",
    "        *,\n",
    "        -- max(era_end) over (partition by warehouse_id)\n",
    "        lag(type) over (partition by warehouse_id order by era_end) as previous_ending_type,\n",
    "        lag(era_end) over (partition by warehouse_id order by era_end) as previous_ending_time,\n",
    "        lag(max_cluster_number) over (partition by warehouse_id order by era_end) as previous_max_cluster_number,\n",
    "        lag(warehouse_sizes) over (partition by warehouse_id order by era_end) as prevous_wh_sizes,\n",
    "        case when type = 'warehouse' and previous_ending_type = 'query' then timediff(milliseconds, previous_ending_time, era_end)/1000 else null end as suspend_lag,\n",
    "        case when type = 'query' and previous_ending_type = 'query' then timediff(milliseconds, previous_ending_time, era_start)/1000 else null end as time_since_last_query,\n",
    "        timediff(seconds, era_start, era_end) as era_seconds\n",
    "    from eras\n",
    ")\n",
    "-- select * from enriched order by era_end desc limit 10;\n",
    ",\n",
    "suspends as (\n",
    "select\n",
    "    warehouse_id,\n",
    "    warehouse_name,\n",
    "    prevous_wh_sizes as warehouse_sizes,\n",
    "    previous_max_cluster_number,\n",
    "    era_end as suspend_time,\n",
    "    suspend_lag\n",
    "from enriched\n",
    "where suspend_lag is not null\n",
    "),\n",
    "suspension_hour_stats as (\n",
    "-- select * from suspends limit 10;\n",
    "select\n",
    "\twarehouse_id,\n",
    "    warehouse_name,\n",
    "    date_trunc('hour', suspend_time) as hour,\n",
    "    array_union_agg(warehouse_sizes) as sizes,\n",
    "    sizes[0]::text as size,\n",
    "    max(previous_max_cluster_number) as clusters,\n",
    "    count(*) as num_suspensions,\n",
    "    sum(previous_max_cluster_number*(60 - suspend_lag)) as max_saved_idle_seconds,\n",
    "    avg(suspend_lag) suspend_lag_avg,\n",
    "    median(suspend_lag) suspend_lag_median,\n",
    "    min(suspend_lag) suspend_lag_min,\n",
    "    percentile_cont(.99) within group(order by suspend_lag) as \"99_pctile\",\n",
    "    max(suspend_lag) suspend_lag_max\n",
    "from suspends s\n",
    "where warehouse_name = '{wh_name}'\n",
    "and suspend_time > dateadd('days',-$lookback_days, current_timestamp())\n",
    "group by 1,2,3\n",
    "),\n",
    "suspension_savings as (\n",
    "select \n",
    "    s.*,\n",
    "    max_saved_idle_seconds * wc.credits_per_hour / 3600 as max_credit_diff,\n",
    "    max_credit_diff * 3 as max_dollar_diff,\n",
    "    sum(max_dollar_diff) over(order by hour asc) as cum_savings\n",
    "from \n",
    "    suspension_hour_stats s\n",
    "left join warehouse_credits wc\n",
    "on s.size = wc.size\n",
    ")\n",
    "select * from suspension_savings\n",
    "order by hour desc;\n",
    "\"\"\"\n",
    "\n",
    "sdf = sql_to_df(suspension_stats)\n",
    "itables.show(sdf)\n",
    "\n",
    "whload = f\"\"\"\n",
    "select \n",
    "    *\n",
    "from stg_warehouse_load_history \n",
    "where warehouse_name = '{wh_name}' \n",
    "    and start_time > dateadd('days',-$lookback_days, current_timestamp())\n",
    "    order by start_time desc;\n",
    "\"\"\"\n",
    "\n",
    "load_df = sql_to_df(whload)\n",
    "\n",
    "whload = f\"\"\"\n",
    "select \n",
    "    date_trunc(hour, start_time) start_hour,\n",
    "    avg(avg_running) as avg_running,\n",
    "    avg(avg_queued_load) as avg_queued_load,\n",
    "    avg(avg_queued_provisioning) as avg_queued_provisioning,\n",
    "    avg(avg_blocked) as avg_blocked\n",
    "from stg_warehouse_load_history \n",
    "where warehouse_name = '{wh_name}' \n",
    "    and start_time > dateadd('days',-$lookback_days, current_timestamp())\n",
    "    group by 1\n",
    "    order by start_hour desc;\n",
    "\"\"\"\n",
    "\n",
    "load_hour_df = sql_to_df(whload)\n",
    "\n",
    "exec_times = f\"\"\"\n",
    "\n",
    "with times as (\n",
    "    select\n",
    "        start_time,\n",
    "        execution_time/1000 as execution_time\n",
    "    from \n",
    "        stg_query_history\n",
    "    where start_time > dateadd('days',-$lookback_days, current_timestamp())\n",
    "    and warehouse_name = '{wh_name}'\n",
    ")\n",
    "select \n",
    "    date_trunc(hour, start_time) as start_hour,\n",
    "    count(*) as num,\n",
    "    avg(execution_time) avg,\n",
    "    median(execution_time) median,\n",
    "    min(execution_time) min,\n",
    "    max(execution_time) max,\n",
    "    percentile_cont(.90) within group(order by execution_time) as \"90_pctile\",\n",
    "    percentile_cont(.75) within group(order by execution_time) as \"75_pctile\",\n",
    "    percentile_cont(.25) within group(order by execution_time) as \"25_pctile\",\n",
    "    percentile_cont(.10) within group(order by execution_time) as \"10_pctile\"\n",
    "from\n",
    "   times\n",
    "group by 1\n",
    "having num > 1000\n",
    "order by 1 desc;\n",
    "\"\"\"\n",
    "exec_times_df = sql_to_df(exec_times)\n",
    "itables.show(exec_times_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115ba1e0-3a5e-4733-86e2-6cb517a22cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "wpdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6092f4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Suspension start\n",
    "wedf[\"source\"].unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45d01ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "wedf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323a8fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calculates epoch on and off based on number of events coming from Optiml Svc or Snowflake Autosuspend\n",
    "def get_aero_suspend_on_epochs(df, timecolumn=\"hour_start\"):\n",
    "    optiml_on = wedf.groupby([wedf['timestamp'].dt.floor('H'), 'source']).size().unstack().reset_index()\n",
    "    optiml_on[\"optiml_on\"] = optiml_on['WAREHOUSE_AUTOSUSPEND:null'] < optiml_on['WAREHOUSE_SUSPEND:OPTIML_SVC']\n",
    "    optiml_on.fillna(0, inplace=True)\n",
    "    optiml_on.sort_values('timestamp',inplace=True, ascending=False)\n",
    "    df = df.merge(optiml_on, left_on=timecolumn, right_on='timestamp')\n",
    "    df_on = df[df[\"optiml_on\"]==True]\n",
    "    df_off = df[df[\"optiml_on\"]==False]   \n",
    "\n",
    "    return df_on, df_off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f31e10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%markdown\n",
    "# Cost savings estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4307a872",
   "metadata": {},
   "outputs": [],
   "source": [
    "wpdf_on, wpdf_off = get_aero_suspend_on_epochs(wpdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08caaee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def get_cost_hourly(df, pessimistic=False):\n",
    "    if not pessimistic:\n",
    "        cost = np.mean((df[\"idle_minutes\"]+df[\"query_minutes\"])*df[\"max_cluster_number\"]*2.2)/60.\n",
    "    else:    \n",
    "        cost = np.mean((df[\"idle_minutes\"]+df[\"query_minutes\"])*2.2)/60.\n",
    "    \n",
    "    return cost\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cdb0389",
   "metadata": {},
   "outputs": [],
   "source": [
    "cost_off_max = get_cost_hourly(wpdf_off)\n",
    "cost_off_min = get_cost_hourly(wpdf_off, pessimistic=True)\n",
    "# Annualized\n",
    "cost_off = np.mean([cost_off_max, cost_off_min])*24*365"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b72448",
   "metadata": {},
   "outputs": [],
   "source": [
    "cost_on_max = get_cost_hourly(wpdf_on)\n",
    "cost_on_min = get_cost_hourly(wpdf_on, pessimistic=True)\n",
    "# Annualized\n",
    "cost_on = np.mean([cost_on_max, cost_on_min])*24*365"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68066c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "annual_savings = (cost_off - cost_on)\n",
    "pct_savings = annual_savings/cost_off*100\n",
    "cost_on, cost_off, annual_savings, pct_savings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae572c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%markdown\n",
    "# Suspension lag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac9fd8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed3205a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_on, sdf_off = get_aero_suspend_on_epochs(sdf, \"hour\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e58bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "suspension_lag_on = np.median(sdf_on[\"suspend_lag_avg\"])\n",
    "suspension_lag_off = np.median(sdf_off[\"suspend_lag_avg\"])\n",
    "change_suspension_lag = suspension_lag_off - suspension_lag_on\n",
    "pct_suspension_lag = change_suspension_lag/suspension_lag_off*100\n",
    "suspension_lag_off, suspension_lag_on, change_suspension_lag, pct_suspension_lag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b185176",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%markdown\n",
    "# Idling minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d682a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "wpdf_on, wpdf_off = get_aero_suspend_on_epochs(wpdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f81715c",
   "metadata": {},
   "outputs": [],
   "source": [
    "wpdf.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130f6ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pct_idle_off = np.mean(wpdf_off[\"pct_idle\"])\n",
    "pct_idle_on = np.mean(wpdf_on[\"pct_idle\"])\n",
    "pct_idle_change = pct_idle_off - pct_idle_on\n",
    "pct_idle_off, pct_idle_on, pct_idle_change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601e965f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# median_hourly_idle_minutes_on = np.median(wpdf_on[\"idle_minutes\"])\n",
    "# median_hourly_idle_minutes_off = np.median(wpdf_off[\"idle_minutes\"])\n",
    "# annualized_idle_minutes_on = median_hourly_idle_minutes_on*24*265\n",
    "# annualized_idle_minutes_off = median_hourly_idle_minutes_off*24*265\n",
    "# annual_minutes_saved = annualized_idle_minutes_off - annualized_idle_minutes_on\n",
    "# annualized_idle_minutes_on, annualized_idle_minutes_off, annual_minutes_saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657acda8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%markdown\n",
    "# Query latency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd66f15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "exec_times_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62899e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "exec_times_df_on, exec_times_df_off = get_aero_suspend_on_epochs(exec_times_df, 'start_hour')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df18e600",
   "metadata": {},
   "outputs": [],
   "source": [
    "# median_query_latency_off = np.median(exec_times_df_off[\"90_pctile\"])\n",
    "# median_query_latency_on = np.median(exec_times_df_on[\"90_pctile\"])\n",
    "# change_median_query_latency = median_query_latency_off-median_query_latency_on\n",
    "# pct_change_median_query_latency = change_median_query_latency/median_query_latency_off*100\n",
    "# median_query_latency_on, median_query_latency_off, change_median_query_latency, pct_change_median_query_latency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dbf280d",
   "metadata": {},
   "outputs": [],
   "source": [
    "median_query_latency_off = np.median(exec_times_df_off[\"median\"])\n",
    "median_query_latency_on = np.median(exec_times_df_on[\"median\"])\n",
    "change_median_query_latency = median_query_latency_off-median_query_latency_on\n",
    "pct_change_median_query_latency = change_median_query_latency/median_query_latency_off*100\n",
    "median_query_latency_on, median_query_latency_off, change_median_query_latency, pct_change_median_query_latency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a801c8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%markdown\n",
    "# Query load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046ff02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_hour_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed11d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_hour_df_on, load_hour_df_off = get_aero_suspend_on_epochs(load_hour_df, 'start_hour')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7eb7a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_running_load_off = np.mean(load_hour_df_off[\"avg_running\"])\n",
    "avg_running_load_on = np.mean(load_hour_df_on[\"avg_running\"])\n",
    "pct_change = (avg_running_load_off - avg_running_load_on)/avg_running_load_off*100\n",
    "avg_running_load_off, avg_running_load_on, pct_change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bddbdf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_queued_load_off = np.mean(load_hour_df_off[\"avg_queued_load\"])\n",
    "avg_queued_load_on = np.mean(load_hour_df_on[\"avg_queued_load\"])\n",
    "pct_change = (avg_queued_load_off - avg_queued_load_on)/avg_queued_load_off*100\n",
    "avg_queued_load_off, avg_queued_load_on, pct_change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "069a8555",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%markdown\n",
    "# Costs with and without Aero on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c8f46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_sql(\"set lookback_days=50\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591deb72",
   "metadata": {},
   "outputs": [],
   "source": [
    "wh_profile = f\"\"\"\n",
    "select \n",
    "    *,\n",
    "    (active_hours - idle_hours)*60 as query_minutes,\n",
    "    idle_hours * 60 as idle_minutes\n",
    "from warehouse_profile_by_hour\n",
    "where warehouse_name = '{wh_name}' \n",
    "and hour_start > dateadd('days',-$lookback_days, current_timestamp())\n",
    "order by hour_start desc;\n",
    "\"\"\"\n",
    "\n",
    "wpdf = sql_to_df(wh_profile)\n",
    "itables.show(wpdf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4eca1f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "wpdf.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7fcdea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "wpdf.sort_values(\"hour_start\",inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2d9ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "wpdf_epochs_on, wpdf_epochs_off = get_aero_suspend_on_epochs(wpdf,\"hour_start\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a73571",
   "metadata": {},
   "outputs": [],
   "source": [
    "wpdf_epochs_on.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0d12f7cf",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e14c729",
   "metadata": {},
   "outputs": [],
   "source": [
    "optiml_start = wpdf_epochs_on.iloc[0][\"hour_start\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3d68c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "wpdf_off  = wpdf[wpdf[\"hour_start\"] < optiml_start]\n",
    "wpdf_on  = wpdf[wpdf[\"hour_start\"] >= optiml_start]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad636e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "wpdf_on[\"cumulative_dollars_used_compute\"] = wpdf_on[\"dollars_used_compute\"].cumsum(axis=0)\n",
    "wpdf_off[\"cumulative_dollars_used_compute\"] = wpdf_off[\"dollars_used_compute\"].cumsum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52974247",
   "metadata": {},
   "outputs": [],
   "source": [
    "wpdf_on[\"time_delta\"] = (wpdf_on[\"hour_start\"] - wpdf_on.iloc[0][\"hour_start\"])/ pd.Timedelta(hours=1)\n",
    "wpdf_off[\"time_delta\"] = (wpdf_off[\"hour_start\"] - wpdf_off.iloc[0][\"hour_start\"])/ pd.Timedelta(hours=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d100465a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "df_concat = pd.concat([wpdf_off, wpdf_on], keys=['Aero off', 'Aero on'])\n",
    "\n",
    "fig = px.line(df_concat, x='time_delta', y='cumulative_dollars_used_compute', color=df_concat.index.get_level_values(0), title='Comparison of Columns')\n",
    "fig.update_layout(yaxis_title='Dollars')\n",
    "fig.update_layout(xaxis_title='Hours')\n",
    "fig.update_layout(title='Observational difference in cost after turning on Aero')\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d29b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%markdown\n",
    "## ================ Previous analyses ======================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adeb4b6f-81f0-491a-954c-2b9e08063243",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%markdown\n",
    "# Idle Time, Suspension Lag, Savings Estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675e1e88-278b-4e6b-b232-d66cda2b995e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "\n",
    "fig = px.line(sdf, x=\"hour\", y=[\"suspend_lag_avg\", 'suspend_lag_max', 'suspend_lag_min', 'suspend_lag_median'])\n",
    "fig.show()\n",
    "\n",
    "fig = px.bar(sdf, x=\"hour\", y='num_suspensions')\n",
    "fig.show()\n",
    "\n",
    "NO_SIMULATION_FOUND = wpsimdf.query_minutes_sim_aero.isnull().all()\n",
    "\n",
    "if NO_SIMULATION_FOUND:\n",
    "    print(\"NO AERO SIMULATION FOUND: you may need to run the warehouse simulation notebook and rerun dbt...\")\n",
    "\n",
    "fig = px.line(wpsimdf, x=\"hour_start\", y=['query_minutes_actual', 'query_minutes_sim_snowflake','query_minutes_sim_aero'])\n",
    "fig.show()\n",
    "\n",
    "fig = px.line(wpsimdf, x=\"hour_start\", y=['idle_minutes_actual', 'idle_minutes_sim_snowflake',  'idle_minutes_sim_aero'])\n",
    "fig.show()\n",
    "\n",
    "\n",
    "fig = px.line(wpsimdf, x=\"hour_start\", y=['cum_cluster_idle_minutes_saved'])\n",
    "fig.show()\n",
    "\n",
    "fig = px.line(wpsimdf, x=\"hour_start\", y=['cum_credits_saved_pessimistic', 'cum_credits_saved_upperbound'])\n",
    "fig.show()\n",
    "\n",
    "fig = px.line(wpsimdf, x=\"hour_start\", y=['annualized_credits_saved_projected_pessimistic', 'annualized_credits_saved_projected_upperbound'])\n",
    "fig.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0242293-69ef-4c4a-805c-0677e4704457",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%markdown\n",
    "# Latency / Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e340ed6-bcdb-42ed-8fe8-6029335e347e",
   "metadata": {},
   "outputs": [],
   "source": [
    "whload = f\"\"\"\n",
    "select \n",
    "    *\n",
    "from stg_warehouse_load_history \n",
    "where warehouse_name = '{wh_name}' \n",
    "    and start_time > dateadd('days',-$lookback_days, current_timestamp())\n",
    "    order by start_time desc;\n",
    "\"\"\"\n",
    "\n",
    "load_df = sql_to_df(whload)\n",
    "\n",
    "whload = f\"\"\"\n",
    "select \n",
    "    date_trunc(hour, start_time) start_hour,\n",
    "    avg(avg_running) as avg_running,\n",
    "    avg(avg_queued_load) as avg_queued_load,\n",
    "    avg(avg_queued_provisioning) as avg_queued_provisioning,\n",
    "    avg(avg_blocked) as avg_blocked\n",
    "from stg_warehouse_load_history \n",
    "where warehouse_name = '{wh_name}' \n",
    "    and start_time > dateadd('days',-$lookback_days, current_timestamp())\n",
    "    group by 1\n",
    "    order by start_hour desc;\n",
    "\"\"\"\n",
    "\n",
    "load_hour_df = sql_to_df(whload)\n",
    "\n",
    "exec_times = f\"\"\"\n",
    "\n",
    "with times as (\n",
    "    select\n",
    "        start_time,\n",
    "        execution_time/1000 as execution_time\n",
    "    from \n",
    "        stg_query_history\n",
    "    where start_time > dateadd('days',-$lookback_days, current_timestamp())\n",
    "    and warehouse_name = '{wh_name}'\n",
    ")\n",
    "select \n",
    "    date_trunc(hour, start_time) as start_hour,\n",
    "    count(*) as num,\n",
    "    avg(execution_time) avg,\n",
    "    median(execution_time) median,\n",
    "    min(execution_time) min,\n",
    "    max(execution_time) max,\n",
    "    percentile_cont(.90) within group(order by execution_time) as \"90_pctile\",\n",
    "    percentile_cont(.75) within group(order by execution_time) as \"75_pctile\",\n",
    "    percentile_cont(.25) within group(order by execution_time) as \"25_pctile\",\n",
    "    percentile_cont(.10) within group(order by execution_time) as \"10_pctile\"\n",
    "from\n",
    "   times\n",
    "group by 1\n",
    "having num > 1000\n",
    "order by 1 desc;\n",
    "\"\"\"\n",
    "exec_times_df = sql_to_df(exec_times)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520922b8-5a01-430e-947b-5d43e0e51ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.line(load_df, x=\"start_time\", y=['avg_running', 'avg_queued_load', 'avg_queued_provisioning', 'avg_blocked'])\n",
    "fig.show()\n",
    "\n",
    "fig = px.line(load_hour_df, x=\"start_hour\", y=['avg_running', 'avg_queued_load', 'avg_queued_provisioning', 'avg_blocked'])\n",
    "fig.show()\n",
    "\n",
    "fig = px.line(exec_times_df, x=\"start_hour\", y=['median', '90_pctile', '75_pctile', '25_pctile', '10_pctile'], title='query latency (seconds)')\n",
    "fig.show()\n",
    "fig = px.line(exec_times_df, x=\"start_hour\", y='num', title='number of queries')\n",
    "fig.show()\n",
    "\n",
    "\n",
    "# fig = px.line(wpsimdf, x=\"hour_start\", y=['query_minutes_actual', 'query_minutes_sim_snowflake','query_minutes_sim_aero'])\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ded42a-1cbe-4884-a45b-ac3dd09e7e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotly.subplots import make_subplots\n",
    "import plotly.express as px\n",
    "\n",
    "\n",
    "figures = [\n",
    "    px.line(sdf, x=\"hour\", y=[\"suspend_lag_avg\", 'suspend_lag_max', 'suspend_lag_min', 'suspend_lag_median']),\n",
    "    px.bar(wpdf, x='hour_start', y='dollars_used_compute'),\n",
    "    px.line(wpdf, x='hour_start', y=['query_minutes', 'idle_minutes']),\n",
    "    px.bar(wpdf, x='hour_start', y= 'idle_minutes'),\n",
    "    px.bar(wpdf, x='hour_start', y='pct_idle'),\n",
    "    px.scatter(wedf, x=\"timestamp\", y=\"source\"),\n",
    "    px.area(opdf, x=\"start_time\", y=\"cum_query_cost\")\n",
    "    ]\n",
    "\n",
    "fig = make_subplots(rows=len(figures), cols=1, shared_xaxes=True, vertical_spacing=0.05,\n",
    "                   subplot_titles=['suspend lag stats', 'dollars_used_compute', \n",
    "                                   'query and idle minutes', 'idle minutes', 'pct_idle', \n",
    "                                   'suspension event sources', 'cumulative operational query cost']) \n",
    "\n",
    "for i, figure in enumerate(figures):\n",
    "    for trace in range(len(figure[\"data\"])):\n",
    "        fig.append_trace(figure[\"data\"][trace], row=i+1, col=1)\n",
    "\n",
    "\n",
    "fig.update_xaxes(showgrid=True,minor=dict(showgrid=True))\n",
    "fig.update_yaxes(showgrid=True,minor=dict(showgrid=True))\n",
    "fig.update_xaxes(autorange=True)\n",
    "fig.update_layout(xaxis_showticklabels=True, \n",
    "                  xaxis2_showticklabels=True,\n",
    "                 xaxis3_showticklabels=True,\n",
    "                  xaxis4_showticklabels=True,\n",
    "                  xaxis5_showticklabels=True\n",
    "                 )\n",
    "\n",
    "# fig.update_xaxes(range=[df.ts.min(), df.ts.max()])\n",
    "fig.update_layout(\n",
    "    height=2000,\n",
    ")\n",
    "fig.update_xaxes(type='date', autorange=True)\n",
    "# fig.update_yaxes(row=3, col=1, autorange='reversed')\n",
    "\n",
    "fig.show()\n",
    "\n",
    "# next: try just one layer of test query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff82b97-7767-4dd1-a954-31ac86e62c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%sh \n",
    "# jupyter nbconvert --to html autosuspend_monitoring.ipynb --no-input --output knot-autosuspend-analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22d60dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
