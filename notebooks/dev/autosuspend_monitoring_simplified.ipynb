{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92fe9cab-eea5-40c4-b396-28b5c6d60242",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys, pathlib\n",
    "sys.path.append(str(pathlib.Path.cwd().parent.parent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d472020-1bc3-4001-836f-08a7036c4b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext dotenv\n",
    "\n",
    "%dotenv ../../env/.env\n",
    "\n",
    "import warnings\n",
    "from pandas import Timedelta\n",
    "# from optiml.utils import sf\n",
    "import time\n",
    "from optiml.utils.sf import logger, sql_to_df, run_sql, conn, session\n",
    "import pandas as pd\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "try:\n",
    "    %load_ext autotime\n",
    "except:\n",
    "    !pip install ipython-autotime\n",
    "    %load_ext autotime\n",
    "\n",
    "import plotly\n",
    "plotly.offline.init_notebook_mode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c87c96-124b-43dd-911d-1cb798ab0178",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to rerun this analysis:\n",
    "\n",
    "# in the *staging* app, rerun dbt in knot account in order to pull latest staging data [<=10min]\n",
    "# run `grant select on all tables in schema optiml_share.optiml to database role optiml_share_role` in the query admin console\n",
    "# run this notebook, connecting to KNOT_SHARE.OPTIML\n",
    "\n",
    "# for the counterfactual analysis, additional steps are needed:\n",
    "\n",
    "# use the knot-dba notebook to copy all *tables* from the share into a target schema [in our account]\n",
    "# run dbt: dbt seed && dbt run --exclude staging daily_rates\n",
    "# run the cluster autosuspend simulation smart-suspend-simulate on that schema\n",
    "# rerun dbt # dbt run -s warehouse_era_simulated+\n",
    "# run this notebook connected to the target schema:\n",
    "\n",
    "# rerun dbt, just selecting views dbt run -s config.materialized:view?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835a1e17-5086-46a0-bd0b-f19e1d904277",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_sql(\"set lookback_days=60\")\n",
    "\n",
    "# run_sql(\"set wh_name='SEGMENT_LOAD_WH' \")\n",
    "run_sql(\"set wh_name='XOGRP_DEV_WH' \")\n",
    "\n",
    "run_sql(\"set date_part = 'hour'; \")\n",
    "\n",
    "merge_cte = \"\"\"\n",
    "with whp as (\n",
    "select \n",
    "    date_trunc($date_part, hour_start) as ts,\n",
    "    sum(dollars_used_compute) as dollars_used_compute,\n",
    "    sum(active_hours - idle_hours)*60 as query_minutes,\n",
    "    sum(idle_hours) * 60 as idle_minutes,\n",
    "    100*sum(idle_hours)/sum(active_hours) as pct_idle\n",
    "\n",
    "from warehouse_profile_by_hour\n",
    "where warehouse_name = $wh_name\n",
    "and hour_start > dateadd('days',-$lookback_days, current_timestamp())\n",
    "group by 1\n",
    "),\n",
    "whe as (\n",
    "    select \n",
    "        date_trunc($date_part, timestamp) as ts,\n",
    "        -- date_trunc(hour, timestamp) as day,    \n",
    "        -- event_reason || ':' || nvl(role_name, 'null') as source,\n",
    "        count(*) as num_suspensions,\n",
    "        count_if(role_name='OPTIML_SVC') as num_aero_suspensions,\n",
    "        num_suspensions - num_aero_suspensions num_nonaero_suspensions,\n",
    "        100*num_aero_suspensions/num_suspensions as pct_aero_suspensions\n",
    "    from stg_warehouse_events_history \n",
    "    where event_name = 'SUSPEND_WAREHOUSE'\n",
    "    and event_state = 'COMPLETED'\n",
    "    and warehouse_name=$wh_name\n",
    "    and timestamp > dateadd('days',-$lookback_days, current_timestamp())\n",
    "    group by 1\n",
    "),\n",
    "qh as \n",
    "(\n",
    "    select\n",
    "        start_time,\n",
    "        execution_time/1000 as execution_time, \n",
    "        PERCENTAGE_SCANNED_FROM_CACHE\n",
    "    from \n",
    "        stg_query_history\n",
    "    where start_time > dateadd('days',-$lookback_days, current_timestamp())\n",
    "    and warehouse_name = $wh_name\n",
    "    and warehouse_size is not null\n",
    "),\n",
    "qh_stats as (\n",
    "    select \n",
    "        date_trunc($date_part, start_time) as ts,\n",
    "        count(*) as num_queries,\n",
    "        avg(execution_time) avg_execution_time,\n",
    "        median(execution_time) median_execution_time,\n",
    "        min(execution_time) min_execution_time,\n",
    "        max(execution_time) max_execution_time,\n",
    "        percentile_cont(.90) within group(order by execution_time) as p90_execution_time,\n",
    "        percentile_cont(.75) within group(order by execution_time) as p75_execution_time,\n",
    "        percentile_cont(.25) within group(order by execution_time) as p25_execution_time,\n",
    "        percentile_cont(.10) within group(order by execution_time) as p10_execution_time,\n",
    "        avg(PERCENTAGE_SCANNED_FROM_CACHE) as avg_pct_scanned_cache,\n",
    "        count_if(PERCENTAGE_SCANNED_FROM_CACHE > .50) num_queries_majority_cached,\n",
    "        100*num_queries_majority_cached/num_queries pct_queries_majority_cached\n",
    "    from\n",
    "       qh\n",
    "    group by 1\n",
    "    having num_queries > 1000\n",
    "    order by 1 desc\n",
    "),\n",
    "merged as (\n",
    "select\n",
    "    whp.ts as timestamp,\n",
    "    pct_aero_suspensions > 0 as aero_on,\n",
    "    dollars_used_compute,\n",
    "    query_minutes,\n",
    "    idle_minutes,\n",
    "    pct_idle,\n",
    "    num_suspensions,\n",
    "    num_aero_suspensions,\n",
    "    num_nonaero_suspensions,\n",
    "    pct_aero_suspensions,\n",
    "    num_queries,\n",
    "    avg_execution_time,\n",
    "    median_execution_time,\n",
    "    min_execution_time,\n",
    "    max_execution_time,\n",
    "    p90_execution_time,\n",
    "    p75_execution_time,\n",
    "    p25_execution_time,\n",
    "    p10_execution_time,\n",
    "    avg_pct_scanned_cache,\n",
    "    num_queries_majority_cached,\n",
    "    pct_queries_majority_cached\n",
    "from whp\n",
    "left join whe\n",
    "on whp.ts = whe.ts\n",
    "left join qh_stats q\n",
    "on whp.ts = q.ts\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "sql = f\"\"\"\n",
    "{merge_cte}\n",
    "select * from merged\n",
    "order by timestamp asc\n",
    "\"\"\"\n",
    "\n",
    "summary_sql = f\"\"\"\n",
    "{merge_cte}\n",
    "select \n",
    "    pct_aero_suspensions > 0 as aero_on,\n",
    "    count(*) as num_periods,\n",
    "    avg(query_minutes) as avg_hourly_query_minutes,\n",
    "    avg(idle_minutes) as avg_hourly_idle_minutes,\n",
    "    100*sum(idle_minutes)/(sum(query_minutes) + sum(idle_minutes)) as pct_idle,\n",
    "    avg(dollars_used_compute) avg_hourly_dollars_used_compute,\n",
    "    median(dollars_used_compute) median_hourly_dollars_used_compute,\n",
    "    stddev(dollars_used_compute) stddev_hourly_dollars_used_compute,\n",
    "    100*sum(num_queries_majority_cached)/sum(num_queries) as pct_queries_majority_cached\n",
    "from \n",
    "    merged\n",
    "group by 1;\n",
    "\"\"\"\n",
    "\n",
    "wpdf_hour = sql_to_df(sql)\n",
    "\n",
    "summary = sql_to_df(summary_sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29474da-8387-4e6c-b3d6-ed64e03fd8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c843075-a874-4503-a4c9-cc0eaa033c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "for c in summary.columns:\n",
    "    if c != 'aero_on':\n",
    "        fig = px.bar(summary, y='aero_on', x=c, orientation='h', height=300, width=1000, text=c)\n",
    "        fig.update_traces( textposition='inside')\n",
    "        fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "067a65d6-39c3-4c15-9412-27d73cc481f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "for hist_col in ['dollars_used_compute', 'pct_idle', 'idle_minutes', 'query_minutes', 'median_execution_time', 'pct_queries_majority_cached']:\n",
    "    # wpdf.aero_on = wpdf.aero_on.map(bool)\n",
    "    x0 = wpdf_hour[hist_col][wpdf_hour.aero_on==False]\n",
    "    # Add 1 to shift the mean of the Gaussian distribution\n",
    "    x1 = wpdf_hour[hist_col][wpdf_hour.aero_on==True]\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Histogram(x=x0, histnorm='probability', marker=dict(color='red'),name='aero off'))\n",
    "    fig.add_trace(go.Histogram(x=x1, histnorm='probability',marker=dict(color='green'), name = 'aero on'))\n",
    "\n",
    "    # Overlay both histograms\n",
    "    fig.update_layout(barmode='overlay', height=400, width=600)\n",
    "    # Reduce opacity to see both histograms\n",
    "    fig.update_traces(opacity=0.75)\n",
    "    fig.update_layout(xaxis_title_text = hist_col)\n",
    "    fig.update_layout(yaxis_title_text = 'percent of hours')\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947f9d08-ef51-4fe3-87d1-b4c3b573539a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x0.mean())\n",
    "print(x0.median())\n",
    "\n",
    "print(x1.mean())\n",
    "print(x1.median())\n",
    "\n",
    "print(len(x0))\n",
    "print(len(x1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff07e0f-da81-43a8-b1b2-28b2ba3777ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_sql(\"set date_part = 'day'; \")\n",
    "wpdf = sql_to_df(sql)\n",
    "\n",
    "whload = f\"\"\"\n",
    "select \n",
    "    date_trunc(hour, start_time) start_hour,\n",
    "    avg(avg_running) as avg_running,\n",
    "    avg(avg_queued_load) as avg_queued_load,\n",
    "    avg(avg_queued_provisioning) as avg_queued_provisioning,\n",
    "    avg(avg_blocked) as avg_blocked\n",
    "from stg_warehouse_load_history \n",
    "where warehouse_name = $wh_name \n",
    "    and start_time > dateadd('days',-$lookback_days, current_timestamp())\n",
    "    group by 1\n",
    "    order by start_hour desc;\n",
    "\"\"\"\n",
    "\n",
    "load_hour_df = sql_to_df(whload)\n",
    "\n",
    "suspension_stats = f\"\"\"\n",
    "with eras as (\n",
    "    select\n",
    "        'query' as type,\n",
    "        warehouse_id,\n",
    "        warehouse_name,\n",
    "        warehouse_sizes,\n",
    "        max_cluster_number,\n",
    "        era_start,\n",
    "        era_end\n",
    "    from query_era\n",
    "    where era_end <= (select max(era_end) from warehouse_era)\n",
    "    \n",
    "    union\n",
    "\n",
    "    select\n",
    "        'warehouse'as type,\n",
    "        warehouse_id,\n",
    "        warehouse_name,\n",
    "        null as max_cluster_number,\n",
    "        null as warehouse_sizes,\n",
    "        era_start,\n",
    "        era_end\n",
    "    from warehouse_era\n",
    "    where era_start > (select min(era_start) from query_era)\n",
    "    and era_end <= (select max(era_end) from query_era)\n",
    "),\n",
    "enriched as (\n",
    "    select \n",
    "        row_number() over(order by warehouse_id, era_end) as era_id,\n",
    "        *,\n",
    "        -- max(era_end) over (partition by warehouse_id)\n",
    "        lag(type) over (partition by warehouse_id order by era_end) as previous_ending_type,\n",
    "        lag(era_end) over (partition by warehouse_id order by era_end) as previous_ending_time,\n",
    "        lag(max_cluster_number) over (partition by warehouse_id order by era_end) as previous_max_cluster_number,\n",
    "        lag(warehouse_sizes) over (partition by warehouse_id order by era_end) as prevous_wh_sizes,\n",
    "        case when type = 'warehouse' and previous_ending_type = 'query' then timediff(milliseconds, previous_ending_time, era_end)/1000 else null end as suspend_lag,\n",
    "        case when type = 'query' and previous_ending_type = 'query' then timediff(milliseconds, previous_ending_time, era_start)/1000 else null end as time_since_last_query,\n",
    "        timediff(seconds, era_start, era_end) as era_seconds\n",
    "    from eras\n",
    ")\n",
    "-- select * from enriched order by era_end desc limit 10;\n",
    ",\n",
    "suspends as (\n",
    "select\n",
    "    warehouse_id,\n",
    "    warehouse_name,\n",
    "    prevous_wh_sizes as warehouse_sizes,\n",
    "    previous_max_cluster_number,\n",
    "    era_end as suspend_time,\n",
    "    suspend_lag\n",
    "from enriched\n",
    "where suspend_lag is not null\n",
    "),\n",
    "suspension_hour_stats as (\n",
    "-- select * from suspends limit 10;\n",
    "select\n",
    "\twarehouse_id,\n",
    "    warehouse_name,\n",
    "    date_trunc('hour', suspend_time) as hour,\n",
    "    array_union_agg(warehouse_sizes) as sizes,\n",
    "    sizes[0]::text as size,\n",
    "    max(previous_max_cluster_number) as clusters,\n",
    "    count(*) as num_suspensions,\n",
    "    sum(previous_max_cluster_number*(60 - suspend_lag)) as max_saved_idle_seconds,\n",
    "    avg(suspend_lag) suspend_lag_avg,\n",
    "    median(suspend_lag) suspend_lag_median,\n",
    "    min(suspend_lag) suspend_lag_min,\n",
    "    percentile_cont(.99) within group(order by suspend_lag) as \"99_pctile\",\n",
    "    max(suspend_lag) suspend_lag_max\n",
    "from suspends s\n",
    "where warehouse_name = $wh_name\n",
    "and suspend_time > dateadd('days',-$lookback_days, current_timestamp())\n",
    "group by 1,2,3\n",
    "),\n",
    "suspension_savings as (\n",
    "select \n",
    "    s.*,\n",
    "    max_saved_idle_seconds * wc.credits_per_hour / 3600 as max_credit_diff,\n",
    "    max_credit_diff * 3 as max_dollar_diff,\n",
    "    sum(max_dollar_diff) over(order by hour asc) as cum_savings\n",
    "from \n",
    "    suspension_hour_stats s\n",
    "left join warehouse_credits wc\n",
    "on s.size = wc.size\n",
    ")\n",
    "select * from suspension_savings\n",
    "order by hour desc;\n",
    "\"\"\"\n",
    "\n",
    "sdf = sql_to_df(suspension_stats)\n",
    "# itables.show(sdf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c45ae4-cf55-4dc6-ab00-949a865ebdfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "wpdf.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09064d30-c44b-4ce9-b2c7-47417f8d82ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "from pandas import Timedelta\n",
    "aero_on_ts = wpdf['timestamp'][wpdf.aero_on==True].to_list()\n",
    "\n",
    "def color_ontime(fig):\n",
    "    for on_time in aero_on_ts:\n",
    "        fig.add_vrect(\n",
    "            x0=on_time,\n",
    "            x1=on_time + Timedelta(days = 1),\n",
    "            fillcolor=\"green\",\n",
    "            opacity=0.5,\n",
    "            line_width=0,\n",
    "        )\n",
    "\n",
    "for mistyped_col in ['query_minutes', 'idle_minutes', \"num_nonaero_suspensions\", \"num_aero_suspensions\"]:\n",
    "    wpdf[mistyped_col] = wpdf[mistyped_col].map(float)\n",
    "    \n",
    "    # p90_execution_time,\n",
    "    # p75_execution_time,\n",
    "    # p25_execution_time,\n",
    "    # p10_execution_time,\n",
    "    # avg_pct_scanned_cache,\n",
    "    # pct_queries_majority_cached\n",
    "    \n",
    "for col in [[\"num_nonaero_suspensions\", \"num_aero_suspensions\"], 'dollars_used_compute', 'pct_idle', ['query_minutes', 'idle_minutes'], 'num_queries', 'avg_pct_scanned_cache', 'pct_queries_majority_cached']:\n",
    "    fig = px.bar(wpdf, x='timestamp', y=col)\n",
    "    color_ontime(fig)\n",
    "    fig.show()\n",
    "\n",
    "for col in [['p90_execution_time', 'p75_execution_time', 'p25_execution_time', 'p10_execution_time']]:\n",
    "    fig = px.line(wpdf, x='timestamp', y=col)\n",
    "    color_ontime(fig)\n",
    "    fig.show()\n",
    "\n",
    "for col in [[\"suspend_lag_avg\", 'suspend_lag_max', 'suspend_lag_min', 'suspend_lag_median']]:\n",
    "    fig = px.line(sdf, x='hour', y=col)\n",
    "    color_ontime(fig)\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0854f2-5c5f-47d8-9f78-9156e101073f",
   "metadata": {},
   "outputs": [],
   "source": [
    "wpdf.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01cb02ac-418b-4e74-8fb3-e9f74bb8398c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%sh \n",
    "# jupyter nbconvert --to html autosuspend_monitoring_simplified.ipynb --output xo_grp_dev_wh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df40bb9-6468-4561-b902-db432c5b2e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%sh \n",
    "# jupyter nbconvert --to html smart-suspend-analysis.ipynb --no-input --output test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a284be60-630d-499a-933d-b450fdc352d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "jupyter nbconvert --ClearOutputPreprocessor.enabled=True --inplace *.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08107830-7ec7-4b34-b6af-4f9465aa9360",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
