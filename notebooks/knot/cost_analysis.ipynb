{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
    "    return false;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding system path\n",
    "import sys, pathlib\n",
    "sys.path.append(str(pathlib.Path.cwd().parent.parent))\n",
    "# sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set to show warnings only once\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up displays\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "from dash import Dash,html,dcc,Input,Output\n",
    "app = Dash(__name__)\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from tabulate import tabulate\n",
    "color_scheme=[\"red\",\"blue\",\"green\",\"orange\",\"purple\",\"brown\",\"pink\",\"gray\",\"olive\",\"cyan\",\"darkviolet\",\"goldenrod\",\"darkgreen\",\"chocolate\",\"lawngreen\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize connection to Snowflake and set analysis date\n",
    "from optiml.connection import SnowflakeConnConfig\n",
    "connection = SnowflakeConnConfig(accountname='jg84276.us-central1.gcp',warehousename=\"XSMALL_WH\").create_connection()\n",
    "# Initialize local environment\n",
    "import os\n",
    "cache_dir = os.path.expanduser('~/data/knot')\n",
    "# Initialize query library\n",
    "from optiml.backend.cost_profile import CostProfile, get_previous_dates\n",
    "cqlib = CostProfile(connection, 'KNT', cache_dir, \"enterprise\")\n",
    "\n",
    "# sdate = '2022-09-21'\n",
    "# edate = '2022-10-21'\n",
    "sdate = '2022-10-11'\n",
    "edate = '2022-10-21'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up autoreload for libs\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%aimport optiml.backend.cost_profile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Total cost breakdown "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis setup\n",
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "* Analysis date range: '2022-10-11' to '2022-10-21': last rolling month in the data we collected.\n",
    "\n",
    "* (Assumption) on type of Snowflake account: Enterprise Edition\n",
    "\n",
    "* (Assumption) Credit to dollar conversion: `$`3 per credit\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost by usage category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = cqlib.total_cost_breakdown_ts(sdate, edate)\n",
    "df = df.fillna('Unassigned')\n",
    "df_by_usage_category = df.groupby(\"category_name\").sum(\"numeric_only\").reset_index()\n",
    "df_by_usage_category.loc[len(df_by_usage_category.index)] = ['Total', df_by_usage_category['credits'].sum(), df_by_usage_category['dollars'].sum()]\n",
    "df_by_usage_category = df_by_usage_category.round(2)\n",
    "print('Credit and dollar usage by category (Current month)')\n",
    "print('---------------------------------------------------')\n",
    "print(tabulate(df_by_usage_category, headers='keys', tablefmt='rounded_outline', showindex=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get usage for previous month as a predictive sanity check\n",
    "p1_sdate, p1_edate = get_previous_dates(sdate, edate, 1)\n",
    "df_prev = cqlib.total_cost_breakdown_ts(p1_sdate, p1_edate)\n",
    "df_prev = df_prev.fillna('Unassigned')\n",
    "df_by_usage_category_prev = df_prev.groupby(\"category_name\").sum(\"numeric_only\").reset_index()\n",
    "df_by_usage_category_prev.loc[len(df_by_usage_category_prev.index)] = ['Total', df_by_usage_category_prev['credits'].sum(), \n",
    "                                                                       df_by_usage_category_prev['dollars'].sum()]\n",
    "df_by_usage_category_prev = df_by_usage_category_prev.round(2)\n",
    "print('Credit and dollar usage by category (Previous month)')\n",
    "print('----------------------------------------------------')\n",
    "print(tabulate(df_by_usage_category_prev, headers='keys', tablefmt='rounded_outline', showindex=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_change = pd.DataFrame().assign(category_name=df_by_usage_category[\"category_name\"])\n",
    "df_change[\"percent_change\"] = ((df_by_usage_category[\"dollars\"] - df_by_usage_category_prev[\"dollars\"])/df_by_usage_category_prev[\"dollars\"]*100).round(2)\n",
    "print('Percentage change in dollar usage')\n",
    "print('---------------------------------')\n",
    "print(tabulate(df_change, headers='keys', tablefmt='rounded_outline', showindex=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pie charts for total cost breakdown\n",
    "## Remove the last row of totals for the plot\n",
    "df_by_usage_category.reset_index(inplace=True)\n",
    "df_by_usage_category.drop(columns=[\"index\"], inplace=True)\n",
    "df_by_usage_category = df_by_usage_category.drop(len(df_by_usage_category)-1) \n",
    "fig = make_subplots(\n",
    "    rows=1, cols=2,\n",
    "    specs=[[{\"type\": \"pie\"},{\"type\": \"pie\"}]],\n",
    "    subplot_titles=(\"Dollars\", \"Credits\")\n",
    ")\n",
    "\n",
    "fig.add_trace(go.Pie(labels=df_by_usage_category['category_name'].tolist(), values=df_by_usage_category['dollars'].tolist(),name=\"Dollars\", \n",
    "                     rotation=45, marker_colors=color_scheme),row=1,col=1)\n",
    "fig.add_trace(go.Pie(labels=df_by_usage_category['category_name'].tolist(), values=df_by_usage_category['credits'].tolist(),name='Credits',\n",
    "                     rotation=45, marker_colors=color_scheme),row=1,col=2)\n",
    "\n",
    "fig.update_layout(\n",
    "    title={\n",
    "        'text': \"Breakdown of total cost by usage category\",\n",
    "        'y':0.95,\n",
    "        'x':0.5,\n",
    "        'xanchor': 'center',\n",
    "        'yanchor': 'top'})\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost by usage category timeseries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_by_category_ts = df.groupby(['category_name','hourly_start_time']).sum('numeric_only').reset_index()\n",
    "fig = px.area(df_by_category_ts, x=\"hourly_start_time\", y=\"dollars\", color=\"category_name\",color_discrete_sequence=color_scheme)\n",
    "fig.update_layout(\n",
    "    title={\n",
    "        'text': \"Timeseries of cost by usage category\",\n",
    "        'y':0.95,\n",
    "        'x':0.5,\n",
    "        'xanchor': 'center',\n",
    "        'yanchor': 'top'},\n",
    "    xaxis_title=\"Hourly start time (UTC)\",\n",
    "    yaxis_title=\"US Dollars\"\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st = df[\"start_time\"].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "etime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = [get_queries(s,e) for (s,e) in (start_time, end_time)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_compute = df_by_category_ts[df_by_category_ts[\"category_name\"] == \"Compute\"].round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_consumption = df_compute.mean().round(2)\n",
    "max_consumption = df_compute.loc[df_compute['credits'].idxmax()]\n",
    "max_consumption.drop(\"category_name\", inplace=True)\n",
    "min_consumption = df_compute.loc[df_compute['credits'].idxmin()]\n",
    "min_consumption.drop(\"category_name\", inplace=True)\n",
    "\n",
    "print('Avg. hourly consumption:')\n",
    "print('-----------------')\n",
    "print(avg_consumption)\n",
    "\n",
    "print('')\n",
    "\n",
    "print('Max. hourly consumption:')\n",
    "print('-----------------')\n",
    "print(max_consumption)\n",
    "\n",
    "print('')\n",
    "\n",
    "print('Min. hourly consumption:')\n",
    "print('-----------------')\n",
    "print(min_consumption)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis & recommendations\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "#### Compute\n",
    "* Compute consumes ~75% of the credits budget and > 65% of the dollar budget. This is as expected - compared to other datasets seems a little low.\n",
    "* The average hourly compute consumption is Credits: 7.93, Dollars: `$`15.86, with usage peaks typically 5-9 am with a period max of Credits: 17.88, Dollars: `$` 35.76. \n",
    "* At off-peak compute times (11 am - 4 am) credit consumption is 15-20 credits. \n",
    "* There is a decline in usage trends from beginning of the analysis period (11th Oct) which is led by cloud services consumption - an 80% decline.\n",
    "\n",
    "#### Cloud Services\n",
    "* Cloud services credit cnsumption is > 15% of compute. It should be lowere than 10% so there are definitely overages here and it should be explored how to reduce this. \n",
    "* Reduced usage since beginning of the pay period but overall still a significant fraction of compute which would result in overages.\n",
    "    \n",
    "#### Snowpipe\n",
    "* Snowpipe usage is high - is there a slower cadence on which data can be made availabe in tables or is it required to have it available with minutes latency every time its refreshed.\n",
    "\n",
    "#### Storage\n",
    "* Storage is ~8-9% of the cost.\n",
    "* Next steps are to break it down by:\n",
    "    * Storage bytes\n",
    "    * Staging bytes\n",
    "    * Failsafe bytes\n",
    "    \n",
    "#### General trends\n",
    "* Month over month:\n",
    "    * Autoclustering              31.53\n",
    "    * Cloud services              34.77\n",
    "    * Compute                      6.06\n",
    "    * Snowpipe                   -24.56\n",
    "    * Storage                      0.42\n",
    "    * Total                        5.11\n",
    "\n",
    "* Since Compute is majority of the expense a smaller increase in it may have a significant impact\n",
    "* Significant increase in cloud services and autoclustering. \n",
    "    * Analyze cloud services to see whats driving the cost\n",
    "    * ROI on enabling autoclustering should be analyzed\n",
    "      \n",
    "* This is very basic predictive analysis - more complex analyses can be made available in the pilot \n",
    "* Load is well spread out. There may be scope for dithering some workload from 11 am - 3 am\n",
    "    \n",
    "* Set up a time varying resource monitor on the account level based on these usage patterns to flag any anomalous usage. Usage monitoring set naively:\n",
    "    * Will not be proactive about cost containment\n",
    "    * Can generate false positives/negatives causing nuisance alerts/shutdowns.\n",
    "* Evaluate opportunities for savings through:\n",
    "    * Dithering peak workloads to off-peak\n",
    "    * Reducing weekend consumption\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost by user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = cqlib.cost_by_user_ts(sdate, edate)\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_by_user = df.groupby(['user_name']).sum('numeric_only').reset_index()\n",
    "df_by_user = df_by_user.round(2)\n",
    "df_by_user.loc[len(df_by_user.index)] = ['Total', df_by_user['approximate_credits_used'].sum()]\n",
    "print('Credit and dollar usage by user (Current month)')\n",
    "print('-----------------------------------------------')\n",
    "print(tabulate(df_by_user, headers='keys', tablefmt='rounded_outline', showindex=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_by_user[\"percent_usage\"] = df_by_user[\"approximate_credits_used\"]/df_by_user[df_by_user[\"user_name\"]==\"Total\"][\"approximate_credits_used\"].values[0]*100\n",
    "df_by_user[\"percent_usage\"] = df_by_user[\"percent_usage\"].round(3)\n",
    "x = df_by_user.loc[df_by_user[\"percent_usage\"]<1.00].sum(axis=0,numeric_only=True)\n",
    "df_low_usage_users = df_by_user.loc[df_by_user[\"percent_usage\"] < 1.00].reset_index(drop=True)\n",
    "print('Credit and dollar for low usage users (Current month)')\n",
    "print('-----------------------------------------------------')\n",
    "print(tabulate(df_low_usage_users, headers='keys', tablefmt='rounded_outline', showindex=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_by_user = df_by_user.loc[df_by_user[\"percent_usage\"] > 1.00].reset_index(drop=True)\n",
    "df_by_user.loc[len(df_by_user)-1.5] = [\"Low_usage_users\", x[\"approximate_credits_used\"], x[\"percent_usage\"]]\n",
    "df_by_user = df_by_user.sort_index().reset_index(drop=True)\n",
    "print('Credit and dollar usage by user with low usage users consolidated (Current month)')\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print(tabulate(df_by_user, headers='keys', tablefmt='rounded_outline', showindex=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_by_user.drop(df_by_user.tail(1).index,inplace=True)\n",
    "fig = make_subplots(\n",
    "    rows=1, cols=1,\n",
    "    specs=[[{\"type\": \"pie\"}]],\n",
    "    subplot_titles=(\"Credits\")\n",
    ")\n",
    "\n",
    "fig.add_trace(go.Pie(labels=df_by_user['user_name'].tolist(), values=df_by_user['approximate_credits_used'].tolist(),name=\"Credits\", rotation=270,marker_colors=color_scheme),row=1,col=1)\n",
    "# fig.add_trace(go.Pie(labels=df_by_user['user_name'].tolist(), values=df_by_user['credits'].tolist(),name='Credits', rotation=45,marker_colors=color_scheme),row=1,col=2)\n",
    "\n",
    "fig.update_layout(\n",
    "    title={\n",
    "        'text': \"Breakdown of total cost by user\",\n",
    "        'y':0.1,\n",
    "        'x':0.5,\n",
    "        'xanchor': 'center',\n",
    "        'yanchor': 'bottom'})\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_by_user_ts = df.groupby(['user_name','hourly_start_time']).sum('numeric_only').reset_index()\n",
    "df_by_user_ts = df_by_user_ts[~df_by_user_ts.user_name.isin(df_low_usage_users[\"user_name\"].values)]\n",
    "df_by_user_ts.reset_index(drop=True)\n",
    "fig = px.area(df_by_user_ts, x=\"hourly_start_time\", y=\"approximate_credits_used\", color=\"user_name\",color_discrete_sequence=color_scheme)\n",
    "fig.update_layout(\n",
    "    title={\n",
    "        'text': \"Timeseries of cost by user\",\n",
    "        'y':0.95,\n",
    "        'x':0.5,\n",
    "        'xanchor': 'center',\n",
    "        'yanchor': 'top'},\n",
    "    xaxis_title=\"Hourly start time (UTC)\",\n",
    "    yaxis_title=\"Credits used (approx.)\"\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "* APIUSER_WP, BIPROCESS and GP_ETL_USER are the majority users consuming ~ 75% of the overall credits\n",
    "* APIUSER_WP usage seems correlated with Cloud Services use and should be examined to see if that is driving Cloud Services costs\n",
    "* Following users are bursty in usage and it should be explored if the peak usage can be dithered to off-peak times:\n",
    "    * BI_PROCESS \n",
    "    * GP_ETL_USER\n",
    "    * SVCPRDSFMYACCT\n",
    "* Explore Low_usage_users - are they users that use frequently or are idle/not with the organization anymore (security thread)\n",
    "* Tagging should be made available on queries from biggest users for better cost attribution\n",
    "            \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost by warehouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns results only for ACCOUNTADMIN role or any other role that has been granted MONITOR USAGE global privilege\n",
    "# So results consisten with Greg's usage\n",
    "df = cqlib.cost_by_wh_ts(sdate, edate)\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_by_wh = df.groupby(['warehouse_name']).sum('numeric_only').reset_index()\n",
    "df_by_wh = df_by_wh.round(2)\n",
    "df_by_wh.loc[len(df.index)] = ['Total', df_by_wh['credits'].sum(), df_by_wh['dollars'].sum(),  df_by_wh['cloud_services_credits'].sum(), df_by_wh['cloud_services_dollars'].sum()]\n",
    "print('Credit and dollar usage overall and for cloud services by warehouse (Current month)')\n",
    "print('-----------------------------------------------------------------------------------')\n",
    "print(tabulate(df_by_wh, headers='keys', tablefmt='rounded_outline', showindex=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the last row of totals for the plot\n",
    "df_by_wh.drop(df_by_wh.tail(1).index,inplace=True)\n",
    "\n",
    "fig = make_subplots(\n",
    "    rows=1, cols=1,\n",
    "    specs=[[{\"type\": \"pie\"}]],\n",
    ")\n",
    "\n",
    "fig.add_trace(go.Pie(labels=df_by_wh['warehouse_name'].tolist(), values=df_by_wh['dollars'].tolist(),name='dollars',marker_colors=color_scheme),row=1,col=1)\n",
    "\n",
    "fig.update_layout(\n",
    "    title={\n",
    "        'text': \"Breakdown of total cost by warehouse\",\n",
    "        'y':0.1,\n",
    "        'x':0.5,\n",
    "        'xanchor': 'center',\n",
    "        'yanchor': 'top'})\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_by_wh_ts = df.groupby(['warehouse_name','hourly_start_time']).sum('numeric_only').reset_index()\n",
    "# df_by_wh_ts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##TODO: Investigate why tunring off cloud services only makes daily refresh plot jump in some points\n",
    "fig = px.area(df_by_wh_ts, x=\"hourly_start_time\", y=\"credits\", color=\"warehouse_name\",color_discrete_sequence=color_scheme)\n",
    "fig.update_layout(\n",
    "    title={\n",
    "        'text': \"Timeseries of cost by warehouse\",\n",
    "        'y':0.95,\n",
    "        'x':0.5,\n",
    "        'xanchor': 'center',\n",
    "        'yanchor': 'top'},\n",
    "    xaxis_title=\"Hourly start time (UTC)\",\n",
    "    yaxis_title=\"Credits used\"\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "<div class=\"alert alert-info\">    \n",
    "  \n",
    "* LOAD_WH incurs highest and ~60% of the costs at ~ 2800 credits\n",
    "    * Usage for this WH is extremely bursty so there might be opportunity to move queries to this WH between 11 am and 3 am\n",
    "    * Maybe opportunity to scale the WH size or number of clusters at different times to account for bursty usage\n",
    "\n",
    "* LOCAL_LOAD_WH is highly correlated with API_USER_WP and high CLOUD SERVICES usage - can be used to track causal drivers of high CLOUD SERVICES costs\n",
    "    * Bursty usage and has high so there might be scope for dithering usage\n",
    "    \n",
    "* SEGMENT_WH is the 3rd largest consumer of credits but is running almost continuously. Unless the events are needed continuously it might be a better strategy to time the events and dither them to off peak times for other WH\n",
    "\n",
    "* If better cost attribution is the aim then consider tagging jobs by function in LOAD_WH, LOCAL_LOAD_WH\n",
    "\n",
    "* Set up a time varying resource monitor at warehouse level based on usage patterns to flag any anomalous usage.\n",
    "    \n",
    "* Similar trend analysis as total cost by usage can be provided for each user and especially for high usage users to capture increases in cost trends and set monitors against    \n",
    "    \n",
    "* Team is keeping track of warehouse resizing in comments - we can make that available through tracking changes in warehouse events history \n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost by Partner Tools\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=cqlib.cost_by_partner_tool_ts(sdate, edate)\n",
    "# df.to_csv('/home/manas/DS_data/partner_tools.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_by_pt = df.groupby(['client_application_name']).sum('numeric_only').reset_index()\n",
    "df_by_pt = df_by_pt.round(2)\n",
    "df_by_pt.loc[len(df.index)] = ['Total', df_by_pt['approximate_credits_used'].sum()]\n",
    "print(tabulate(df_by_pt, headers='keys', tablefmt='rounded_outline', showindex=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the last row of totals for the plot\n",
    "df_by_pt.drop(df_by_pt.tail(1).index,inplace=True)\n",
    "\n",
    "fig = make_subplots(\n",
    "    rows=1, cols=1,\n",
    "    specs=[[{\"type\": \"pie\"}]],\n",
    ")\n",
    "\n",
    "fig.add_trace(go.Pie(labels=df_by_pt['client_application_name'].tolist(), values=df_by_pt['approximate_credits_used'].tolist(),name='credits',marker_colors=color_scheme, rotation=45),row=1,col=1)\n",
    "\n",
    "fig.update_layout(\n",
    "    title={\n",
    "        'text': \"Breakdown of total cost by partner tools\",\n",
    "        'y':0.1,\n",
    "        'x':0.5,\n",
    "        'xanchor': 'center',\n",
    "        'yanchor': 'top'})\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_by_pt_ts = df.groupby(['client_application_name','hourly_start_time']).sum('numeric_only').reset_index()\n",
    "# df_by_pt_ts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.area(df_by_pt_ts, x=\"hourly_start_time\", y=\"approximate_credits_used\", color=\"client_application_name\",color_discrete_sequence=color_scheme)\n",
    "fig.update_layout(\n",
    "    title={\n",
    "        'text': \"Timeseries of cost by partner tools\",\n",
    "        'y':0.95,\n",
    "        'x':0.5,\n",
    "        'xanchor': 'center',\n",
    "        'yanchor': 'top'},\n",
    "    xaxis_title=\"Hourly start time (UTC)\",\n",
    "    yaxis_title=\"Credits used (approx.)\"\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_titles=sorted(df[\"warehouse_name\"].unique())\n",
    "df_warehouse = [d for _, d in df.groupby(['warehouse_name'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(df_warehouse)):\n",
    "    fig = px.area(df_warehouse[i], x=\"hourly_start_time\", y=\"approximate_credits_used\", color=\"client_application_name\",color_discrete_sequence=color_scheme,title=df_titles[i])\n",
    "    fig.update_layout(\n",
    "        xaxis_title=\"Hourly start time (UTC)\",\n",
    "        yaxis_title=\"Credits used (approx.)\"\n",
    "    )\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "<div class=\"alert alert-info\">\n",
    "    \n",
    "* Tools connecting through JDBC driver consume most of the credits ~50%\n",
    "* Go consumes a surprisingly large amount given that its a data system - would have expected Python - but presumably caused by Segment related events which might be getting used in real time for marketing/funnel\n",
    "* The increased usage in credits at the beginning of the period is almost entirely caused by Javascript on LOCAL_LOAD_WH    \n",
    "* Similar trend analysis as total cost by usage can be provided for each partner tool and especially for high usage users to capture increases in cost trends and set monitors against.\n",
    "* Tag jobs associated with each partner tool better to discern which physical tool is connecting to optimize usage across entire lineage\n",
    "* Set up a time varying resource monitor at warehouse level based on usage patterns to flag any anomalous usage.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost of data transfers: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further Actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "* Implement cost of data transfers query using https://docs.snowflake.com/en/user-guide/cost-exploring-data-transfer.html\n",
    "    * Will need access to:\n",
    "        * DATA_TRANSFER_DAILY_HISTORY\n",
    "        * DATA_TRANSFER_HISTORY\n",
    "        * ORGANIZATION_USAGE ACCOUNT_USAGE\n",
    "        * DATABASE_REPLICATION_USAGE_HISTORY\n",
    "        * REPLICATION_USAGE_HISTORY\n",
    "        * REPLICATION_GROUP_USAGE_HISTORY\n",
    "        * USAGE_IN_CURRENCY_DAILY\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "fcbd4ad66d969ea49516a1cf27383420b67e9e950ebdd1bbb64e01b736f968b6"
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
